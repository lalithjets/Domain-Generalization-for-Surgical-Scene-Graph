{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "arguments\n",
    "    Hyperparameters, file location, optimizer, network, data_processing\n",
    "'''\n",
    "ver = 'g_cbs_t_resnet18_09_cbs_ts'\n",
    "f_e = 'resnet18_09_cbs_ts'\n",
    "\n",
    "class arguments():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # hyper parameters\n",
    "        self.lr = 0.00001\n",
    "        self.epoch = 251\n",
    "        self.ft_epoch = 81\n",
    "        self.start_epoch = 0\n",
    "        self.batch_size = 32\n",
    "        self.gpu = True\n",
    "        self.print_every = 10 \n",
    "        self.train_model = 'epoch'\n",
    "        self.exp_ver= ver\n",
    "\n",
    "        # file locations\n",
    "        self.log_dir = './log/' + ver\n",
    "        self.save_dir = './checkpoints/' + ver\n",
    "        self.output_img_dir = './results/' + ver\n",
    "        self.save_every = 10\n",
    "        self.pretrained = None                 \n",
    "\n",
    "        # optimizer\n",
    "        self.optim='adam' # choices=['sgd', 'adam']\n",
    "\n",
    "        # network\n",
    "        self.layers= 1\n",
    "        self.bn = False\n",
    "        self.drop_prob = 0.3\n",
    "        self.bias = True\n",
    "        self.multi_attn = False\n",
    "        self.diff_edge = False\n",
    "\n",
    "        # data_processing\n",
    "        self.sampler = 0\n",
    "        self.data_aug = False\n",
    "        self.feature_extractor = f_e\n",
    "        \n",
    "        # CBS\n",
    "        self.use_cbs = True\n",
    "        \n",
    "        # temperature_scaling\n",
    "        self.use_t = True\n",
    "        self.t_scale = 1.5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "configurations of the network\n",
    "    \n",
    "    readout: G_ER_L_S = [1024+300+16+300+1024,  1024, 117]\n",
    "\n",
    "    node_func: G_N_L_S = [1024+1024, 1024]\n",
    "    node_lang_func: G_N_L_S2 = [300+300+300]\n",
    "    \n",
    "    edge_func : G_E_L_S = [1024*2+16, 1024]\n",
    "    edge_lang_func: [300*2, 1024]\n",
    "    \n",
    "    attn: [1024, 1]\n",
    "    attn_lang: [1024, 1]\n",
    "'''\n",
    "\n",
    "class CONFIGURATION(object):\n",
    "    '''\n",
    "    Configuration arguments: feature type, layer, bias, batch normalization, dropout, multi-attn\n",
    "    \n",
    "    readout           : fc_size, activation, bias, bn, droupout\n",
    "    gnn_node          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_node_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    gnn_edge          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_edge_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    gnn_attn          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_attn_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    '''\n",
    "    def __init__(self, layer=1, bias=True, bn=False, dropout=0.2, multi_attn=False):\n",
    "        \n",
    "        # if multi_attn:\n",
    "        if True:\n",
    "            if layer==1:\n",
    "                feature_size = 512\n",
    "                # readout\n",
    "                self.G_ER_L_S = [feature_size+300+16+300+feature_size, feature_size, 13]\n",
    "                self.G_ER_A   = ['ReLU', 'Identity']\n",
    "                self.G_ER_B   = bias    #true\n",
    "                self.G_ER_BN  = bn      #false\n",
    "                self.G_ER_D   = dropout #0.3\n",
    "                # self.G_ER_GRU = feature_size\n",
    "\n",
    "                # # gnn node function\n",
    "                self.G_N_L_S = [feature_size+feature_size, feature_size]\n",
    "                self.G_N_A   = ['ReLU']\n",
    "                self.G_N_B   = bias #true\n",
    "                self.G_N_BN  = bn      #false\n",
    "                self.G_N_D   = dropout #0.3\n",
    "                # self.G_N_GRU = feature_size\n",
    "\n",
    "                # # gnn node function for language\n",
    "                self.G_N_L_S2 = [300+300, 300]\n",
    "                self.G_N_A2   = ['ReLU']\n",
    "                self.G_N_B2   = bias    #true\n",
    "                self.G_N_BN2  = bn      #false\n",
    "                self.G_N_D2   = dropout #0.3\n",
    "                # self.G_N_GRU2 = feature_size\n",
    "\n",
    "                # gnn edge function1\n",
    "                self.G_E_L_S           = [feature_size*2+16, feature_size]\n",
    "                self.G_E_A             = ['ReLU']\n",
    "                self.G_E_B             = bias     # true\n",
    "                self.G_E_BN            = bn       # false\n",
    "                self.G_E_D             = dropout  # 0.3\n",
    "                self.G_E_c_std         = 1.0\n",
    "                self.G_E_c_std_factor  = 0.9      # 0.985 (LOG), 0.95 (gau)\n",
    "                self.G_E_c_epoch       = 20\n",
    "                self.G_E_c_kernel_size = 3\n",
    "                self.G_E_c_filter      = 'gau' # 'gau', 'LOG'\n",
    "\n",
    "                # gnn edge function2 for language\n",
    "                self.G_E_L_S2 = [300*2, feature_size]\n",
    "                self.G_E_A2   = ['ReLU']\n",
    "                self.G_E_B2   = bias     #true\n",
    "                self.G_E_BN2  = bn       #false\n",
    "                self.G_E_D2   = dropout  #0.3\n",
    "\n",
    "                # gnn attention mechanism\n",
    "                self.G_A_L_S = [feature_size, 1]\n",
    "                self.G_A_A   = ['LeakyReLU']\n",
    "                self.G_A_B   = bias     #true\n",
    "                self.G_A_BN  = bn       #false\n",
    "                self.G_A_D   = dropout  #0.3\n",
    "\n",
    "                # gnn attention mechanism2 for language\n",
    "                self.G_A_L_S2 = [feature_size, 1]\n",
    "                self.G_A_A2   = ['LeakyReLU']\n",
    "                self.G_A_B2   = bias    #true\n",
    "                self.G_A_BN2  = bn      #false\n",
    "                self.G_A_D2   = dropout #0.3\n",
    "                    \n",
    "    def save_config(self):\n",
    "        model_config = {'graph_head':{}, 'graph_node':{}, 'graph_edge':{}, 'graph_attn':{}}\n",
    "        CONFIG=self.__dict__\n",
    "        for k, v in CONFIG.items():\n",
    "            if 'G_H' in k:\n",
    "                model_config['graph_head'][k]=v\n",
    "            elif 'G_N' in k:\n",
    "                model_config['graph_node'][k]=v\n",
    "            elif 'G_E' in k:\n",
    "                model_config['graph_edge'][k]=v\n",
    "            elif 'G_A' in k:\n",
    "                model_config['graph_attn'][k]=v\n",
    "            else:\n",
    "                model_config[k]=v\n",
    "        \n",
    "        return model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_gaussian_filter_1D(kernel_size=3, sigma=2, channels=3):\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    \n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "    mean = (kernel_size - 1)/2.\n",
    "    variance = sigma**2.\n",
    "    xy_grid = torch.sum((xy_grid[:kernel_size,:kernel_size,:] - mean)**2., dim=-1)\n",
    "\n",
    "    # Calculate the 1-dimensional gaussian kernel\n",
    "    gaussian_kernel = (1./((math.sqrt(2.*math.pi)*sigma))) * \\\n",
    "                        torch.exp(-1* (xy_grid[int(kernel_size/2)]) / (2*variance))\n",
    "\n",
    "    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size)\n",
    "    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1)\n",
    "\n",
    "    padding = 1 if kernel_size==3 else 2 if kernel_size == 5 else 0\n",
    "    gaussian_filter = nn.Conv1d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels,\n",
    "                                bias=False, padding=padding)\n",
    "    gaussian_filter.weight.data = gaussian_kernel\n",
    "    gaussian_filter.weight.requires_grad = False \n",
    "    return gaussian_filter\n",
    "\n",
    "def get_laplaceOfGaussian_filter_1D(kernel_size=3, sigma=2, channels=3):\n",
    "    \n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "    mean = (kernel_size - 1)/2.\n",
    "\n",
    "    used_sigma = sigma\n",
    "    # Calculate the 2-dimensional gaussian kernel which is\n",
    "    log_kernel = (-1./(math.pi*(used_sigma**4))) \\\n",
    "                  * (1-(torch.sum((xy_grid[int(kernel_size/2)] - mean)**2., dim=-1) / (2*(used_sigma**2)))) \\\n",
    "                  * torch.exp(-torch.sum((xy_grid[int(kernel_size/2)] - mean)**2., dim=-1) / (2*(used_sigma**2)))\n",
    "    \n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    log_kernel = log_kernel / torch.sum(log_kernel)\n",
    "    log_kernel = log_kernel.view(1, 1, kernel_size)\n",
    "    log_kernel = log_kernel.repeat(channels, 1, 1)\n",
    "\n",
    "    padding = 1 if kernel_size==3 else 2 if kernel_size == 5 else 0\n",
    "    log_filter = nn.Conv1d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels,\n",
    "                                bias=False, padding=padding)\n",
    "    log_filter.weight.data = log_kernel\n",
    "    log_filter.weight.requires_grad = False\n",
    "    return log_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primary activation and MLP layer\n",
    "acivation:\n",
    "    Identity\n",
    "    ReLU\n",
    "    LeakyReLU\n",
    "MLP:\n",
    "    init: layer size, activation, bias, use_BN, dropout_probability\n",
    "    forward: x\n",
    "'''\n",
    "\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    '''\n",
    "    Identity class activation layer\n",
    "    x = x\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Identity,self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def get_activation(name):\n",
    "    '''\n",
    "    get_activation sub-function\n",
    "    argument: activatoin name (eg. ReLU, Identity, LeakyReLU)\n",
    "    '''\n",
    "    if name=='ReLU': return nn.ReLU(inplace=True)\n",
    "    elif name=='Identity': return Identity()\n",
    "    elif name=='LeakyReLU': return nn.LeakyReLU(0.2,inplace=True)\n",
    "    else: assert(False), 'Not Implemented'\n",
    "    #elif name=='Tanh': return nn.Tanh()\n",
    "    #elif name=='Sigmoid': return nn.Sigmoid()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Args:\n",
    "        layer_sizes: a list, [1024,1024,...]\n",
    "        activation: a list, ['ReLU', 'Tanh',...]\n",
    "        bias : bool\n",
    "        use_bn: bool\n",
    "        drop_prob: default is None, use drop out layer or not\n",
    "    '''\n",
    "    def __init__(self, layer_sizes, activation, bias=True, use_bn=False, drop_prob=None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.bn = use_bn\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=bias)\n",
    "            activate = get_activation(activation[i])\n",
    "            block = nn.Sequential(OrderedDict([(f'L{i}', layer), ]))\n",
    "            \n",
    "            # !NOTE:# Actually, it is inappropriate to use batch-normalization here\n",
    "            if use_bn:                                  \n",
    "                bn = nn.BatchNorm1d(layer_sizes[i+1])\n",
    "                block.add_module(f'B{i}', bn)\n",
    "            \n",
    "            # batch normalization is put before activation function \n",
    "            block.add_module(f'A{i}', activate)\n",
    "\n",
    "            # dropout probablility\n",
    "            if drop_prob:\n",
    "                block.add_module(f'D{i}', nn.Dropout(drop_prob))\n",
    "            \n",
    "            self.layers.append(block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # !NOTE: sometime the shape of x will be [1,N], and we cannot use batch-normailzation in that situation\n",
    "            if self.bn and x.shape[0]==1:\n",
    "                x = layer[0](x)\n",
    "                x = layer[:-1](x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "H_H_EdgeApplyModule\n",
    "    init    : config, multi_attn \n",
    "    forward : edge\n",
    "    \n",
    "H_NodeApplyModule\n",
    "    init    : config\n",
    "    forward : node\n",
    "    \n",
    "E_AttentionModule1\n",
    "    init    : config\n",
    "    forward : edge\n",
    "    \n",
    "GNN\n",
    "    init    : config, multi_attn, diff_edge\n",
    "    forward : g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_features\n",
    "    \n",
    "GRNN\n",
    "    init    : config, multi_attn, diff_edge\n",
    "    forward : b_graph, b_h_node_list, b_o_node_list, b_h_h_e_list, b_o_o_e_list, b_h_o_e_list, features, spatial_features, word2vec, valid, pop_features, initial_features\n",
    "'''\n",
    "\n",
    "import ipdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class H_H_EdgeApplyModule(nn.Module): #human to human edge\n",
    "    '''\n",
    "        init    : config, multi_attn \n",
    "        forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, use_cbs = False):\n",
    "        super(H_H_EdgeApplyModule, self).__init__()\n",
    "        self.use_cbs = use_cbs\n",
    "        if use_cbs:\n",
    "            self.init_std = CONFIG.G_E_c_std \n",
    "            self.cbs_std = CONFIG.G_E_c_std\n",
    "            self.cbs_std_factor = CONFIG.G_E_c_std_factor\n",
    "            self.cbs_epoch = CONFIG.G_E_c_epoch\n",
    "            self.cbs_kernel_size = CONFIG.G_E_c_kernel_size\n",
    "            self.cbs_filter = CONFIG.G_E_c_filter\n",
    "        \n",
    "        self.edge_fc = MLP(CONFIG.G_E_L_S, CONFIG.G_E_A, CONFIG.G_E_B, CONFIG.G_E_BN, CONFIG.G_E_D)\n",
    "        self.edge_fc_lang = MLP(CONFIG.G_E_L_S2, CONFIG.G_E_A2, CONFIG.G_E_B2, CONFIG.G_E_BN2, CONFIG.G_E_D2)\n",
    "    \n",
    "    def forward(self, edge):\n",
    "        feat = torch.cat([edge.src['n_f'], edge.data['s_f'], edge.dst['n_f']], dim=1)\n",
    "        feat_lang = torch.cat([edge.src['word2vec'], edge.dst['word2vec']], dim=1)\n",
    "        if self.use_cbs:\n",
    "            feat = self.kernel1(feat[:,None,:])\n",
    "            feat = torch.squeeze(feat, 1)\n",
    "        e_feat = self.edge_fc(feat)\n",
    "        e_feat_lang = self.edge_fc_lang(feat_lang)\n",
    "  \n",
    "        return {'e_f': e_feat, 'e_f_lang': e_feat_lang}\n",
    "\n",
    "    def get_new_kernels(self, epoch_count):\n",
    "        if self.use_cbs:\n",
    "            if epoch_count == 0:\n",
    "                self.cbs_std = self.init_std\n",
    "                \n",
    "            if epoch_count % self.cbs_epoch == 0 and epoch_count is not 0:\n",
    "                self.cbs_std *= self.cbs_std_factor\n",
    "            \n",
    "            if (self.cbs_filter == 'gau'): \n",
    "                self.kernel1 = get_gaussian_filter_1D(kernel_size=self.cbs_kernel_size, sigma= self.cbs_std, channels= 1)\n",
    "            elif (self.cbs_filter == 'LOG'): \n",
    "                self.kernel1 = get_laplaceOfGaussian_filter_1D(kernel_size=self.cbs_kernel_size, sigma= self.cbs_std, channels= 1)\n",
    "\n",
    "class H_NodeApplyModule(nn.Module): #human node\n",
    "    '''\n",
    "        init    : config\n",
    "        forward : node\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(H_NodeApplyModule, self).__init__()\n",
    "        self.node_fc = MLP(CONFIG.G_N_L_S, CONFIG.G_N_A, CONFIG.G_N_B, CONFIG.G_N_BN, CONFIG.G_N_D)\n",
    "        self.node_fc_lang = MLP(CONFIG.G_N_L_S2, CONFIG.G_N_A2, CONFIG.G_N_B2, CONFIG.G_N_BN2, CONFIG.G_N_D2)\n",
    "    \n",
    "    def forward(self, node):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        feat = torch.cat([node.data['n_f'], node.data['z_f']], dim=1)\n",
    "        feat_lang = torch.cat([node.data['word2vec'], node.data['z_f_lang']], dim=1)\n",
    "        n_feat = self.node_fc(feat)\n",
    "        n_feat_lang = self.node_fc_lang(feat_lang)\n",
    "\n",
    "        return {'new_n_f': n_feat, 'new_n_f_lang': n_feat_lang}\n",
    "\n",
    "class E_AttentionModule1(nn.Module): #edge attention\n",
    "    '''\n",
    "        init    : config\n",
    "        forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(E_AttentionModule1, self).__init__()\n",
    "        self.attn_fc = MLP(CONFIG.G_A_L_S, CONFIG.G_A_A, CONFIG.G_A_B, CONFIG.G_A_BN, CONFIG.G_A_D)\n",
    "        self.attn_fc_lang = MLP(CONFIG.G_A_L_S2, CONFIG.G_A_A2, CONFIG.G_A_B2, CONFIG.G_A_BN2, CONFIG.G_A_D2)\n",
    "\n",
    "    def forward(self, edge):\n",
    "        a_feat = self.attn_fc(edge.data['e_f'])\n",
    "        a_feat_lang = self.attn_fc_lang(edge.data['e_f_lang'])\n",
    "        return {'a_feat': a_feat, 'a_feat_lang': a_feat_lang}\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    '''\n",
    "        init    : config, multi_attn, diff_edge\n",
    "        forward : g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_features\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, diff_edge=True, use_cbs = False):\n",
    "        super(GNN, self).__init__()\n",
    "        self.diff_edge = diff_edge # false\n",
    "        self.apply_h_h_edge = H_H_EdgeApplyModule(CONFIG, multi_attn, use_cbs)\n",
    "        self.apply_edge_attn1 = E_AttentionModule1(CONFIG)  \n",
    "        self.apply_h_node = H_NodeApplyModule(CONFIG)\n",
    "\n",
    "    def _message_func(self, edges):\n",
    "        return {'nei_n_f': edges.src['n_f'], 'nei_n_w': edges.src['word2vec'], 'e_f': edges.data['e_f'], 'e_f_lang': edges.data['e_f_lang'], 'a_feat': edges.data['a_feat'], 'a_feat_lang': edges.data['a_feat_lang']}\n",
    "\n",
    "    def _reduce_func(self, nodes):\n",
    "        alpha = F.softmax(nodes.mailbox['a_feat'], dim=1)\n",
    "        alpha_lang = F.softmax(nodes.mailbox['a_feat_lang'], dim=1)\n",
    "\n",
    "        z_raw_f = nodes.mailbox['nei_n_f']+nodes.mailbox['e_f']\n",
    "        z_f = torch.sum( alpha * z_raw_f, dim=1)\n",
    "\n",
    "        z_raw_f_lang = nodes.mailbox['nei_n_w']\n",
    "        z_f_lang = torch.sum(alpha_lang * z_raw_f_lang, dim=1)\n",
    "         \n",
    "        # we cannot return 'alpha' for the different dimension \n",
    "        if self.training or validation: return {'z_f': z_f, 'z_f_lang': z_f_lang}\n",
    "        else: return {'z_f': z_f, 'z_f_lang': z_f_lang, 'alpha': alpha, 'alpha_lang': alpha_lang}\n",
    "\n",
    "    def forward(self, g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_feat=False):\n",
    "        \n",
    "        g.apply_edges(self.apply_h_h_edge, g.edges())\n",
    "        g.apply_edges(self.apply_edge_attn1)\n",
    "        g.update_all(self._message_func, self._reduce_func)\n",
    "        g.apply_nodes(self.apply_h_node, h_node+o_node)\n",
    "\n",
    "        # !NOTE:PAY ATTENTION WHEN ADDING MORE FEATURE\n",
    "        g.ndata.pop('n_f')\n",
    "        g.ndata.pop('word2vec')\n",
    "\n",
    "        g.ndata.pop('z_f')\n",
    "        g.edata.pop('e_f')\n",
    "        g.edata.pop('a_feat')\n",
    "\n",
    "        g.ndata.pop('z_f_lang')\n",
    "        g.edata.pop('e_f_lang')\n",
    "        g.edata.pop('a_feat_lang')\n",
    "\n",
    "class GRNN(nn.Module):\n",
    "    '''\n",
    "    init: \n",
    "        config, multi_attn, diff_edge\n",
    "    forward: \n",
    "        batch_graph, batch_h_node_list, batch_obj_node_list,\n",
    "        batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list,\n",
    "        features, spatial_features, word2vec,\n",
    "        valid, pop_features, initial_features\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, diff_edge=True, use_cbs = False):\n",
    "        super(GRNN, self).__init__()\n",
    "        self.multi_attn = multi_attn #false\n",
    "        self.gnn = GNN(CONFIG, multi_attn, diff_edge, use_cbs)\n",
    "\n",
    "    def forward(self, batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec, valid=False, pop_feat=False, initial_feat=False):\n",
    "        \n",
    "        # !NOTE: if node_num==1, there will be something wrong to forward the attention mechanism\n",
    "        global validation \n",
    "        validation = valid\n",
    "\n",
    "        # initialize the graph with some datas\n",
    "        batch_graph.ndata['n_f'] = feat           # node: features \n",
    "        batch_graph.ndata['word2vec'] = word2vec  # node: words\n",
    "        batch_graph.edata['s_f'] = spatial_feat   # edge: spatial features\n",
    "\n",
    "        try:\n",
    "            self.gnn(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            ipdb.set_trace()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Predictor \n",
    "    init    : config\n",
    "    forward : edge\n",
    "\n",
    "AGRNN\n",
    "    init    : bias, bn, dropout, multi_attn, layer, diff_edge\n",
    "    forward : node_num, feat, spatial_feat, word2vec, roi_label, validation, choose_nodes, remove_nodes\n",
    "'''\n",
    "\n",
    "import dgl\n",
    "import ipdb\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchvision\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    '''\n",
    "    init    : config\n",
    "    forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.classifier = MLP(CONFIG.G_ER_L_S, CONFIG.G_ER_A, CONFIG.G_ER_B, CONFIG.G_ER_BN, CONFIG.G_ER_D)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, edge):\n",
    "        feat = torch.cat([edge.dst['new_n_f'], edge.dst['new_n_f_lang'], edge.data['s_f'], edge.src['new_n_f_lang'], edge.src['new_n_f']], dim=1)\n",
    "        pred = self.classifier(feat)\n",
    "        # if the criterion is BCELoss, you need to uncomment the following code\n",
    "        # output = self.sigmoid(output)\n",
    "        return {'pred': pred}\n",
    "\n",
    "class AGRNN(nn.Module):\n",
    "    '''\n",
    "    init    : \n",
    "        feature_type, bias, bn, dropout, multi_attn, layer, diff_edge\n",
    "        \n",
    "    forward : \n",
    "        node_num, features, spatial_features, word2vec, roi_label,\n",
    "        validation, choose_nodes, remove_nodes\n",
    "    '''\n",
    "    def __init__(self, bias=True, bn=True, dropout=None, multi_attn=False, layer=1, diff_edge=True, use_cbs = False):\n",
    "        super(AGRNN, self).__init__()\n",
    " \n",
    "        self.multi_attn = multi_attn # false\n",
    "        self.layer = layer           # 1 layer\n",
    "        self.diff_edge = diff_edge   # false\n",
    "        \n",
    "        self.CONFIG1 = CONFIGURATION(layer=1, bias=bias, bn=bn, dropout=dropout, multi_attn=multi_attn)\n",
    "\n",
    "        self.grnn1 = GRNN(self.CONFIG1, multi_attn=multi_attn, diff_edge=diff_edge, use_cbs = use_cbs)\n",
    "        self.edge_readout = Predictor(self.CONFIG1)\n",
    "        \n",
    "    def _collect_edge(self, node_num, roi_label, node_space, diff_edge):\n",
    "        '''\n",
    "        arguments: node_num, roi_label, node_space, diff_edge\n",
    "        '''\n",
    "        \n",
    "        # get human nodes && object nodes\n",
    "        h_node_list = np.where(roi_label == 0)[0]\n",
    "        obj_node_list = np.where(roi_label != 0)[0]\n",
    "        edge_list = []\n",
    "        \n",
    "        h_h_e_list = []\n",
    "        o_o_e_list = []\n",
    "        h_o_e_list = []\n",
    "        \n",
    "        readout_edge_list = []\n",
    "        readout_h_h_e_list = []\n",
    "        readout_h_o_e_list = []\n",
    "        \n",
    "        # get all edge in the fully-connected graph, edge_list, For node_num = 2, edge_list = [(0, 1), (1, 0)]\n",
    "        for src in range(node_num):\n",
    "            for dst in range(node_num):\n",
    "                if src == dst:\n",
    "                    continue\n",
    "                else:\n",
    "                    edge_list.append((src, dst))\n",
    "        \n",
    "        # readout_edge_list, get corresponding readout edge in the graph\n",
    "        src_box_list = np.arange(roi_label.shape[0])\n",
    "        for dst in h_node_list:\n",
    "            # if dst == roi_label.shape[0]-1:\n",
    "            #    continue\n",
    "            # src_box_list = src_box_list[1:]\n",
    "            for src in src_box_list:\n",
    "                if src not in h_node_list:\n",
    "                    readout_edge_list.append((src, dst))\n",
    "        \n",
    "        # readout h_h_e_list, get corresponding readout h_h edges && h_o edges\n",
    "        temp_h_node_list = h_node_list[:]\n",
    "        for dst in h_node_list:\n",
    "            if dst == h_node_list.shape[0]-1:\n",
    "                continue\n",
    "            temp_h_node_list = temp_h_node_list[1:]\n",
    "            for src in temp_h_node_list:\n",
    "                if src == dst: continue\n",
    "                readout_h_h_e_list.append((src, dst))\n",
    "\n",
    "        # readout h_o_e_list\n",
    "        readout_h_o_e_list = [x for x in readout_edge_list if x not in readout_h_h_e_list]\n",
    "\n",
    "        # add node space to match the batch graph\n",
    "        h_node_list = (np.array(h_node_list)+node_space).tolist()\n",
    "        obj_node_list = (np.array(obj_node_list)+node_space).tolist()\n",
    "        \n",
    "        h_h_e_list = (np.array(h_h_e_list)+node_space).tolist() #empty no diff_edge\n",
    "        o_o_e_list = (np.array(o_o_e_list)+node_space).tolist() #empty no diff_edge\n",
    "        h_o_e_list = (np.array(h_o_e_list)+node_space).tolist() #empty no diff_edge\n",
    "\n",
    "        readout_h_h_e_list = (np.array(readout_h_h_e_list)+node_space).tolist()\n",
    "        readout_h_o_e_list = (np.array(readout_h_o_e_list)+node_space).tolist()   \n",
    "        readout_edge_list = (np.array(readout_edge_list)+node_space).tolist()\n",
    "\n",
    "        return edge_list, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list\n",
    "    \n",
    "    def _build_graph(self, node_num, roi_label, node_space, diff_edge):\n",
    "        '''\n",
    "        Declare graph, add_nodes, collect edges, add_edges\n",
    "        '''\n",
    "        graph = dgl.DGLGraph()\n",
    "        graph.add_nodes(node_num)\n",
    "\n",
    "        edge_list, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list = self._collect_edge(node_num, roi_label, node_space, diff_edge)\n",
    "        src, dst = tuple(zip(*edge_list))\n",
    "        graph.add_edges(src, dst)   # make the graph bi-directional\n",
    "\n",
    "        return graph, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list\n",
    "\n",
    "    def forward(self, node_num=None, feat=None, spatial_feat=None, word2vec=None, roi_label=None, validation=False, choose_nodes=None, remove_nodes=None):\n",
    "        \n",
    "        batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, batch_readout_edge_list, batch_readout_h_h_e_list, batch_readout_h_o_e_list = [], [], [], [], [], [], [], [], []\n",
    "        node_num_cum = np.cumsum(node_num) # !IMPORTANT\n",
    "        \n",
    "        for i in range(len(node_num)):\n",
    "            # set node space\n",
    "            node_space = 0\n",
    "            if i != 0:\n",
    "                node_space = node_num_cum[i-1]\n",
    "            graph, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list = self._build_graph(node_num[i], roi_label[i], node_space, diff_edge=self.diff_edge)\n",
    "            \n",
    "            # updata batch\n",
    "            batch_graph.append(graph)\n",
    "            batch_h_node_list += h_node_list\n",
    "            batch_obj_node_list += obj_node_list\n",
    "            \n",
    "            batch_h_h_e_list += h_h_e_list\n",
    "            batch_o_o_e_list += o_o_e_list\n",
    "            batch_h_o_e_list += h_o_e_list\n",
    "            \n",
    "            batch_readout_edge_list += readout_edge_list\n",
    "            batch_readout_h_h_e_list += readout_h_h_e_list\n",
    "            batch_readout_h_o_e_list += readout_h_o_e_list\n",
    "        \n",
    "        batch_graph = dgl.batch(batch_graph)\n",
    "        \n",
    "        # GRNN\n",
    "        self.grnn1(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec, validation, initial_feat=True)\n",
    "        batch_graph.apply_edges(self.edge_readout, tuple(zip(*(batch_readout_h_o_e_list+batch_readout_h_h_e_list))))\n",
    "        \n",
    "        if self.training or validation:\n",
    "            # !NOTE: cannot use \"batch_readout_h_o_e_list+batch_readout_h_h_e_list\" because of the wrong order\n",
    "            return batch_graph.edges[tuple(zip(*batch_readout_edge_list))].data['pred']\n",
    "        else:\n",
    "            return batch_graph.edges[tuple(zip(*batch_readout_edge_list))].data['pred'], \\\n",
    "                   batch_graph.nodes[batch_h_node_list].data['alpha'], \\\n",
    "                   batch_graph.nodes[batch_h_node_list].data['alpha_lang'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils.io as io\n",
    "\n",
    "class SurgicalSceneConstants():\n",
    "    def __init__( self):\n",
    "        self.instrument_classes = ('kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "        \n",
    "        #self.instrument_classes = ( 'kidney', 'bipolar_forceps', 'fenestrated_bipolar', \n",
    "        #                             'prograsp_forceps', 'large_needle_driver', 'vessel_sealer',\n",
    "        #                             'grasping_retractor', 'monopolar_curved_scissors', \n",
    "        #                             'ultrasound_probe', 'suction', 'clip_applier', 'stapler')\n",
    "        \n",
    "        self.action_classes = ( 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                                'Tool_Manipulation', 'Cutting', 'Cauterization', \n",
    "                                'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', \n",
    "                                'Ultrasound_Sensing')\n",
    "        self.xml_data_dir = 'datasets/instruments18/seq_'\n",
    "        self.word2vec_loc = 'datasets/surgicalscene_word2vec.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "    \n",
    "class SurgicalSceneDataset(Dataset):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, seq_set, data_dir, img_dir, dset, dataconst, feature_extractor, reduce_size = False):\n",
    "        \n",
    "        \n",
    "        self.data_size = 143\n",
    "        self.dataconst = dataconst\n",
    "        self.img_dir = img_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.reduce_size = reduce_size\n",
    "        \n",
    "        self.xml_dir_list = []\n",
    "        self.dset = []\n",
    "        \n",
    "        for domain in range(len(seq_set)):\n",
    "            domain_dir_list = []\n",
    "            for i in seq_set[domain]:\n",
    "                xml_dir_temp = data_dir[domain] + str(i) + '/xml/'\n",
    "                domain_dir_list = domain_dir_list + glob(xml_dir_temp + '/*.xml')\n",
    "            if self.reduce_size:\n",
    "                indices = np.random.permutation(len(domain_dir_list))\n",
    "                domain_dir_list = [domain_dir_list[j] for j in indices[0:self.data_size]]\n",
    "            for file in domain_dir_list: \n",
    "                self.xml_dir_list.append(file)\n",
    "                self.dset.append(dset[domain])\n",
    "        self.word2vec = h5py.File('datasets/surgicalscene_word2vec.hdf5', 'r')\n",
    "    \n",
    "    # word2vec\n",
    "    def _get_word2vec(self,node_ids, sgh = 0):\n",
    "        word2vec = np.empty((0,300))\n",
    "        for node_id in node_ids:\n",
    "            if sgh == 1 and node_id == 0:\n",
    "                vec = self.word2vec['tissue']\n",
    "            else:\n",
    "                vec = self.word2vec[self.dataconst.instrument_classes[node_id]]\n",
    "            word2vec = np.vstack((word2vec, vec))\n",
    "        return word2vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xml_dir_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        file_name = os.path.splitext(os.path.basename(self.xml_dir_list[idx]))[0]\n",
    "        file_root = os.path.dirname(os.path.dirname(self.xml_dir_list[idx]))\n",
    "        if len(self.img_dir) == 1:\n",
    "            _img_loc = os.path.join(file_root+self.img_dir[0]+ file_name + '.png')\n",
    "        else:\n",
    "            _img_loc = os.path.join(file_root+self.img_dir[self.dset[idx]]+ file_name + '.png')\n",
    "        frame_data = h5py.File(os.path.join(file_root+'/vsgat/'+self.feature_extractor+'/'+ file_name + '_features.hdf5'), 'r')    \n",
    "        data = {}\n",
    "        data['img_name'] = frame_data['img_name'].value[:] + '.jpg'\n",
    "        data['img_loc'] = _img_loc\n",
    "        \n",
    "        data['node_num'] = frame_data['node_num'].value\n",
    "        data['roi_labels'] = frame_data['classes'][:]\n",
    "        data['det_boxes'] = frame_data['boxes'][:]\n",
    "        \n",
    "        \n",
    "        data['edge_labels'] = frame_data['edge_labels'][:]\n",
    "        data['edge_num'] = data['edge_labels'].shape[0]\n",
    "        \n",
    "        data['features'] = frame_data['node_features'][:]\n",
    "        data['spatial_feat'] = frame_data['spatial_features'][:]\n",
    "        \n",
    "        \n",
    "        data['word2vec'] = self._get_word2vec(data['roi_labels'], self.dset[idx])\n",
    "        return data\n",
    "\n",
    "# for DatasetLoader\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "        Default collate_fn(): https://github.com/pytorch/pytorch/blob/1d53d0756668ce641e4f109200d9c65b003d05fa/torch/utils/data/_utils/collate.py#L43\n",
    "    '''\n",
    "    batch_data = {}\n",
    "    batch_data['img_name'] = []\n",
    "    batch_data['img_loc'] = []\n",
    "    batch_data['node_num'] = []\n",
    "    batch_data['roi_labels'] = []\n",
    "    batch_data['det_boxes'] = []\n",
    "    batch_data['edge_labels'] = []\n",
    "    batch_data['edge_num'] = []\n",
    "    batch_data['features'] = []\n",
    "    batch_data['spatial_feat'] = []\n",
    "    batch_data['word2vec'] = []\n",
    "    \n",
    "    for data in batch:\n",
    "        batch_data['img_name'].append(data['img_name'])\n",
    "        batch_data['img_loc'].append(data['img_loc'])\n",
    "        batch_data['node_num'].append(data['node_num'])\n",
    "        batch_data['roi_labels'].append(data['roi_labels'])\n",
    "        batch_data['det_boxes'].append(data['det_boxes'])\n",
    "        batch_data['edge_labels'].append(data['edge_labels'])\n",
    "        batch_data['edge_num'].append(data['edge_num'])\n",
    "        batch_data['features'].append(data['features'])\n",
    "        batch_data['spatial_feat'].append(data['spatial_feat'])\n",
    "        batch_data['word2vec'].append(data['word2vec'])\n",
    "        \n",
    "    batch_data['edge_labels'] = torch.FloatTensor(np.concatenate(batch_data['edge_labels'], axis=0))\n",
    "    batch_data['features'] = torch.FloatTensor(np.concatenate(batch_data['features'], axis=0))\n",
    "    batch_data['spatial_feat'] = torch.FloatTensor(np.concatenate(batch_data['spatial_feat'], axis=0))\n",
    "    batch_data['word2vec'] = torch.FloatTensor(np.concatenate(batch_data['word2vec'], axis=0))\n",
    "    \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch as t\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plot\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def vis_img(img, node_classes, bboxs,  det_action, score_thresh = 0.7):\n",
    "    \n",
    "    Drawer = ImageDraw.Draw(img)\n",
    "    line_width = 3\n",
    "    outline = '#FF0000'\n",
    "    font = ImageFont.truetype(font='/usr/share/fonts/truetype/freefont/FreeMono.ttf', size=25)\n",
    "    \n",
    "    im_w,im_h = img.size\n",
    "    node_num = len(node_classes)\n",
    "    edge_num = len(det_action)\n",
    "    tissue_num = len(np.where(node_classes == 1)[0])\n",
    "    \n",
    "    for node in range(node_num):\n",
    "        \n",
    "        r_color = random.choice(np.arange(256))\n",
    "        g_color = random.choice(np.arange(256))\n",
    "        b_color = random.choice(np.arange(256))\n",
    "        \n",
    "        text = data_const.instrument_classes[node_classes[node]]\n",
    "        h, w = font.getsize(text)\n",
    "        Drawer.rectangle(list(bboxs[node]), outline=outline, width=line_width)\n",
    "        Drawer.text(xy=(bboxs[node][0], bboxs[node][1]-w-1), text=text, font=font, fill=(r_color,g_color,b_color))\n",
    "  \n",
    "    edge_idx = 0\n",
    "    \n",
    "    for tissue in range(tissue_num):\n",
    "        for instrument in range(tissue+1, node_num):\n",
    "            \n",
    "            #action_idx = np.where(det_action[edge_idx] > score_thresh)[0]\n",
    "            action_idx = np.argmax(det_action[edge_idx])\n",
    "#             print('det_action', det_action[edge_idx])\n",
    "#             print('action_idx',action_idx)\n",
    "            \n",
    "            text = data_const.action_classes[action_idx]\n",
    "            r_color = random.choice(np.arange(256))\n",
    "            g_color = random.choice(np.arange(256))\n",
    "            b_color = random.choice(np.arange(256))\n",
    "        \n",
    "            x1,y1,x2,y2 = bboxs[tissue]\n",
    "            x1_,y1_,x2_,y2_ = bboxs[instrument]\n",
    "            \n",
    "            c0 = int(0.5*x1)+int(0.5*x2)\n",
    "            c0 = max(0,min(c0,im_w-1))\n",
    "            r0 = int(0.5*y1)+int(0.5*y2)\n",
    "            r0 = max(0,min(r0,im_h-1))\n",
    "            c1 = int(0.5*x1_)+int(0.5*x2_)\n",
    "            c1 = max(0,min(c1,im_w-1))\n",
    "            r1 = int(0.5*y1_)+int(0.5*y2_)\n",
    "            r1 = max(0,min(r1,im_h-1))\n",
    "            Drawer.line(((c0,r0),(c1,r1)), fill=(r_color,g_color,b_color), width=3)\n",
    "            Drawer.text(xy=(c1, r1), text=text, font=font, fill=(r_color,g_color,b_color))\n",
    "\n",
    "            edge_idx +=1\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import utils.io as io\n",
    "#from utils.vis_tool import vis_img\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def run_model(args, data_const):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # use cpu or cuda\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.gpu else 'cpu')\n",
    "    print('training on {}...'.format(device))\n",
    "\n",
    "    # model\n",
    "    model = AGRNN(bias=args.bias, bn=args.bn, dropout=args.drop_prob, multi_attn=args.multi_attn, layer=args.layers, diff_edge=args.diff_edge, use_cbs = args.use_cbs)\n",
    "    if args.use_cbs: model.grnn1.gnn.apply_h_h_edge.get_new_kernels(0)\n",
    "    \n",
    "    # calculate the amount of all the learned parameters\n",
    "    parameter_num = 0\n",
    "    for param in model.parameters(): parameter_num += param.numel()\n",
    "    print(f'The parameters number of the model is {parameter_num / 1e6} million')\n",
    "\n",
    "    # load pretrained model\n",
    "    if args.pretrained:\n",
    "        print(f\"loading pretrained model {args.pretrained}\")\n",
    "        checkpoints = torch.load(args.pretrained, map_location=device)\n",
    "        model.load_state_dict(checkpoints['state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.3) #the scheduler divides the lr by 10 every 150 epochs\n",
    "\n",
    "    # get the configuration of the model and save some key configurations\n",
    "    io.mkdir_if_not_exists(os.path.join(args.save_dir, args.exp_ver), recursive=True)\n",
    "    for i in range(args.layers):\n",
    "        if i==0:\n",
    "            model_config = model.CONFIG1.save_config()\n",
    "            model_config['lr'] = args.lr\n",
    "            model_config['bs'] = args.batch_size\n",
    "            model_config['layers'] = args.layers\n",
    "            model_config['multi_attn'] = args.multi_attn\n",
    "            model_config['data_aug'] = args.data_aug\n",
    "            model_config['drop_out'] = args.drop_prob\n",
    "            model_config['optimizer'] = args.optim\n",
    "            model_config['diff_edge'] = args.diff_edge\n",
    "            model_config['model_parameters'] = parameter_num\n",
    "            io.dump_json_object(model_config, os.path.join(args.save_dir, args.exp_ver, 'l1_config.json'))\n",
    "    print('save key configurations successfully...')\n",
    "\n",
    "    # domain 1\n",
    "    train_seq = [[2,3,4,6,7,9,10,11,12,14,15]]\n",
    "    val_seq = [[1,5,16]]\n",
    "    data_dir = ['datasets/instruments18/seq_']\n",
    "    img_dir = ['/left_frames/']\n",
    "    dset = [0] # 0 for ISC, 1 for SGH\n",
    "    seq = {'train_seq': train_seq, 'val_seq': val_seq, 'data_dir': data_dir, 'img_dir':img_dir, 'dset': dset}\n",
    "    print('======================== Domain 1 ==============================')\n",
    "    epoch_train(args, model,seq, device, \"D1\")\n",
    "    \n",
    "#     # domain 2\n",
    "#     train_seq = [[2,3,4,6,7,9,10,11,12,14,15], [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]]\n",
    "#     val_seq = [[1,5,16],[16,17,18,19,20,21,22]]\n",
    "#     data_dir = ['datasets/instruments18/seq_', 'datasets/SGH_dataset_2020/']\n",
    "#     img_dir = ['/left_frames/', '/resized_frames/']\n",
    "#     dset = [0, 1]\n",
    "#     seq = {'train_seq': train_seq, 'val_seq': val_seq, 'data_dir': data_dir, 'img_dir':img_dir, 'dset': dset}\n",
    "#     print('======================== Domain 2 ==============================')\n",
    "#     epoch_train(args, model,seq, device, \"D2\")\n",
    "#     print('======================== Domain 1-2 FT =========================')\n",
    "#     epoch_train(args, model,seq, device, \"D2F\", finetune = True)\n",
    "    \n",
    "\n",
    "def epoch_train(args, model, seq, device, dname, finetune = False):\n",
    "    '''\n",
    "    input: model, dataloader, dataset, criterain, optimizer, scheduler, device, data_const\n",
    "    data: \n",
    "        img_name, node_num, roi_labels, det_boxes, edge_labels,\n",
    "        edge_num, features, spatial_features, word2vec\n",
    "    '''\n",
    "    \n",
    "    new_domain = False\n",
    "    stop_epoch = args.epoch\n",
    "    \n",
    "    if finetune:\n",
    "        stop_epoch = args.ft_epoch\n",
    "        train_dataset = SurgicalSceneDataset(seq_set = seq['train_seq'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = True)\n",
    "        val_dataset = SurgicalSceneDataset(seq_set = seq['val_seq'], dset = seq['dset'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "        dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "        model_old = None\n",
    "    \n",
    "    # train and test dataset for one domain\n",
    "    elif (len(seq['train_seq']) == 1):\n",
    "        # set up dataset variable\n",
    "        train_dataset = SurgicalSceneDataset(seq_set = seq['train_seq'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "        val_dataset = SurgicalSceneDataset(seq_set = seq['val_seq'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "        dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "        model_old = None\n",
    "   \n",
    "    # train and test for multiple domain\n",
    "    elif (len(seq['train_seq']) > 1):\n",
    "        # set up dataset variable\n",
    "        new_domain = True\n",
    "        curr_tr_seq = seq['train_seq'][len(seq['train_seq'])-1:]\n",
    "        curr_tr_data_dir = seq['data_dir'][len(seq['data_dir'])-1:]\n",
    "        curr_tr_img_dir = seq['img_dir'][len(seq['img_dir'])-1:]\n",
    "        curr_dset = seq['dset'][len(seq['dset'])-1:]\n",
    "        #print(curr_tr_seq, curr_tr_data_dir, curr_tr_img_dir, curr_dset)\n",
    "        train_dataset = SurgicalSceneDataset(seq_set = curr_tr_seq, data_dir = curr_tr_data_dir, \\\n",
    "                            img_dir = curr_tr_img_dir, dset = curr_dset, dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "        val_dataset = SurgicalSceneDataset(seq_set = seq['val_seq'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "        dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "        model_old = copy.deepcopy(model)\n",
    "    \n",
    "    # use default DataLoader() to load the data. \n",
    "    train_dataloader = DataLoader(dataset=dataset['train'], batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(dataset=dataset['val'], batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "    dataloader = {'train': train_dataloader, 'val': val_dataloader}\n",
    "    \n",
    "    # criterion and scheduler\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # set visualization and create folder to save checkpoints\n",
    "    writer = SummaryWriter(log_dir=args.log_dir + '/' + args.exp_ver + '/' + 'epoch_train')\n",
    "    io.mkdir_if_not_exists(os.path.join(args.save_dir, args.exp_ver, 'epoch_train'), recursive=True)\n",
    "\n",
    "    for epoch in range(args.start_epoch, stop_epoch):\n",
    "        \n",
    "        # each epoch has a training and validation step\n",
    "        epoch_acc = 0\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # finetune\n",
    "        if finetune:\n",
    "            train_dataset = SurgicalSceneDataset(seq_set = seq['train_seq'], data_dir = seq['data_dir'], \\\n",
    "                                img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                                feature_extractor = args.feature_extractor, reduce_size = True)\n",
    "            dataset['train'] = train_dataset\n",
    "            train_dataloader = DataLoader(dataset=dataset['train'], batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "            dataloader['train'] = train_dataloader\n",
    "\n",
    "        # build optimizer  \n",
    "        if finetune: lrc = args.lr / 10\n",
    "        else: lrc = args.lr\n",
    "        \n",
    "        if args.optim == 'sgd': \n",
    "            optimizer = optim.SGD(model.parameters(), lr= lrc, momentum=0.9, weight_decay=0)\n",
    "        else: \n",
    "            optimizer = optim.Adam(model.parameters(), lr= lrc, weight_decay=0)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            idx = 0\n",
    "            running_acc = 0.0\n",
    "            running_loss = 0.0\n",
    "            running_edge_count = 0\n",
    "            \n",
    "            if phase == 'train' and args.use_cbs:\n",
    "                model.grnn1.gnn.apply_h_h_edge.get_new_kernels(epoch)\n",
    "                model.to(device)\n",
    "            \n",
    "            #print(len(dataloader[phase]))\n",
    "            #for data in tqdm(dataloader[phase]):\n",
    "            for data in dataloader[phase]:\n",
    "                train_data = data\n",
    "                img_name = train_data['img_name']\n",
    "                img_loc = train_data['img_loc']\n",
    "                node_num = train_data['node_num']\n",
    "                roi_labels = train_data['roi_labels']\n",
    "                det_boxes = train_data['det_boxes']\n",
    "                edge_labels = train_data['edge_labels']\n",
    "                edge_num = train_data['edge_num']\n",
    "                features = train_data['features']\n",
    "                spatial_feat = train_data['spatial_feat']\n",
    "                word2vec = train_data['word2vec']\n",
    "                features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "                \n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                    model.zero_grad()\n",
    "                    outputs = model(node_num, features, spatial_feat, word2vec, roi_labels)\n",
    "                    \n",
    "                    # loss and accuracy\n",
    "                    if args.use_t: outputs = outputs / args.t_scale\n",
    "                    loss = criterion(outputs, edge_labels.float())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    acc = np.sum(np.equal(np.argmax(outputs.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    # turn off the gradients for validation, save memory and computations\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(node_num, features, spatial_feat, word2vec, roi_labels, validation=True)\n",
    "                        \n",
    "                        # loss and accuracy\n",
    "                        loss = criterion(outputs, edge_labels.float())\n",
    "                        acc = np.sum(np.equal(np.argmax(outputs.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "                    \n",
    "                        # print result every 1000 iteration during validation\n",
    "                        if idx == 10:\n",
    "                            #print(img_loc[0])\n",
    "                            io.mkdir_if_not_exists(os.path.join(args.output_img_dir, ('epoch_'+str(epoch))), recursive=True)\n",
    "                            image = Image.open(img_loc[0]).convert('RGB')\n",
    "                            det_actions = nn.Sigmoid()(outputs[0:int(edge_num[0])])\n",
    "                            det_actions = det_actions.cpu().detach().numpy()\n",
    "                            action_img = vis_img(image, roi_labels[0], det_boxes[0],  det_actions, score_thresh = 0.7)\n",
    "                            image = image.save(os.path.join(args.output_img_dir, ('epoch_'+str(epoch)),img_name[0]))\n",
    "\n",
    "                idx+=1\n",
    "                # accumulate loss of each batch\n",
    "                running_loss += loss.item() * edge_labels.shape[0]\n",
    "                running_acc += acc\n",
    "                running_edge_count += edge_labels.shape[0]\n",
    "            \n",
    "            # distillation learning\n",
    "            if phase == 'train' and new_domain:\n",
    "                \n",
    "                # distillation loss activation\n",
    "                dist_loss_act = nn.Softmax(dim=1)\n",
    "                dist_loss_act = dist_loss_act.to(device)\n",
    "            \n",
    "                dis_seq = seq['train_seq'][:-1]\n",
    "                dis_data_dir = seq['data_dir'][:-1]\n",
    "                dis_img_dir = seq['img_dir'][:-1]\n",
    "                dis_dset = seq['dset'][:-1]\n",
    "                dis_train_dataset = SurgicalSceneDataset(seq_set =  dis_seq, data_dir = dis_data_dir, \\\n",
    "                                        img_dir = dis_img_dir, dset = dis_dset, dataconst = data_const, \\\n",
    "                                        feature_extractor = args.feature_extractor, reduce_size = True)\n",
    "                dis_train_dataloader = DataLoader(dataset=dis_train_dataset, batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "                \n",
    "#                 if args.use_cbs:\n",
    "#                     model_old.grnn1.gnn.apply_h_h_edge.get_new_kernels(epoch)\n",
    "#                     model_old.to(device)\n",
    "        \n",
    "                #print(len(dis_train_dataloader))\n",
    "                #for data in tqdm(dataloader[phase]):\n",
    "                for data in dis_train_dataloader:\n",
    "                    train_data = data\n",
    "                    img_name = train_data['img_name']\n",
    "                    img_loc = train_data['img_loc']\n",
    "                    node_num = train_data['node_num']\n",
    "                    roi_labels = train_data['roi_labels']\n",
    "                    det_boxes = train_data['det_boxes']\n",
    "                    edge_labels = train_data['edge_labels']\n",
    "                    edge_num = train_data['edge_num']\n",
    "                    features = train_data['features']\n",
    "                    spatial_feat = train_data['spatial_feat']\n",
    "                    word2vec = train_data['word2vec']\n",
    "                    features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "                    \n",
    "                    model.train()\n",
    "                    model_old.train()\n",
    "                    model.zero_grad()\n",
    "                    outputs = model(node_num, features, spatial_feat, word2vec, roi_labels)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # old network output\n",
    "                        output_old = model_old(node_num, features, spatial_feat, word2vec, roi_labels)\n",
    "                        output_old = Variable(output_old, requires_grad=False)\n",
    "                    \n",
    "                    if args.use_t:\n",
    "                        outputs = outputs/args.t_scale\n",
    "                        output_old = output_old/args.t_scale\n",
    "                    d_loss = F.binary_cross_entropy(dist_loss_act(outputs), dist_loss_act(output_old))\n",
    "                    loss = criterion(outputs, edge_labels.float()) + 0.5* d_loss\n",
    "                    \n",
    "                    # loss and accuracy\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # calculate the loss and accuracy of each epoch\n",
    "            epoch_loss = running_loss / len(dataset[phase])\n",
    "            epoch_acc = running_acc / running_edge_count\n",
    "            \n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            # log trainval datas, and visualize them in the same graph\n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss \n",
    "            else:\n",
    "                writer.add_scalars('trainval_loss_epoch', {'train': train_loss, 'val': epoch_loss}, epoch)\n",
    "            \n",
    "            # print data\n",
    "            if (epoch % args.print_every) == 0:\n",
    "                end_time = time.time()\n",
    "                print(\"[{}] Epoch: {}/{} Acc: {:0.6f} Loss: {:0.6f} Execution time: {:0.6f}\".format(\\\n",
    "                        phase, epoch+1, args.epoch, epoch_acc, epoch_loss, (end_time-start_time)))\n",
    "                        \n",
    "        # scheduler.step()\n",
    "        # save model\n",
    "        if epoch_loss<0.0405 or epoch % args.save_every == (args.save_every - 1) and epoch >= (20-1):\n",
    "            checkpoint = { \n",
    "                            'lr': args.lr,\n",
    "                           'b_s': args.batch_size,\n",
    "                          'bias': args.bias, \n",
    "                            'bn': args.bn, \n",
    "                       'dropout': args.drop_prob,\n",
    "                        'layers': args.layers,\n",
    "                    'multi_head': args.multi_attn,\n",
    "                     'diff_edge': args.diff_edge,\n",
    "                    'state_dict': model.state_dict()\n",
    "            }\n",
    "            save_name = \"checkpoint_\" + dname + str(epoch+1) + '_epoch.pth'\n",
    "            torch.save(checkpoint, os.path.join(args.save_dir, args.exp_ver, 'epoch_train', save_name))\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18_09_cbs_ts\n",
      "training on cuda...\n",
      "The parameters number of the model is 2.393694 million\n",
      "save key configurations successfully...\n",
      "======================== Domain 1 ==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Epoch: 1/251 Acc: 0.053662 Loss: 3.177348 Execution time: 5.920356\n",
      "[val] Epoch: 1/251 Acc: 0.003445 Loss: 1.689490 Execution time: 1.662262\n",
      "[train] Epoch: 11/251 Acc: 0.290046 Loss: 0.991574 Execution time: 6.419871\n",
      "[val] Epoch: 11/251 Acc: 0.412575 Loss: 0.618523 Execution time: 1.758457\n",
      "[train] Epoch: 21/251 Acc: 0.309364 Loss: 0.880009 Execution time: 7.309571\n",
      "[val] Epoch: 21/251 Acc: 0.417743 Loss: 0.498531 Execution time: 1.909256\n",
      "[train] Epoch: 31/251 Acc: 0.309901 Loss: 0.826830 Execution time: 7.366508\n",
      "[val] Epoch: 31/251 Acc: 0.479759 Loss: 0.462508 Execution time: 1.979013\n",
      "[train] Epoch: 41/251 Acc: 0.340757 Loss: 0.800426 Execution time: 8.746249\n",
      "[val] Epoch: 41/251 Acc: 0.546081 Loss: 0.439276 Execution time: 2.009312\n",
      "[train] Epoch: 51/251 Acc: 0.343171 Loss: 0.789249 Execution time: 7.362647\n",
      "[val] Epoch: 51/251 Acc: 0.533161 Loss: 0.436872 Execution time: 1.997704\n",
      "[train] Epoch: 61/251 Acc: 0.369198 Loss: 0.776392 Execution time: 7.275374\n",
      "[val] Epoch: 61/251 Acc: 0.530577 Loss: 0.435413 Execution time: 1.976511\n",
      "[train] Epoch: 71/251 Acc: 0.387711 Loss: 0.761825 Execution time: 7.086827\n",
      "[val] Epoch: 71/251 Acc: 0.570198 Loss: 0.428499 Execution time: 1.867456\n",
      "[train] Epoch: 81/251 Acc: 0.411323 Loss: 0.748270 Execution time: 7.458228\n",
      "[val] Epoch: 81/251 Acc: 0.525409 Loss: 0.426229 Execution time: 2.088957\n",
      "[train] Epoch: 91/251 Acc: 0.421519 Loss: 0.744571 Execution time: 7.220492\n",
      "[val] Epoch: 91/251 Acc: 0.523686 Loss: 0.427921 Execution time: 1.967058\n",
      "[train] Epoch: 101/251 Acc: 0.449155 Loss: 0.732026 Execution time: 7.933455\n",
      "[val] Epoch: 101/251 Acc: 0.570198 Loss: 0.421793 Execution time: 2.075140\n",
      "[train] Epoch: 111/251 Acc: 0.450496 Loss: 0.730834 Execution time: 7.023030\n",
      "[val] Epoch: 111/251 Acc: 0.610680 Loss: 0.414705 Execution time: 1.729391\n",
      "[train] Epoch: 121/251 Acc: 0.462570 Loss: 0.725684 Execution time: 8.050085\n",
      "[val] Epoch: 121/251 Acc: 0.594315 Loss: 0.421670 Execution time: 2.290944\n",
      "[train] Epoch: 131/251 Acc: 0.460692 Loss: 0.717948 Execution time: 8.093540\n",
      "[val] Epoch: 131/251 Acc: 0.605512 Loss: 0.417701 Execution time: 2.523910\n",
      "[train] Epoch: 141/251 Acc: 0.473840 Loss: 0.706459 Execution time: 7.456261\n",
      "[val] Epoch: 141/251 Acc: 0.603790 Loss: 0.417786 Execution time: 2.005353\n",
      "[train] Epoch: 151/251 Acc: 0.492621 Loss: 0.712720 Execution time: 7.658226\n",
      "[val] Epoch: 151/251 Acc: 0.607235 Loss: 0.416152 Execution time: 2.051483\n",
      "[train] Epoch: 161/251 Acc: 0.493963 Loss: 0.707173 Execution time: 7.249943\n",
      "[val] Epoch: 161/251 Acc: 0.621016 Loss: 0.412389 Execution time: 2.143899\n",
      "[train] Epoch: 171/251 Acc: 0.495036 Loss: 0.702786 Execution time: 7.268732\n",
      "[val] Epoch: 171/251 Acc: 0.608958 Loss: 0.415255 Execution time: 1.893324\n",
      "[train] Epoch: 181/251 Acc: 0.497988 Loss: 0.704417 Execution time: 7.405298\n",
      "[val] Epoch: 181/251 Acc: 0.629630 Loss: 0.408495 Execution time: 2.025555\n",
      "[train] Epoch: 191/251 Acc: 0.525356 Loss: 0.700760 Execution time: 7.584368\n",
      "[val] Epoch: 191/251 Acc: 0.623600 Loss: 0.411658 Execution time: 2.023137\n",
      "[train] Epoch: 201/251 Acc: 0.525087 Loss: 0.696383 Execution time: 7.522167\n",
      "[val] Epoch: 201/251 Acc: 0.630491 Loss: 0.406063 Execution time: 1.861248\n",
      "[train] Epoch: 211/251 Acc: 0.525892 Loss: 0.693353 Execution time: 6.496179\n",
      "[val] Epoch: 211/251 Acc: 0.637382 Loss: 0.406232 Execution time: 2.306007\n",
      "[train] Epoch: 221/251 Acc: 0.534478 Loss: 0.688895 Execution time: 7.696420\n",
      "[val] Epoch: 221/251 Acc: 0.615848 Loss: 0.403541 Execution time: 1.961307\n",
      "[train] Epoch: 231/251 Acc: 0.532868 Loss: 0.689146 Execution time: 6.886551\n",
      "[val] Epoch: 231/251 Acc: 0.621878 Loss: 0.410194 Execution time: 2.088380\n",
      "[train] Epoch: 241/251 Acc: 0.551918 Loss: 0.685940 Execution time: 6.139376\n",
      "[val] Epoch: 241/251 Acc: 0.623600 Loss: 0.409576 Execution time: 1.699386\n",
      "[train] Epoch: 251/251 Acc: 0.560236 Loss: 0.680920 Execution time: 6.156538\n",
      "[val] Epoch: 251/251 Acc: 0.631352 Loss: 0.408420 Execution time: 1.672801\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed=27):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    seed_everything()\n",
    "    args = arguments()\n",
    "    print(args.feature_extractor)\n",
    "    data_const = SurgicalSceneConstants()\n",
    "    run_model(args, data_const)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
