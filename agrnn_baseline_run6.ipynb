{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "arguments\n",
    "    Hyperparameters, file location, optimizer, network, data_processing\n",
    "'''\n",
    "\n",
    "class arguments():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # hyper parameters\n",
    "        self.lr = 0.00001\n",
    "        self.epoch = 351 \n",
    "        self.start_epoch = 0\n",
    "        self.batch_size = 32\n",
    "        self.gpu = True\n",
    "        self.print_every = 10 \n",
    "        self.train_model = 'epoch'\n",
    "        self.exp_ver='g_ecbs_resnet18_11_ts_ls'\n",
    "\n",
    "        # file locations\n",
    "        self.log_dir = './log/g_ecbs_resnet18_11_ts_ls'\n",
    "        self.save_dir = './checkpoints/g_ecbs_resnet18_11_ts_ls'\n",
    "        self.output_img_dir = './results/g_ecbs_resnet18_11_ts_ls'\n",
    "        self.save_every = 10\n",
    "        self.pretrained = None                 \n",
    "\n",
    "        # optimizer\n",
    "        self.optim='adam' # choices=['sgd', 'adam']\n",
    "\n",
    "        # network\n",
    "        self.layers= 1\n",
    "        self.bn = False\n",
    "        self.drop_prob = 0.3\n",
    "        self.bias = True\n",
    "        self.multi_attn = False\n",
    "        self.diff_edge = False\n",
    "\n",
    "        # data_processing\n",
    "        self.sampler = 0\n",
    "        self.data_aug = False\n",
    "        self.feature_extractor = 'resnet18_11_ts_ls'\n",
    "        \n",
    "        # CBS\n",
    "        self.use_cbs = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "configurations of the network\n",
    "    \n",
    "    readout: G_ER_L_S = [1024+300+16+300+1024,  1024, 117]\n",
    "\n",
    "    node_func: G_N_L_S = [1024+1024, 1024]\n",
    "    node_lang_func: G_N_L_S2 = [300+300+300]\n",
    "    \n",
    "    edge_func : G_E_L_S = [1024*2+16, 1024]\n",
    "    edge_lang_func: [300*2, 1024]\n",
    "    \n",
    "    attn: [1024, 1]\n",
    "    attn_lang: [1024, 1]\n",
    "'''\n",
    "\n",
    "class CONFIGURATION(object):\n",
    "    '''\n",
    "    Configuration arguments: feature type, layer, bias, batch normalization, dropout, multi-attn\n",
    "    \n",
    "    readout           : fc_size, activation, bias, bn, droupout\n",
    "    gnn_node          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_node_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    gnn_edge          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_edge_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    gnn_attn          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_attn_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    '''\n",
    "    def __init__(self, layer=1, bias=True, bn=False, dropout=0.2, multi_attn=False):\n",
    "        \n",
    "        # if multi_attn:\n",
    "        if True:\n",
    "            if layer==1:\n",
    "                feature_size = 512\n",
    "                # readout\n",
    "                self.G_ER_L_S = [feature_size+300+16+300+feature_size, feature_size, 13]\n",
    "                self.G_ER_A   = ['ReLU', 'Identity']\n",
    "                self.G_ER_B   = bias    #true\n",
    "                self.G_ER_BN  = bn      #false\n",
    "                self.G_ER_D   = dropout #0.3\n",
    "                # self.G_ER_GRU = feature_size\n",
    "\n",
    "                # # gnn node function\n",
    "                self.G_N_L_S = [feature_size+feature_size, feature_size]\n",
    "                self.G_N_A   = ['ReLU']\n",
    "                self.G_N_B   = bias #true\n",
    "                self.G_N_BN  = bn      #false\n",
    "                self.G_N_D   = dropout #0.3\n",
    "                # self.G_N_GRU = feature_size\n",
    "\n",
    "                # # gnn node function for language\n",
    "                self.G_N_L_S2 = [300+300, 300]\n",
    "                self.G_N_A2   = ['ReLU']\n",
    "                self.G_N_B2   = bias    #true\n",
    "                self.G_N_BN2  = bn      #false\n",
    "                self.G_N_D2   = dropout #0.3\n",
    "                # self.G_N_GRU2 = feature_size\n",
    "\n",
    "                # gnn edge function1\n",
    "                self.G_E_L_S           = [feature_size*2+16, feature_size]\n",
    "                self.G_E_A             = ['ReLU']\n",
    "                self.G_E_B             = bias     #true\n",
    "                self.G_E_BN            = bn       #false\n",
    "                self.G_E_D             = dropout  #0.3\n",
    "                self.G_E_c_std         = 1.0\n",
    "                self.G_E_c_std_factor  = 0.985    # 0.985 (LOG), 0.95 (gau)\n",
    "                self.G_E_c_epoch       = 20\n",
    "                self.G_E_c_kernel_size = 3\n",
    "                self.G_E_c_filter      = 'LOG' # 'gau', 'LOG'\n",
    "\n",
    "                # gnn edge function2 for language\n",
    "                self.G_E_L_S2 = [300*2, feature_size]\n",
    "                self.G_E_A2   = ['ReLU']\n",
    "                self.G_E_B2   = bias     #true\n",
    "                self.G_E_BN2  = bn       #false\n",
    "                self.G_E_D2   = dropout  #0.3\n",
    "\n",
    "                # gnn attention mechanism\n",
    "                self.G_A_L_S = [feature_size, 1]\n",
    "                self.G_A_A   = ['LeakyReLU']\n",
    "                self.G_A_B   = bias     #true\n",
    "                self.G_A_BN  = bn       #false\n",
    "                self.G_A_D   = dropout  #0.3\n",
    "\n",
    "                # gnn attention mechanism2 for language\n",
    "                self.G_A_L_S2 = [feature_size, 1]\n",
    "                self.G_A_A2   = ['LeakyReLU']\n",
    "                self.G_A_B2   = bias    #true\n",
    "                self.G_A_BN2  = bn      #false\n",
    "                self.G_A_D2   = dropout #0.3\n",
    "                    \n",
    "    def save_config(self):\n",
    "        model_config = {'graph_head':{}, 'graph_node':{}, 'graph_edge':{}, 'graph_attn':{}}\n",
    "        CONFIG=self.__dict__\n",
    "        for k, v in CONFIG.items():\n",
    "            if 'G_H' in k:\n",
    "                model_config['graph_head'][k]=v\n",
    "            elif 'G_N' in k:\n",
    "                model_config['graph_node'][k]=v\n",
    "            elif 'G_E' in k:\n",
    "                model_config['graph_edge'][k]=v\n",
    "            elif 'G_A' in k:\n",
    "                model_config['graph_attn'][k]=v\n",
    "            else:\n",
    "                model_config[k]=v\n",
    "        \n",
    "        return model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_gaussian_filter_1D(kernel_size=3, sigma=2, channels=3):\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    \n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "    mean = (kernel_size - 1)/2.\n",
    "    variance = sigma**2.\n",
    "    xy_grid = torch.sum((xy_grid[:kernel_size,:kernel_size,:] - mean)**2., dim=-1)\n",
    "\n",
    "    # Calculate the 1-dimensional gaussian kernel\n",
    "    gaussian_kernel = (1./((math.sqrt(2.*math.pi)*sigma))) * \\\n",
    "                        torch.exp(-1* (xy_grid[int(kernel_size/2)]) / (2*variance))\n",
    "\n",
    "    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size)\n",
    "    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1)\n",
    "\n",
    "    padding = 1 if kernel_size==3 else 2 if kernel_size == 5 else 0\n",
    "    gaussian_filter = nn.Conv1d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels,\n",
    "                                bias=False, padding=padding)\n",
    "    gaussian_filter.weight.data = gaussian_kernel\n",
    "    gaussian_filter.weight.requires_grad = False \n",
    "    return gaussian_filter\n",
    "\n",
    "def get_laplaceOfGaussian_filter_1D(kernel_size=3, sigma=2, channels=3):\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "    mean = (kernel_size - 1)/2.\n",
    "\n",
    "    used_sigma = sigma\n",
    "    # Calculate the 2-dimensional gaussian kernel which is\n",
    "    log_kernel = (-1./(math.pi*(used_sigma**4))) \\\n",
    "                  * (1-(torch.sum((xy_grid[int(kernel_size/2)] - mean)**2., dim=-1) / (2*(used_sigma**2)))) \\\n",
    "                  * torch.exp(-torch.sum((xy_grid[int(kernel_size/2)] - mean)**2., dim=-1) / (2*(used_sigma**2)))\n",
    "    \n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    log_kernel = log_kernel / torch.sum(log_kernel)\n",
    "    log_kernel = log_kernel.view(1, 1, kernel_size)\n",
    "    log_kernel = log_kernel.repeat(channels, 1, 1)\n",
    "\n",
    "    padding = 1 if kernel_size==3 else 2 if kernel_size == 5 else 0\n",
    "    log_filter = nn.Conv1d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels,\n",
    "                                bias=False, padding=padding)\n",
    "    log_filter.weight.data = log_kernel\n",
    "    log_filter.weight.requires_grad = False\n",
    "    return log_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primary activation and MLP layer\n",
    "acivation:\n",
    "    Identity\n",
    "    ReLU\n",
    "    LeakyReLU\n",
    "MLP:\n",
    "    init: layer size, activation, bias, use_BN, dropout_probability\n",
    "    forward: x\n",
    "'''\n",
    "\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    '''\n",
    "    Identity class activation layer\n",
    "    x = x\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Identity,self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def get_activation(name):\n",
    "    '''\n",
    "    get_activation sub-function\n",
    "    argument: activatoin name (eg. ReLU, Identity, LeakyReLU)\n",
    "    '''\n",
    "    if name=='ReLU': return nn.ReLU(inplace=True)\n",
    "    elif name=='Identity': return Identity()\n",
    "    elif name=='LeakyReLU': return nn.LeakyReLU(0.2,inplace=True)\n",
    "    else: assert(False), 'Not Implemented'\n",
    "    #elif name=='Tanh': return nn.Tanh()\n",
    "    #elif name=='Sigmoid': return nn.Sigmoid()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Args:\n",
    "        layer_sizes: a list, [1024,1024,...]\n",
    "        activation: a list, ['ReLU', 'Tanh',...]\n",
    "        bias : bool\n",
    "        use_bn: bool\n",
    "        drop_prob: default is None, use drop out layer or not\n",
    "    '''\n",
    "    def __init__(self, layer_sizes, activation, bias=True, use_bn=False, drop_prob=None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.bn = use_bn\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=bias)\n",
    "            activate = get_activation(activation[i])\n",
    "            block = nn.Sequential(OrderedDict([(f'L{i}', layer), ]))\n",
    "            \n",
    "            # !NOTE:# Actually, it is inappropriate to use batch-normalization here\n",
    "            if use_bn:                                  \n",
    "                bn = nn.BatchNorm1d(layer_sizes[i+1])\n",
    "                block.add_module(f'B{i}', bn)\n",
    "            \n",
    "            # batch normalization is put before activation function \n",
    "            block.add_module(f'A{i}', activate)\n",
    "\n",
    "            # dropout probablility\n",
    "            if drop_prob:\n",
    "                block.add_module(f'D{i}', nn.Dropout(drop_prob))\n",
    "            \n",
    "            self.layers.append(block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # !NOTE: sometime the shape of x will be [1,N], and we cannot use batch-normailzation in that situation\n",
    "            if self.bn and x.shape[0]==1:\n",
    "                x = layer[0](x)\n",
    "                x = layer[:-1](x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "H_H_EdgeApplyModule\n",
    "    init    : config, multi_attn \n",
    "    forward : edge\n",
    "    \n",
    "H_NodeApplyModule\n",
    "    init    : config\n",
    "    forward : node\n",
    "    \n",
    "E_AttentionModule1\n",
    "    init    : config\n",
    "    forward : edge\n",
    "    \n",
    "GNN\n",
    "    init    : config, multi_attn, diff_edge\n",
    "    forward : g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_features\n",
    "    \n",
    "GRNN\n",
    "    init    : config, multi_attn, diff_edge\n",
    "    forward : b_graph, b_h_node_list, b_o_node_list, b_h_h_e_list, b_o_o_e_list, b_h_o_e_list, features, spatial_features, word2vec, valid, pop_features, initial_features\n",
    "'''\n",
    "\n",
    "import ipdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class H_H_EdgeApplyModule(nn.Module): #human to human edge\n",
    "    '''\n",
    "        init    : config, multi_attn \n",
    "        forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, use_cbs = False):\n",
    "        super(H_H_EdgeApplyModule, self).__init__()\n",
    "        if use_cbs:\n",
    "            self.use_cbs = use_cbs\n",
    "            self.cbs_std = CONFIG.G_E_c_std\n",
    "            self.cbs_std_factor = CONFIG.G_E_c_std_factor\n",
    "            self.cbs_epoch = CONFIG.G_E_c_epoch\n",
    "            self.cbs_kernel_size = CONFIG.G_E_c_kernel_size\n",
    "            self.cbs_filter = CONFIG.G_E_c_filter\n",
    "        \n",
    "        self.edge_fc = MLP(CONFIG.G_E_L_S, CONFIG.G_E_A, CONFIG.G_E_B, CONFIG.G_E_BN, CONFIG.G_E_D)\n",
    "        self.edge_fc_lang = MLP(CONFIG.G_E_L_S2, CONFIG.G_E_A2, CONFIG.G_E_B2, CONFIG.G_E_BN2, CONFIG.G_E_D2)\n",
    "    \n",
    "    def forward(self, edge):\n",
    "        feat = torch.cat([edge.src['n_f'], edge.data['s_f'], edge.dst['n_f']], dim=1)\n",
    "        feat_lang = torch.cat([edge.src['word2vec'], edge.dst['word2vec']], dim=1)\n",
    "        if self.use_cbs:\n",
    "            feat = self.kernel1(feat[:,None,:])\n",
    "            feat = torch.squeeze(feat, 1)\n",
    "        e_feat = self.edge_fc(feat)\n",
    "        e_feat_lang = self.edge_fc_lang(feat_lang)\n",
    "  \n",
    "        return {'e_f': e_feat, 'e_f_lang': e_feat_lang}\n",
    "\n",
    "    def get_new_kernels(self, epoch_count):\n",
    "        if self.use_cbs:\n",
    "            if epoch_count % self.cbs_epoch == 0 and epoch_count is not 0:\n",
    "                self.cbs_std *= self.cbs_std_factor\n",
    "            \n",
    "            if (self.cbs_filter == 'gau'): \n",
    "                self.kernel1 = get_gaussian_filter_1D(kernel_size=self.cbs_kernel_size, sigma= self.cbs_std, channels= 1)\n",
    "            elif (self.cbs_filter == 'LOG'): \n",
    "                self.kernel1 = get_laplaceOfGaussian_filter_1D(kernel_size=self.cbs_kernel_size, sigma= self.cbs_std, channels= 1)\n",
    "\n",
    "class H_NodeApplyModule(nn.Module): #human node\n",
    "    '''\n",
    "        init    : config\n",
    "        forward : node\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(H_NodeApplyModule, self).__init__()\n",
    "        self.node_fc = MLP(CONFIG.G_N_L_S, CONFIG.G_N_A, CONFIG.G_N_B, CONFIG.G_N_BN, CONFIG.G_N_D)\n",
    "        self.node_fc_lang = MLP(CONFIG.G_N_L_S2, CONFIG.G_N_A2, CONFIG.G_N_B2, CONFIG.G_N_BN2, CONFIG.G_N_D2)\n",
    "    \n",
    "    def forward(self, node):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        feat = torch.cat([node.data['n_f'], node.data['z_f']], dim=1)\n",
    "        feat_lang = torch.cat([node.data['word2vec'], node.data['z_f_lang']], dim=1)\n",
    "        n_feat = self.node_fc(feat)\n",
    "        n_feat_lang = self.node_fc_lang(feat_lang)\n",
    "\n",
    "        return {'new_n_f': n_feat, 'new_n_f_lang': n_feat_lang}\n",
    "\n",
    "class E_AttentionModule1(nn.Module): #edge attention\n",
    "    '''\n",
    "        init    : config\n",
    "        forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(E_AttentionModule1, self).__init__()\n",
    "        self.attn_fc = MLP(CONFIG.G_A_L_S, CONFIG.G_A_A, CONFIG.G_A_B, CONFIG.G_A_BN, CONFIG.G_A_D)\n",
    "        self.attn_fc_lang = MLP(CONFIG.G_A_L_S2, CONFIG.G_A_A2, CONFIG.G_A_B2, CONFIG.G_A_BN2, CONFIG.G_A_D2)\n",
    "\n",
    "    def forward(self, edge):\n",
    "        a_feat = self.attn_fc(edge.data['e_f'])\n",
    "        a_feat_lang = self.attn_fc_lang(edge.data['e_f_lang'])\n",
    "        return {'a_feat': a_feat, 'a_feat_lang': a_feat_lang}\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    '''\n",
    "        init    : config, multi_attn, diff_edge\n",
    "        forward : g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_features\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, diff_edge=True, use_cbs = False):\n",
    "        super(GNN, self).__init__()\n",
    "        self.diff_edge = diff_edge # false\n",
    "        self.apply_h_h_edge = H_H_EdgeApplyModule(CONFIG, multi_attn, use_cbs)\n",
    "        self.apply_edge_attn1 = E_AttentionModule1(CONFIG)  \n",
    "        self.apply_h_node = H_NodeApplyModule(CONFIG)\n",
    "\n",
    "    def _message_func(self, edges):\n",
    "        return {'nei_n_f': edges.src['n_f'], 'nei_n_w': edges.src['word2vec'], 'e_f': edges.data['e_f'], 'e_f_lang': edges.data['e_f_lang'], 'a_feat': edges.data['a_feat'], 'a_feat_lang': edges.data['a_feat_lang']}\n",
    "\n",
    "    def _reduce_func(self, nodes):\n",
    "        alpha = F.softmax(nodes.mailbox['a_feat'], dim=1)\n",
    "        alpha_lang = F.softmax(nodes.mailbox['a_feat_lang'], dim=1)\n",
    "\n",
    "        z_raw_f = nodes.mailbox['nei_n_f']+nodes.mailbox['e_f']\n",
    "        z_f = torch.sum( alpha * z_raw_f, dim=1)\n",
    "\n",
    "        z_raw_f_lang = nodes.mailbox['nei_n_w']\n",
    "        z_f_lang = torch.sum(alpha_lang * z_raw_f_lang, dim=1)\n",
    "         \n",
    "        # we cannot return 'alpha' for the different dimension \n",
    "        if self.training or validation: return {'z_f': z_f, 'z_f_lang': z_f_lang}\n",
    "        else: return {'z_f': z_f, 'z_f_lang': z_f_lang, 'alpha': alpha, 'alpha_lang': alpha_lang}\n",
    "\n",
    "    def forward(self, g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_feat=False):\n",
    "        \n",
    "        g.apply_edges(self.apply_h_h_edge, g.edges())\n",
    "        g.apply_edges(self.apply_edge_attn1)\n",
    "        g.update_all(self._message_func, self._reduce_func)\n",
    "        g.apply_nodes(self.apply_h_node, h_node+o_node)\n",
    "\n",
    "        # !NOTE:PAY ATTENTION WHEN ADDING MORE FEATURE\n",
    "        g.ndata.pop('n_f')\n",
    "        g.ndata.pop('word2vec')\n",
    "\n",
    "        g.ndata.pop('z_f')\n",
    "        g.edata.pop('e_f')\n",
    "        g.edata.pop('a_feat')\n",
    "\n",
    "        g.ndata.pop('z_f_lang')\n",
    "        g.edata.pop('e_f_lang')\n",
    "        g.edata.pop('a_feat_lang')\n",
    "\n",
    "class GRNN(nn.Module):\n",
    "    '''\n",
    "    init: \n",
    "        config, multi_attn, diff_edge\n",
    "    forward: \n",
    "        batch_graph, batch_h_node_list, batch_obj_node_list,\n",
    "        batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list,\n",
    "        features, spatial_features, word2vec,\n",
    "        valid, pop_features, initial_features\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, diff_edge=True, use_cbs = False):\n",
    "        super(GRNN, self).__init__()\n",
    "        self.multi_attn = multi_attn #false\n",
    "        self.gnn = GNN(CONFIG, multi_attn, diff_edge, use_cbs)\n",
    "\n",
    "    def forward(self, batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec, valid=False, pop_feat=False, initial_feat=False):\n",
    "        \n",
    "        # !NOTE: if node_num==1, there will be something wrong to forward the attention mechanism\n",
    "        global validation \n",
    "        validation = valid\n",
    "\n",
    "        # initialize the graph with some datas\n",
    "        batch_graph.ndata['n_f'] = feat           # node: features \n",
    "        batch_graph.ndata['word2vec'] = word2vec  # node: words\n",
    "        batch_graph.edata['s_f'] = spatial_feat   # edge: spatial features\n",
    "\n",
    "        try:\n",
    "            self.gnn(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            ipdb.set_trace()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Predictor \n",
    "    init    : config\n",
    "    forward : edge\n",
    "\n",
    "AGRNN\n",
    "    init    : bias, bn, dropout, multi_attn, layer, diff_edge\n",
    "    forward : node_num, feat, spatial_feat, word2vec, roi_label, validation, choose_nodes, remove_nodes\n",
    "'''\n",
    "\n",
    "import dgl\n",
    "import ipdb\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchvision\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    '''\n",
    "    init    : config\n",
    "    forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.classifier = MLP(CONFIG.G_ER_L_S, CONFIG.G_ER_A, CONFIG.G_ER_B, CONFIG.G_ER_BN, CONFIG.G_ER_D)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, edge):\n",
    "        feat = torch.cat([edge.dst['new_n_f'], edge.dst['new_n_f_lang'], edge.data['s_f'], edge.src['new_n_f_lang'], edge.src['new_n_f']], dim=1)\n",
    "        pred = self.classifier(feat)\n",
    "        # if the criterion is BCELoss, you need to uncomment the following code\n",
    "        # output = self.sigmoid(output)\n",
    "        return {'pred': pred}\n",
    "\n",
    "class AGRNN(nn.Module):\n",
    "    '''\n",
    "    init    : \n",
    "        feature_type, bias, bn, dropout, multi_attn, layer, diff_edge\n",
    "        \n",
    "    forward : \n",
    "        node_num, features, spatial_features, word2vec, roi_label,\n",
    "        validation, choose_nodes, remove_nodes\n",
    "    '''\n",
    "    def __init__(self, bias=True, bn=True, dropout=None, multi_attn=False, layer=1, diff_edge=True, use_cbs = False):\n",
    "        super(AGRNN, self).__init__()\n",
    " \n",
    "        self.multi_attn = multi_attn # false\n",
    "        self.layer = layer           # 1 layer\n",
    "        self.diff_edge = diff_edge   # false\n",
    "        \n",
    "        self.CONFIG1 = CONFIGURATION(layer=1, bias=bias, bn=bn, dropout=dropout, multi_attn=multi_attn)\n",
    "\n",
    "        self.grnn1 = GRNN(self.CONFIG1, multi_attn=multi_attn, diff_edge=diff_edge, use_cbs = use_cbs)\n",
    "        self.edge_readout = Predictor(self.CONFIG1)\n",
    "        \n",
    "    def _collect_edge(self, node_num, roi_label, node_space, diff_edge):\n",
    "        '''\n",
    "        arguments: node_num, roi_label, node_space, diff_edge\n",
    "        '''\n",
    "        \n",
    "        # get human nodes && object nodes\n",
    "        h_node_list = np.where(roi_label == 0)[0]\n",
    "        obj_node_list = np.where(roi_label != 0)[0]\n",
    "        edge_list = []\n",
    "        \n",
    "        h_h_e_list = []\n",
    "        o_o_e_list = []\n",
    "        h_o_e_list = []\n",
    "        \n",
    "        readout_edge_list = []\n",
    "        readout_h_h_e_list = []\n",
    "        readout_h_o_e_list = []\n",
    "        \n",
    "        # get all edge in the fully-connected graph, edge_list, For node_num = 2, edge_list = [(0, 1), (1, 0)]\n",
    "        for src in range(node_num):\n",
    "            for dst in range(node_num):\n",
    "                if src == dst:\n",
    "                    continue\n",
    "                else:\n",
    "                    edge_list.append((src, dst))\n",
    "        \n",
    "        # readout_edge_list, get corresponding readout edge in the graph\n",
    "        src_box_list = np.arange(roi_label.shape[0])\n",
    "        for dst in h_node_list:\n",
    "            if dst == roi_label.shape[0]-1:\n",
    "                continue\n",
    "            src_box_list = src_box_list[1:]\n",
    "            for src in src_box_list:\n",
    "                readout_edge_list.append((src, dst))\n",
    "        \n",
    "        # readout h_h_e_list, get corresponding readout h_h edges && h_o edges\n",
    "        temp_h_node_list = h_node_list[:]\n",
    "        for dst in h_node_list:\n",
    "            if dst == h_node_list.shape[0]-1:\n",
    "                continue\n",
    "            temp_h_node_list = temp_h_node_list[1:]\n",
    "            for src in temp_h_node_list:\n",
    "                if src == dst: continue\n",
    "                readout_h_h_e_list.append((src, dst))\n",
    "\n",
    "        # readout h_o_e_list\n",
    "        readout_h_o_e_list = [x for x in readout_edge_list if x not in readout_h_h_e_list]\n",
    "\n",
    "        # add node space to match the batch graph\n",
    "        h_node_list = (np.array(h_node_list)+node_space).tolist()\n",
    "        obj_node_list = (np.array(obj_node_list)+node_space).tolist()\n",
    "        \n",
    "        h_h_e_list = (np.array(h_h_e_list)+node_space).tolist() #empty no diff_edge\n",
    "        o_o_e_list = (np.array(o_o_e_list)+node_space).tolist() #empty no diff_edge\n",
    "        h_o_e_list = (np.array(h_o_e_list)+node_space).tolist() #empty no diff_edge\n",
    "\n",
    "        readout_h_h_e_list = (np.array(readout_h_h_e_list)+node_space).tolist()\n",
    "        readout_h_o_e_list = (np.array(readout_h_o_e_list)+node_space).tolist()   \n",
    "        readout_edge_list = (np.array(readout_edge_list)+node_space).tolist()\n",
    "\n",
    "        return edge_list, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list\n",
    "    \n",
    "    def _build_graph(self, node_num, roi_label, node_space, diff_edge):\n",
    "        '''\n",
    "        Declare graph, add_nodes, collect edges, add_edges\n",
    "        '''\n",
    "        graph = dgl.DGLGraph()\n",
    "        graph.add_nodes(node_num)\n",
    "\n",
    "        edge_list, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list = self._collect_edge(node_num, roi_label, node_space, diff_edge)\n",
    "        src, dst = tuple(zip(*edge_list))\n",
    "        graph.add_edges(src, dst)   # make the graph bi-directional\n",
    "\n",
    "        return graph, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list\n",
    "\n",
    "    def forward(self, node_num=None, feat=None, spatial_feat=None, word2vec=None, roi_label=None, validation=False, choose_nodes=None, remove_nodes=None):\n",
    "        \n",
    "        batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, batch_readout_edge_list, batch_readout_h_h_e_list, batch_readout_h_o_e_list = [], [], [], [], [], [], [], [], []\n",
    "        node_num_cum = np.cumsum(node_num) # !IMPORTANT\n",
    "        \n",
    "        for i in range(len(node_num)):\n",
    "            # set node space\n",
    "            node_space = 0\n",
    "            if i != 0:\n",
    "                node_space = node_num_cum[i-1]\n",
    "            graph, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list = self._build_graph(node_num[i], roi_label[i], node_space, diff_edge=self.diff_edge)\n",
    "            \n",
    "            # updata batch\n",
    "            batch_graph.append(graph)\n",
    "            batch_h_node_list += h_node_list\n",
    "            batch_obj_node_list += obj_node_list\n",
    "            \n",
    "            batch_h_h_e_list += h_h_e_list\n",
    "            batch_o_o_e_list += o_o_e_list\n",
    "            batch_h_o_e_list += h_o_e_list\n",
    "            \n",
    "            batch_readout_edge_list += readout_edge_list\n",
    "            batch_readout_h_h_e_list += readout_h_h_e_list\n",
    "            batch_readout_h_o_e_list += readout_h_o_e_list\n",
    "        \n",
    "        batch_graph = dgl.batch(batch_graph)\n",
    "        \n",
    "        # GRNN\n",
    "        self.grnn1(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec, validation, initial_feat=True)\n",
    "        batch_graph.apply_edges(self.edge_readout, tuple(zip(*(batch_readout_h_o_e_list+batch_readout_h_h_e_list))))\n",
    "        \n",
    "        if self.training or validation:\n",
    "            # !NOTE: cannot use \"batch_readout_h_o_e_list+batch_readout_h_h_e_list\" because of the wrong order\n",
    "            return batch_graph.edges[tuple(zip(*batch_readout_edge_list))].data['pred']\n",
    "        else:\n",
    "            return batch_graph.edges[tuple(zip(*batch_readout_edge_list))].data['pred'], \\\n",
    "                   batch_graph.nodes[batch_h_node_list].data['alpha'], \\\n",
    "                   batch_graph.nodes[batch_h_node_list].data['alpha_lang'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils.io as io\n",
    "\n",
    "class SurgicalSceneConstants():\n",
    "    def __init__( self):\n",
    "        self.instrument_classes = ('kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "        \n",
    "        #self.instrument_classes = ( 'kidney', 'bipolar_forceps', 'fenestrated_bipolar', \n",
    "        #                             'prograsp_forceps', 'large_needle_driver', 'vessel_sealer',\n",
    "        #                             'grasping_retractor', 'monopolar_curved_scissors', \n",
    "        #                             'ultrasound_probe', 'suction', 'clip_applier', 'stapler')\n",
    "        \n",
    "        self.action_classes = ( 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                                'Tool_Manipulation', 'Cutting', 'Cauterization', \n",
    "                                'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', \n",
    "                                'Ultrasound_Sensing')\n",
    "        self.xml_data_dir = 'datasets/instruments18/seq_'\n",
    "        self.word2vec_loc = 'datasets/surgicalscene_word2vec.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "    \n",
    "class SurgicalSceneDataset(Dataset):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, seq_set, dataconst, feature_extractor):\n",
    "        self.dataconst = dataconst\n",
    "        self.dir_root_gt = 'datasets/instruments18/seq_'\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        self.xml_dir_list = []\n",
    "        for i in seq_set:\n",
    "            xml_dir_temp = self.dir_root_gt + str(i) + '/xml/'\n",
    "            self.xml_dir_list = self.xml_dir_list + glob(xml_dir_temp + '/*.xml')\n",
    "        \n",
    "        self.word2vec = h5py.File('datasets/surgicalscene_word2vec.hdf5', 'r')\n",
    "    \n",
    "    # word2vec\n",
    "    def _get_word2vec(self,node_ids):\n",
    "        word2vec = np.empty((0,300))\n",
    "        for node_id in node_ids:\n",
    "            vec = self.word2vec[self.dataconst.instrument_classes[node_id]]\n",
    "            word2vec = np.vstack((word2vec, vec))\n",
    "        return word2vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xml_dir_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        file_name = os.path.splitext(os.path.basename(self.xml_dir_list[idx]))[0]\n",
    "        file_root = os.path.dirname(os.path.dirname(self.xml_dir_list[idx]))\n",
    "        _img_loc = os.path.join(file_root+'/left_frames/'+ file_name + '.png')\n",
    "        \n",
    "        frame_data = h5py.File(os.path.join(file_root+'/vsgat/'+self.feature_extractor+'/'+ file_name + '_features.hdf5'), 'r')    \n",
    "        data = {}\n",
    "        data['img_name'] = frame_data['img_name'].value[:] + '.jpg'\n",
    "        data['img_loc'] = _img_loc\n",
    "        \n",
    "        data['node_num'] = frame_data['node_num'].value\n",
    "        data['roi_labels'] = frame_data['classes'][:]\n",
    "        data['det_boxes'] = frame_data['boxes'][:]\n",
    "        \n",
    "        \n",
    "        data['edge_labels'] = frame_data['edge_labels'][:]\n",
    "        data['edge_num'] = data['edge_labels'].shape[0]\n",
    "        \n",
    "        data['features'] = frame_data['node_features'][:]\n",
    "        data['spatial_feat'] = frame_data['spatial_features'][:]\n",
    "        data['word2vec'] = self._get_word2vec(data['roi_labels'])\n",
    "        return data\n",
    "\n",
    "# for DatasetLoader\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "        Default collate_fn(): https://github.com/pytorch/pytorch/blob/1d53d0756668ce641e4f109200d9c65b003d05fa/torch/utils/data/_utils/collate.py#L43\n",
    "    '''\n",
    "    batch_data = {}\n",
    "    batch_data['img_name'] = []\n",
    "    batch_data['img_loc'] = []\n",
    "    batch_data['node_num'] = []\n",
    "    batch_data['roi_labels'] = []\n",
    "    batch_data['det_boxes'] = []\n",
    "    batch_data['edge_labels'] = []\n",
    "    batch_data['edge_num'] = []\n",
    "    batch_data['features'] = []\n",
    "    batch_data['spatial_feat'] = []\n",
    "    batch_data['word2vec'] = []\n",
    "    \n",
    "    for data in batch:\n",
    "        batch_data['img_name'].append(data['img_name'])\n",
    "        batch_data['img_loc'].append(data['img_loc'])\n",
    "        batch_data['node_num'].append(data['node_num'])\n",
    "        batch_data['roi_labels'].append(data['roi_labels'])\n",
    "        batch_data['det_boxes'].append(data['det_boxes'])\n",
    "        batch_data['edge_labels'].append(data['edge_labels'])\n",
    "        batch_data['edge_num'].append(data['edge_num'])\n",
    "        batch_data['features'].append(data['features'])\n",
    "        batch_data['spatial_feat'].append(data['spatial_feat'])\n",
    "        batch_data['word2vec'].append(data['word2vec'])\n",
    "        \n",
    "    batch_data['edge_labels'] = torch.FloatTensor(np.concatenate(batch_data['edge_labels'], axis=0))\n",
    "    batch_data['features'] = torch.FloatTensor(np.concatenate(batch_data['features'], axis=0))\n",
    "    batch_data['spatial_feat'] = torch.FloatTensor(np.concatenate(batch_data['spatial_feat'], axis=0))\n",
    "    batch_data['word2vec'] = torch.FloatTensor(np.concatenate(batch_data['word2vec'], axis=0))\n",
    "    \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch as t\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plot\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def vis_img(img, node_classes, bboxs,  det_action, score_thresh = 0.7):\n",
    "    \n",
    "    Drawer = ImageDraw.Draw(img)\n",
    "    line_width = 3\n",
    "    outline = '#FF0000'\n",
    "    font = ImageFont.truetype(font='/usr/share/fonts/truetype/freefont/FreeMono.ttf', size=25)\n",
    "    \n",
    "    im_w,im_h = img.size\n",
    "    node_num = len(node_classes)\n",
    "    edge_num = len(det_action)\n",
    "    tissue_num = len(np.where(node_classes == 1)[0])\n",
    "    \n",
    "    for node in range(node_num):\n",
    "        \n",
    "        r_color = random.choice(np.arange(256))\n",
    "        g_color = random.choice(np.arange(256))\n",
    "        b_color = random.choice(np.arange(256))\n",
    "        \n",
    "        text = data_const.instrument_classes[node_classes[node]]\n",
    "        h, w = font.getsize(text)\n",
    "        Drawer.rectangle(list(bboxs[node]), outline=outline, width=line_width)\n",
    "        Drawer.text(xy=(bboxs[node][0], bboxs[node][1]-w-1), text=text, font=font, fill=(r_color,g_color,b_color))\n",
    "  \n",
    "    edge_idx = 0\n",
    "    \n",
    "    for tissue in range(tissue_num):\n",
    "        for instrument in range(tissue+1, node_num):\n",
    "            \n",
    "            #action_idx = np.where(det_action[edge_idx] > score_thresh)[0]\n",
    "            action_idx = np.argmax(det_action[edge_idx])\n",
    "#             print('det_action', det_action[edge_idx])\n",
    "#             print('action_idx',action_idx)\n",
    "            \n",
    "            text = data_const.action_classes[action_idx]\n",
    "            r_color = random.choice(np.arange(256))\n",
    "            g_color = random.choice(np.arange(256))\n",
    "            b_color = random.choice(np.arange(256))\n",
    "        \n",
    "            x1,y1,x2,y2 = bboxs[tissue]\n",
    "            x1_,y1_,x2_,y2_ = bboxs[instrument]\n",
    "            \n",
    "            c0 = int(0.5*x1)+int(0.5*x2)\n",
    "            c0 = max(0,min(c0,im_w-1))\n",
    "            r0 = int(0.5*y1)+int(0.5*y2)\n",
    "            r0 = max(0,min(r0,im_h-1))\n",
    "            c1 = int(0.5*x1_)+int(0.5*x2_)\n",
    "            c1 = max(0,min(c1,im_w-1))\n",
    "            r1 = int(0.5*y1_)+int(0.5*y2_)\n",
    "            r1 = max(0,min(r1,im_h-1))\n",
    "            Drawer.line(((c0,r0),(c1,r1)), fill=(r_color,g_color,b_color), width=3)\n",
    "            Drawer.text(xy=(c1, r1), text=text, font=font, fill=(r_color,g_color,b_color))\n",
    "\n",
    "            edge_idx +=1\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import utils.io as io\n",
    "#from utils.vis_tool import vis_img\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def run_model(args, data_const):\n",
    "    '''\n",
    "    a) set dataset and dataloader for train and validate set\n",
    "    b) set model\n",
    "    c) optimizer\n",
    "    d) loss\n",
    "    e) learning rate scheduler\n",
    "    '''\n",
    "\n",
    "    # set up dataset variable\n",
    "    train_dataset = SurgicalSceneDataset(seq_set = [2,3,4,6,7,9,10,11,12,14,15], dataconst = data_const, feature_extractor = args.feature_extractor)\n",
    "    val_dataset = SurgicalSceneDataset(seq_set= [1,5,16], dataconst = data_const, feature_extractor = args.feature_extractor)\n",
    "    dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "    print('set up dataset variable successfully')\n",
    "   \n",
    "    # use default DataLoader() to load the data. \n",
    "    train_dataloader = DataLoader(dataset=dataset['train'], batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(dataset=dataset['val'], batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "    dataloader = {'train': train_dataloader, 'val': val_dataloader}\n",
    "\n",
    "    print('set up dataloader successfully')\n",
    "\n",
    "    # use cpu or cuda\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.gpu else 'cpu')\n",
    "    print('training on {}...'.format(device))\n",
    "\n",
    "    # model\n",
    "    model = AGRNN(bias=args.bias, bn=args.bn, dropout=args.drop_prob, multi_attn=args.multi_attn, layer=args.layers, diff_edge=args.diff_edge, use_cbs = args.use_cbs)\n",
    "    if args.use_cbs:\n",
    "        model.grnn1.gnn.apply_h_h_edge.get_new_kernels(0)\n",
    "    \n",
    "    # calculate the amount of all the learned parameters\n",
    "    parameter_num = 0\n",
    "    for param in model.parameters(): parameter_num += param.numel()\n",
    "    print(f'The parameters number of the model is {parameter_num / 1e6} million')\n",
    "\n",
    "    # load pretrained model\n",
    "    if args.pretrained:\n",
    "        print(f\"loading pretrained model {args.pretrained}\")\n",
    "        checkpoints = torch.load(args.pretrained, map_location=device)\n",
    "        model.load_state_dict(checkpoints['state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # build optimizer  \n",
    "    if args.optim == 'sgd': optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0)\n",
    "    else: optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=0)\n",
    "    \n",
    "    # criterion and scheduler\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.3) #the scheduler divides the lr by 10 every 150 epochs\n",
    "\n",
    "    # get the configuration of the model and save some key configurations\n",
    "    io.mkdir_if_not_exists(os.path.join(args.save_dir, args.exp_ver), recursive=True)\n",
    "    for i in range(args.layers):\n",
    "        if i==0:\n",
    "            model_config = model.CONFIG1.save_config()\n",
    "            model_config['lr'] = args.lr\n",
    "            model_config['bs'] = args.batch_size\n",
    "            model_config['layers'] = args.layers\n",
    "            model_config['multi_attn'] = args.multi_attn\n",
    "            model_config['data_aug'] = args.data_aug\n",
    "            model_config['drop_out'] = args.drop_prob\n",
    "            model_config['optimizer'] = args.optim\n",
    "            model_config['diff_edge'] = args.diff_edge\n",
    "            model_config['model_parameters'] = parameter_num\n",
    "            io.dump_json_object(model_config, os.path.join(args.save_dir, args.exp_ver, 'l1_config.json'))\n",
    "    print('save key configurations successfully...')\n",
    "\n",
    "    epoch_train(model, dataloader, dataset, criterion, optimizer, scheduler, device, data_const, use_cbs = args.use_cbs)\n",
    "\n",
    "def epoch_train(model, dataloader, dataset, criterion, optimizer, scheduler, device, data_const, use_cbs = False):\n",
    "    '''\n",
    "    input: model, dataloader, dataset, criterain, optimizer, scheduler, device, data_const\n",
    "    data: \n",
    "        img_name, node_num, roi_labels, det_boxes, edge_labels,\n",
    "        edge_num, features, spatial_features, word2vec\n",
    "    '''\n",
    "    print('epoch training...')\n",
    "    \n",
    "    # set visualization and create folder to save checkpoints\n",
    "    writer = SummaryWriter(log_dir=args.log_dir + '/' + args.exp_ver + '/' + 'epoch_train')\n",
    "    io.mkdir_if_not_exists(os.path.join(args.save_dir, args.exp_ver, 'epoch_train'), recursive=True)\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        # each epoch has a training and validation step\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for phase in ['train', 'val']:\n",
    "            start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            running_edge_count = 0\n",
    "            idx = 0\n",
    "            \n",
    "            #for data in tqdm(dataloader[phase]):\n",
    "            for data in dataloader[phase]:\n",
    "                train_data = data\n",
    "                img_name = train_data['img_name']\n",
    "                img_loc = train_data['img_loc']\n",
    "                node_num = train_data['node_num']\n",
    "                roi_labels = train_data['roi_labels']\n",
    "                det_boxes = train_data['det_boxes']\n",
    "                edge_labels = train_data['edge_labels']\n",
    "                edge_num = train_data['edge_num']\n",
    "                features = train_data['features']\n",
    "                spatial_feat = train_data['spatial_feat']\n",
    "                word2vec = train_data['word2vec']\n",
    "                features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "                \n",
    "                #if idx == 2: break\n",
    "                    \n",
    "                if phase == 'train':\n",
    "                    if use_cbs:\n",
    "                        model.grnn1.gnn.apply_h_h_edge.get_new_kernels(epoch)\n",
    "                        model.to(device)\n",
    "                        \n",
    "                    model.train()\n",
    "                    model.zero_grad()\n",
    "                    outputs = model(node_num, features, spatial_feat, word2vec, roi_labels)\n",
    "                    \n",
    "                    # loss and accuracy\n",
    "                    loss = criterion(outputs, edge_labels.float())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    acc = np.sum(np.equal(np.argmax(outputs.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    # turn off the gradients for validation, save memory and computations\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(node_num, features, spatial_feat, word2vec, roi_labels, validation=True)\n",
    "                        \n",
    "                        # loss and accuracy\n",
    "                        loss = criterion(outputs, edge_labels.float())\n",
    "                        acc = np.sum(np.equal(np.argmax(outputs.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "                    \n",
    "                        # print result every 1000 iteration during validation\n",
    "                        if idx == 10:\n",
    "                            #print(img_loc[0])\n",
    "                            io.mkdir_if_not_exists(os.path.join(args.output_img_dir, ('epoch_'+str(epoch))), recursive=True)\n",
    "                            image = Image.open(img_loc[0]).convert('RGB')\n",
    "                            det_actions = nn.Sigmoid()(outputs[0:int(edge_num[0])])\n",
    "                            det_actions = det_actions.cpu().detach().numpy()\n",
    "                            action_img = vis_img(image, roi_labels[0], det_boxes[0],  det_actions, score_thresh = 0.7)\n",
    "                            image = image.save(os.path.join(args.output_img_dir, ('epoch_'+str(epoch)),img_name[0]))\n",
    "\n",
    "                idx+=1\n",
    "                # accumulate loss of each batch\n",
    "                running_loss += loss.item() * edge_labels.shape[0]\n",
    "                running_acc += acc\n",
    "                running_edge_count += edge_labels.shape[0]\n",
    "                \n",
    "            # calculate the loss and accuracy of each epoch\n",
    "            epoch_loss = running_loss / len(dataset[phase])\n",
    "            epoch_acc = running_acc / running_edge_count\n",
    "            \n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            # log trainval datas, and visualize them in the same graph\n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss \n",
    "            else:\n",
    "                writer.add_scalars('trainval_loss_epoch', {'train': train_loss, 'val': epoch_loss}, epoch)\n",
    "            \n",
    "            # print data\n",
    "            if (epoch % args.print_every) == 0:\n",
    "                end_time = time.time()\n",
    "                print(\"[{}] Epoch: {}/{} Acc: {:0.6f} Loss: {:0.6f} Execution time: {:0.6f}\".format(\\\n",
    "                        phase, epoch+1, args.epoch, epoch_acc, epoch_loss, (end_time-start_time)))\n",
    "                        \n",
    "        # scheduler.step()\n",
    "        # save model\n",
    "        if epoch_loss<0.0405 or epoch % args.save_every == (args.save_every - 1) and epoch >= (50-1):\n",
    "            checkpoint = { \n",
    "                            'lr': args.lr,\n",
    "                           'b_s': args.batch_size,\n",
    "                          'bias': args.bias, \n",
    "                            'bn': args.bn, \n",
    "                       'dropout': args.drop_prob,\n",
    "                        'layers': args.layers,\n",
    "                    'multi_head': args.multi_attn,\n",
    "                     'diff_edge': args.diff_edge,\n",
    "                    'state_dict': model.state_dict()\n",
    "            }\n",
    "            save_name = \"checkpoint_\" + str(epoch+1) + '_epoch.pth'\n",
    "            torch.save(checkpoint, os.path.join(args.save_dir, args.exp_ver, 'epoch_train', save_name))\n",
    "\n",
    "    writer.close()\n",
    "    print('Finishing training!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18_11_ts_ls\n",
      "set up dataset variable successfully\n",
      "set up dataloader successfully\n",
      "training on cuda...\n",
      "The parameters number of the model is 2.393694 million\n",
      "save key configurations successfully...\n",
      "epoch training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Epoch: 1/351 Acc: 0.058760 Loss: 3.899910 Execution time: 6.039857\n",
      "[val] Epoch: 1/351 Acc: 0.065461 Loss: 1.484331 Execution time: 1.693634\n",
      "[train] Epoch: 11/351 Acc: 0.279045 Loss: 1.234442 Execution time: 5.624106\n",
      "[val] Epoch: 11/351 Acc: 0.409130 Loss: 0.602940 Execution time: 1.515732\n",
      "[train] Epoch: 21/351 Acc: 0.324390 Loss: 0.983213 Execution time: 6.051103\n",
      "[val] Epoch: 21/351 Acc: 0.502153 Loss: 0.485109 Execution time: 1.721537\n",
      "[train] Epoch: 31/351 Acc: 0.331366 Loss: 0.854449 Execution time: 5.772768\n",
      "[val] Epoch: 31/351 Acc: 0.571921 Loss: 0.457615 Execution time: 1.584185\n",
      "[train] Epoch: 41/351 Acc: 0.333512 Loss: 0.829033 Execution time: 5.924012\n",
      "[val] Epoch: 41/351 Acc: 0.517657 Loss: 0.464118 Execution time: 1.574282\n",
      "[train] Epoch: 51/351 Acc: 0.349343 Loss: 0.797130 Execution time: 5.832133\n",
      "[val] Epoch: 51/351 Acc: 0.593454 Loss: 0.440001 Execution time: 1.628905\n",
      "[train] Epoch: 61/351 Acc: 0.376979 Loss: 0.777216 Execution time: 5.816685\n",
      "[val] Epoch: 61/351 Acc: 0.585702 Loss: 0.431421 Execution time: 1.573968\n",
      "[train] Epoch: 71/351 Acc: 0.403542 Loss: 0.765013 Execution time: 5.791824\n",
      "[val] Epoch: 71/351 Acc: 0.607235 Loss: 0.420026 Execution time: 1.593068\n",
      "[train] Epoch: 81/351 Acc: 0.414811 Loss: 0.754314 Execution time: 5.860781\n",
      "[val] Epoch: 81/351 Acc: 0.596038 Loss: 0.417294 Execution time: 1.581638\n",
      "[train] Epoch: 91/351 Acc: 0.430373 Loss: 0.741517 Execution time: 5.862417\n",
      "[val] Epoch: 91/351 Acc: 0.618432 Loss: 0.412261 Execution time: 1.553048\n",
      "[train] Epoch: 101/351 Acc: 0.444325 Loss: 0.738209 Execution time: 5.702587\n",
      "[val] Epoch: 101/351 Acc: 0.630491 Loss: 0.404327 Execution time: 1.579264\n",
      "[train] Epoch: 111/351 Acc: 0.443252 Loss: 0.738174 Execution time: 5.796173\n",
      "[val] Epoch: 111/351 Acc: 0.623600 Loss: 0.401105 Execution time: 1.571896\n",
      "[train] Epoch: 121/351 Acc: 0.471425 Loss: 0.728144 Execution time: 5.888269\n",
      "[val] Epoch: 121/351 Acc: 0.645134 Loss: 0.393474 Execution time: 1.622116\n",
      "[train] Epoch: 131/351 Acc: 0.481352 Loss: 0.726126 Execution time: 5.888431\n",
      "[val] Epoch: 131/351 Acc: 0.634798 Loss: 0.391012 Execution time: 1.578627\n",
      "[train] Epoch: 141/351 Acc: 0.505232 Loss: 0.703104 Execution time: 5.839766\n",
      "[val] Epoch: 141/351 Acc: 0.630491 Loss: 0.388366 Execution time: 1.579955\n",
      "[train] Epoch: 151/351 Acc: 0.495305 Loss: 0.708475 Execution time: 5.916898\n",
      "[val] Epoch: 151/351 Acc: 0.637382 Loss: 0.383882 Execution time: 1.574021\n",
      "[train] Epoch: 161/351 Acc: 0.508452 Loss: 0.705091 Execution time: 5.913699\n",
      "[val] Epoch: 161/351 Acc: 0.636520 Loss: 0.383149 Execution time: 1.604553\n",
      "[train] Epoch: 171/351 Acc: 0.510330 Loss: 0.700933 Execution time: 5.681204\n",
      "[val] Epoch: 171/351 Acc: 0.635659 Loss: 0.379728 Execution time: 1.562473\n",
      "[train] Epoch: 181/351 Acc: 0.515696 Loss: 0.702275 Execution time: 5.836804\n",
      "[val] Epoch: 181/351 Acc: 0.641688 Loss: 0.378747 Execution time: 1.571086\n",
      "[train] Epoch: 191/351 Acc: 0.523746 Loss: 0.696390 Execution time: 5.811608\n",
      "[val] Epoch: 191/351 Acc: 0.625323 Loss: 0.380410 Execution time: 1.573330\n",
      "[train] Epoch: 201/351 Acc: 0.530185 Loss: 0.692715 Execution time: 5.756751\n",
      "[val] Epoch: 201/351 Acc: 0.633936 Loss: 0.381795 Execution time: 1.565238\n",
      "[train] Epoch: 211/351 Acc: 0.534478 Loss: 0.697236 Execution time: 5.791746\n",
      "[val] Epoch: 211/351 Acc: 0.634798 Loss: 0.374548 Execution time: 1.564903\n",
      "[train] Epoch: 221/351 Acc: 0.537161 Loss: 0.689831 Execution time: 5.778304\n",
      "[val] Epoch: 221/351 Acc: 0.620155 Loss: 0.381669 Execution time: 1.573371\n",
      "[train] Epoch: 231/351 Acc: 0.544674 Loss: 0.685394 Execution time: 5.905820\n",
      "[val] Epoch: 231/351 Acc: 0.620155 Loss: 0.381604 Execution time: 1.577917\n",
      "[train] Epoch: 241/351 Acc: 0.558895 Loss: 0.680510 Execution time: 5.705409\n",
      "[val] Epoch: 241/351 Acc: 0.625323 Loss: 0.377313 Execution time: 1.584886\n",
      "[train] Epoch: 251/351 Acc: 0.561578 Loss: 0.689659 Execution time: 5.897483\n",
      "[val] Epoch: 251/351 Acc: 0.625323 Loss: 0.375376 Execution time: 1.565517\n",
      "[train] Epoch: 261/351 Acc: 0.563188 Loss: 0.675482 Execution time: 5.836833\n",
      "[val] Epoch: 261/351 Acc: 0.624462 Loss: 0.376647 Execution time: 1.657413\n",
      "[train] Epoch: 271/351 Acc: 0.574725 Loss: 0.677562 Execution time: 6.015385\n",
      "[val] Epoch: 271/351 Acc: 0.614126 Loss: 0.377526 Execution time: 1.586641\n",
      "[train] Epoch: 281/351 Acc: 0.567212 Loss: 0.672401 Execution time: 5.796466\n",
      "[val] Epoch: 281/351 Acc: 0.616710 Loss: 0.380667 Execution time: 1.564996\n",
      "[train] Epoch: 291/351 Acc: 0.578750 Loss: 0.670644 Execution time: 5.715875\n",
      "[val] Epoch: 291/351 Acc: 0.602929 Loss: 0.384904 Execution time: 1.580705\n",
      "[train] Epoch: 301/351 Acc: 0.583579 Loss: 0.672736 Execution time: 5.819482\n",
      "[val] Epoch: 301/351 Acc: 0.614987 Loss: 0.377760 Execution time: 1.578988\n",
      "[train] Epoch: 311/351 Acc: 0.587872 Loss: 0.666927 Execution time: 5.865935\n",
      "[val] Epoch: 311/351 Acc: 0.599483 Loss: 0.387606 Execution time: 1.602262\n",
      "[train] Epoch: 321/351 Acc: 0.570432 Loss: 0.672521 Execution time: 5.735044\n",
      "[val] Epoch: 321/351 Acc: 0.603790 Loss: 0.384870 Execution time: 1.578795\n",
      "[train] Epoch: 331/351 Acc: 0.589750 Loss: 0.666902 Execution time: 5.858972\n",
      "[val] Epoch: 331/351 Acc: 0.604651 Loss: 0.389632 Execution time: 1.580345\n",
      "[train] Epoch: 341/351 Acc: 0.591092 Loss: 0.663496 Execution time: 5.876459\n",
      "[val] Epoch: 341/351 Acc: 0.602929 Loss: 0.384556 Execution time: 1.580695\n",
      "[train] Epoch: 351/351 Acc: 0.590555 Loss: 0.662002 Execution time: 5.704991\n",
      "[val] Epoch: 351/351 Acc: 0.595177 Loss: 0.388861 Execution time: 1.575038\n",
      "Finishing training!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = arguments()\n",
    "    print(args.feature_extractor)\n",
    "    data_const = SurgicalSceneConstants()\n",
    "    run_model(args, data_const)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
