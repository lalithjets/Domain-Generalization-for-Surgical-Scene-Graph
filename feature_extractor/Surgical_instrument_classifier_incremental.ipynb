{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Project         : Incremental learning for surgical instrument classification and feature extraction\n",
    "Lab             : MMLAB, National University of Singapore\n",
    "contributors    : Mobarak, lalith, mengya\n",
    "Note            : Dataloader for End-to-End incremental learning, code adopted from our previous work.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import xml.etree.cElementTree as ET\n",
    "else:\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "class SurgicalClassDataset18_incremental(Dataset):\n",
    "    def __init__(self, filenames, fine_tune_size = None, is_train=None):\n",
    "        \n",
    "        self.is_train = is_train\n",
    "        self.img_list = []\n",
    "        \n",
    "        # Using readlines() \n",
    "        for i, txt_file in enumerate(filenames):\n",
    "            curr_file = open((txt_file), 'r') \n",
    "            Lines = curr_file.readlines()  \n",
    "            if (fine_tune_size is not None) and (i == len(filenames)-1):\n",
    "                indices = np.random.permutation(len(Lines))\n",
    "                Lines = [Lines[i] for i in indices[0:fine_tune_size]]\n",
    "            for line in Lines: self.img_list.append(line. rstrip())\n",
    "            #print(self.img_list)\n",
    "            curr_file.close()\n",
    "        \n",
    "    def __len__(self): return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _img_dir = self.img_list[index]\n",
    "        #print(_img_dir)\n",
    "        _img = Image.open(_img_dir).convert('RGB')\n",
    "        _target = int(_img_dir[:-4].split('_')[-1:][0])\n",
    "        _img = np.asarray(_img, np.float32) / 255\n",
    "        _img = torch.from_numpy(np.array(_img).transpose(2, 0, 1,)).float()\n",
    "        _target = torch.from_numpy(np.array(_target)).long()\n",
    "        return _img, _target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBS filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gaussian and laplacian filters for curicullum learning\n",
    "'''\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_gaussian_filter(kernel_size=3, sigma=2, channels=3):\n",
    "    '''\n",
    "    Gaussian 2D filter\n",
    "    '''\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "\n",
    "    mean = (kernel_size - 1)/2.\n",
    "    variance = sigma**2.\n",
    "\n",
    "    # Calculate the 2-dimensional gaussian kernel which is the product of two gaussian distributions \n",
    "    # for two different variables (in this case called x and y)\n",
    "    gaussian_kernel = (1./(2.*math.pi*variance)) *\\\n",
    "                      torch.exp( -torch.sum((xy_grid - mean)**2., dim=-1) / (2*variance))\n",
    "\n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "\n",
    "    # Reshape to 2d depthwise convolutional weight\n",
    "    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)\n",
    "    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1, 1)\n",
    "\n",
    "    if kernel_size == 3: padding = 1\n",
    "    elif kernel_size == 5: padding = 2\n",
    "    else: padding = 0\n",
    "\n",
    "    gaussian_filter = nn.Conv2d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels,\n",
    "                                bias=False, padding=padding)\n",
    "\n",
    "    gaussian_filter.weight.data = gaussian_kernel\n",
    "    gaussian_filter.weight.requires_grad = False\n",
    "    \n",
    "    return gaussian_filter\n",
    "\n",
    "\n",
    "def get_laplaceOfGaussian_filter(kernel_size=3, sigma=2, channels=3):\n",
    "    '''\n",
    "    laplacian 2D filter\n",
    "    '''\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "    mean = (kernel_size - 1)/2.\n",
    "\n",
    "    used_sigma = sigma\n",
    "    # Calculate the 2-dimensional gaussian kernel which is\n",
    "    log_kernel = (-1./(math.pi*(used_sigma**4))) \\\n",
    "                        * (1-(torch.sum((xy_grid - mean)**2., dim=-1) / (2*(used_sigma**2)))) \\\n",
    "                        * torch.exp(-torch.sum((xy_grid - mean)**2., dim=-1) / (2*(used_sigma**2)))\n",
    "       \n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    log_kernel = log_kernel / torch.sum(log_kernel)\n",
    "\n",
    "    # Reshape to 2d depthwise convolutional weight\n",
    "    log_kernel = log_kernel.view(1, 1, kernel_size, kernel_size)\n",
    "    log_kernel = log_kernel.repeat(channels, 1, 1, 1)\n",
    "\n",
    "    if kernel_size == 3: padding = 1\n",
    "    elif kernel_size == 5: padding = 2\n",
    "    else: padding = 0\n",
    "\n",
    "    log_filter = nn.Conv2d( in_channels=channels, out_channels=channels, kernel_size=kernel_size, \n",
    "                            groups=channels, bias=False, padding=padding)\n",
    "\n",
    "    log_filter.weight.data = log_kernel\n",
    "    log_filter.weight.requires_grad = False\n",
    "    \n",
    "    return log_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ResNet (Pytorch implementation), together with curricullum learning filters\n",
    "    Reference:\n",
    "    [1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.planes = planes\n",
    "        self.enable_cbs = False\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut_kernel = True\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def get_new_kernels(self, fil2, fil3, kernel_size, std):\n",
    "        self.enable_cbs = True\n",
    "        if (fil2 == 'gau'): \n",
    "            self.kernel1 = get_gaussian_filter(kernel_size=kernel_size, sigma= std, channels=self.planes)\n",
    "        elif (fil2 == 'LOG'): \n",
    "            self.kernel1 = get_laplaceOfGaussian_filter(kernel_size=kernel_size, sigma= std, channels=self.planes)\n",
    "\n",
    "        if (fil3 == 'gau'): \n",
    "            self.kernel2 = get_gaussian_filter(kernel_size=kernel_size, sigma= std, channels=self.planes)\n",
    "        elif (fil3 == 'LOG'): \n",
    "            self.kernel2 = get_laplaceOfGaussian_filter(kernel_size=kernel_size, sigma= std, channels=self.planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        \n",
    "        if self.enable_cbs: out = F.relu(self.bn1(self.kernel1(out)))         \n",
    "        else: out = F.relu(self.bn1(out))         \n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if self.enable_cbs: out = self.bn2(self.kernel2(out))\n",
    "        else: out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, args):\n",
    "               \n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.std = args.std\n",
    "        self.enable_cbs = args.use_cbs\n",
    "        self.factor = args.std_factor\n",
    "        self.epoch = args.cbs_epoch\n",
    "        self.kernel_size = args.kernel_size\n",
    "\n",
    "        self.fil1 = args.fil1\n",
    "        self.fil2 = args.fil2\n",
    "        self.fil3 = args.fil3\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.linear = nn.Linear(512*block.expansion, args.num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        if self.enable_cbs: out = F.relu(self.bn1(self.kernel1(out)))\n",
    "        else: out = F.relu(self.bn1(out))\n",
    "            \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def get_new_kernels(self, epoch_count):\n",
    "        if epoch_count % self.epoch == 0 and epoch_count is not 0:\n",
    "            self.std *= self.factor\n",
    "        if (self.fil1 == 'gau'): \n",
    "            self.kernel1 = get_gaussian_filter(kernel_size=self.kernel_size, sigma= self.std, channels=64)\n",
    "        elif (self.fil1 == 'LOG'): \n",
    "            self.kernel1 = get_laplaceOfGaussian_filter(kernel_size=self.kernel_size, sigma= self.std, channels=64)\n",
    "\n",
    "        for child in self.layer1.children():\n",
    "            child.get_new_kernels(self.fil2, self.fil3, self.kernel_size, self.std)\n",
    "\n",
    "        for child in self.layer2.children():\n",
    "            child.get_new_kernels(self.fil2, self.fil3, self.kernel_size, self.std)\n",
    "\n",
    "        for child in self.layer3.children():\n",
    "            child.get_new_kernels(self.fil2, self.fil3, self.kernel_size, self.std)\n",
    "\n",
    "        for child in self.layer4.children():\n",
    "            child.get_new_kernels(self.fil2, self.fil3, self.kernel_size, self.std)\n",
    "\n",
    "\n",
    "\n",
    "def ResNet18(args): return ResNet(BasicBlock, [2,2,2,2], args)\n",
    "\n",
    "# def ResNet34(args): return ResNet(BasicBlock, [3,4,6,3], args)\n",
    "# def ResNet50(args): return ResNet(Bottleneck, [3,4,6,3], args)\n",
    "# def ResNet101(args):return ResNet(Bottleneck, [3,4,23,3], args)\n",
    "\n",
    "# def test():\n",
    "#     net = ResNet18()\n",
    "#     y = net(torch.randn(1,3,32,32))\n",
    "#     print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def test(args, model, test_loader, class_old, class_novel):\n",
    "    '''\n",
    "    arguments: net, test_loader, class_old, class_novel\n",
    "    return: tcost, acc_avg\n",
    "    '''\n",
    "\n",
    "    acc_avg = 0\n",
    "    num_exp = 0\n",
    "    tstart = time.clock()\n",
    "\n",
    "    # set net to eval\n",
    "    model.eval()\n",
    "    \n",
    "    # loss\n",
    "    if args.dist_loss_act == 'softmax': \n",
    "        dist_loss_act = nn.Softmax(dim=1)\n",
    "    else:\n",
    "        dist_loss_act = nn.Softmax(dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            # prepare target_onehot\n",
    "            bs = np.shape(target)[0]\n",
    "            target_onehot = np.zeros(shape = (bs, args.num_classes), dtype=np.int)\n",
    "            for i in range(bs): target_onehot[i,target[i]] = 1\n",
    "            target_onehot = torch.from_numpy(target_onehot)\n",
    "            target_onehot = target_onehot.float()\n",
    "            \n",
    "            # indices for combined classes\n",
    "            class_indices = torch.LongTensor(np.concatenate((class_old, class_novel), axis=0))\n",
    "\n",
    "            # send image and target to cuda\n",
    "            if args.cuda:\n",
    "                data = data.cuda()\n",
    "                target_onehot = target_onehot.cuda()\n",
    "                class_indices = class_indices.cuda()\n",
    "                dist_loss_act = dist_loss_act.cuda()\n",
    "\n",
    "            # predict output\n",
    "            output = model(data)\n",
    "\n",
    "            # calculate output and target one_hot\n",
    "            output = torch.index_select(output, 1, class_indices)\n",
    "            output = dist_loss_act(output)\n",
    "            output = output.cpu().data.numpy()\n",
    "            target_onehot = torch.index_select(target_onehot, 1, class_indices)\n",
    "            #target_onehot = target_onehot[:, np.concatenate((class_old, class_novel), axis=0)]\n",
    "\n",
    "            # calculation accuracy\n",
    "            acc = np.sum(np.equal(np.argmax(output, axis=-1), np.argmax(target_onehot.cpu().data.numpy(), axis=-1)))\n",
    "            acc_avg += acc\n",
    "            num_exp += np.shape(target)[0]\n",
    "\n",
    "    # calculate average accuracy\n",
    "    acc_avg /= num_exp\n",
    "            \n",
    "    # time calculation\n",
    "    tend = time.clock()\n",
    "    tcost = tend - tstart\n",
    "\n",
    "    return(tcost, acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def train (args, period, model, model_old, train_loader, loss_criterion, \\\n",
    "           optimizer, class_old, class_novel, finetune):\n",
    "    '''\n",
    "    arguments: period, net, net_old, train_loader, loss_criterion, loss_activation, optimizer, clss_old, class_novel, finetune\n",
    "    returns: tcost, loss_avg, acc_avg\n",
    "    '''\n",
    "\n",
    "    acc_avg = 0\n",
    "    num_exp = 0\n",
    "    loss_avg = 0\n",
    "    loss_cls_avg = 0\n",
    "    loss_dist_avg = 0\n",
    "    tstart = time.clock()\n",
    "\n",
    "    # set net to train mode\n",
    "    model.train()\n",
    "    model_old.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        #optimizer.zero_grad()\n",
    "\n",
    "        # prepare target_onehot\n",
    "        bs = np.shape(target)[0]\n",
    "        target_onehot = np.zeros(shape = (bs, args.num_classes), dtype=np.int)\n",
    "        for i in range(bs): target_onehot[i,target[i]] = 1\n",
    "        target_onehot = torch.from_numpy(target_onehot)\n",
    "        target_onehot = target_onehot.float()\n",
    "\n",
    "        # indices for combined classes\n",
    "        class_indices = torch.LongTensor(np.concatenate((class_old, class_novel), axis=0))\n",
    "        \n",
    "        # send data to cuda\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "            target_onehot = target_onehot.cuda()\n",
    "            class_indices = class_indices.cuda()\n",
    "\n",
    "        # predict output\n",
    "        output = model(data)\n",
    "\n",
    "        # loss for network\n",
    "        output_new_onehot = torch.index_select(output, 1, class_indices)\n",
    "        target_onehot = torch.index_select(target_onehot, 1, class_indices)\n",
    "        combined_loss = loss_criterion(output_new_onehot, target_onehot)\n",
    "\n",
    "        ''' ===== Distillation loss based on old net ====='''\n",
    "        if (period > 0):\n",
    "\n",
    "            # distillation loss activation\n",
    "            if args.dist_loss_act == 'softmax': dist_loss_act = nn.Softmax(dim=1)\n",
    "            if args.cuda: dist_loss_act = dist_loss_act.cuda()\n",
    "            \n",
    "            # indices of old class\n",
    "            if not finetune:\n",
    "                class_indices = torch.LongTensor(class_old)\n",
    "                if args.cuda: class_indices = class_indices.cuda()\n",
    "                    \n",
    "            # current_network output\n",
    "            dist = torch.index_select(output, 1, class_indices)\n",
    "            if args.use_ts: dist = dist/args.tscale\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # old network output\n",
    "                output_old = model_old(data)\n",
    "                output_old = torch.index_select(output_old, 1, class_indices)\n",
    "            \n",
    "            target_dist = Variable(output_old)\n",
    "            if args.use_ts: target_dist = target_dist/args.tscale\n",
    "            #loss_dist = loss_criterion(dist, loss_activation(target_dist))\n",
    "            \n",
    "            if(args.dist_loss == 'ce'):\n",
    "                loss_dist = F.binary_cross_entropy(dist_loss_act(dist), dist_loss_act(target_dist))\n",
    "            else: loss_dist = 0.0\n",
    "            \n",
    "        else: loss_dist = 0.0\n",
    "        '''----------------------------------------------'''\n",
    "\n",
    "        # loss calculatoin\n",
    "        loss = combined_loss + args.dist_ratio*loss_dist\n",
    "        loss_avg += loss.item()\n",
    "        loss_cls_avg += combined_loss.item()\n",
    "        if period == 0: loss_dist_avg += 0\n",
    "        else:loss_dist_avg += loss_dist.item()\n",
    "\n",
    "        acc = np.sum(np.equal(np.argmax(output_new_onehot.cpu().data.numpy(), axis=-1), np.argmax(target_onehot.cpu().data.numpy(), axis=-1)))\n",
    "        acc_avg += acc\n",
    "        num_exp += np.shape(target)[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # average calculation\n",
    "    loss_avg /= num_exp\n",
    "    loss_cls_avg /= num_exp\n",
    "    loss_dist_avg /= num_exp\n",
    "        \n",
    "    # average calculation\n",
    "    acc_avg /= num_exp\n",
    "\n",
    "    # time calculation\n",
    "    tend = time.clock()\n",
    "    tcost = tend - tstart\n",
    "\n",
    "    return(tcost, loss_avg, acc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Project         : Incremental learning for surgical instrument classification and feature extraction\n",
    "Lab             : MMLAB, National University of Singapore\n",
    "contributors    : Mobarak, lalith, mengya\n",
    "Note            : Lable smoothing loss, code adopted from our previous work and modified.\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CELossWithLS(torch.nn.Module):\n",
    "    def __init__(self, smoothing=0.1, gamma=3.0, isCos=True, ignore_index=-1):\n",
    "        super(CELossWithLS, self).__init__()\n",
    "        self.complement = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        with torch.no_grad():\n",
    "            smoothen_ohlabel = target * self.complement + self.smoothing / target.shape[1]\n",
    "        \n",
    "        target_labels = torch.argmax(target, dim=1)\n",
    "        #print(target_labels)\n",
    "        logs = self.log_softmax(logits[target_labels!=self.ignore_index])\n",
    "        pt = torch.exp(logs)\n",
    "        return -torch.sum((1-pt).pow(self.gamma)*logs * smoothen_ohlabel[target_labels!=self.ignore_index], dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def save_model(args, best_model):\n",
    "    '''\n",
    "    save model\n",
    "    '''\n",
    "    if not os.path.exists('./weights'): os.mkdir('weights/')\n",
    "    \n",
    "    filename = os.path.join('weights', args.log_name + '_model.tar')\n",
    "    torch.save(best_model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    print('ls', args.use_ls, 'ts', args.use_ts, 'cbs', args.use_cbs)\n",
    "    # learning rate schedules\n",
    "    schedules = range(args.schedule_interval, args.epoch_base, args.schedule_interval)\n",
    "\n",
    "    # class order in icremental learning\n",
    "    class_order = np.arange(args.num_classes) #np.random.permutation(args.num_class)\n",
    "    print('class order:', class_order)\n",
    "\n",
    "    # check for pre-trained model\n",
    "    model_path = args.checkpointfile + '_%d_%s%s' % (0, ''.join(str(e) for e in class_order[args.num_class_novel[0]:args.num_class_novel[1]]), '.pkl')\n",
    "    flag_model = os.path.exists(model_path)\n",
    "\n",
    "    # network\n",
    "    model = ResNet18(args)\n",
    "    model_old = copy.deepcopy(model)\n",
    "    \n",
    "    # curicullum learning\n",
    "    if args.use_cbs:\n",
    "        model.get_new_kernels(0)\n",
    "        model_old.get_new_kernels(0)  \n",
    "    \n",
    "    # loss\n",
    "    if args.use_ls: \n",
    "        loss_criterion = CELossWithLS(smoothing = 0.1, gamma=0.0, isCos=False, ignore_index=-1)\n",
    "    else: \n",
    "        loss_criterion = CELossWithLS(smoothing= 0.0, gamma=0.0, isCos=False, ignore_index=-1)\n",
    "        \n",
    "    # gpu\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    if num_gpu > 0:\n",
    "        device_ids = np.arange(num_gpu).tolist()\n",
    "        print('device_ids:', device_ids)\n",
    "        model = nn.DataParallel(model, device_ids=device_ids).cuda()\n",
    "        model_old = nn.DataParallel(model_old, device_ids=device_ids).cuda()\n",
    "        loss_criterion = loss_criterion.cuda()\n",
    "    else: print('only cpu is available')\n",
    "        \n",
    " \n",
    "\n",
    "    # initializing classes, accuracy and memory array\n",
    "    memory_train = []                                  # train memory array\n",
    "    class_old = np.array([], dtype=int)                # old class array\n",
    "    acc_nvld_basic = np.zeros((args.period_train))     # accuracy list\n",
    "    acc_nvld_finetune = np.zeros((args.period_train))  # accuracy list\n",
    "\n",
    "    \n",
    "    for period in range(args.period_train):\n",
    "\n",
    "        print('===================== period = %d ========================='%(period))\n",
    "\n",
    "        # current 10 classes\n",
    "        class_novel = class_order[args.num_class_novel[period]:args.num_class_novel[period+1]]\n",
    "        print('class_novel:', class_novel)\n",
    "\n",
    "        # combined train dataloader\n",
    "        combined_train_files = memory_train + args.novel_train_files[period:period+1]\n",
    "        combined_train_dataset = SurgicalClassDataset18_incremental(filenames= combined_train_files, is_train=True)\n",
    "        combined_train_loader = DataLoader(dataset=combined_train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
    "        print('train files: \\t size: ', len(combined_train_loader.dataset), ' , files: ', combined_train_files)\n",
    "\n",
    "        # test dataloader\n",
    "        combined_test_files = args.novel_test_files[0:period+1]\n",
    "        test_dataset = SurgicalClassDataset18_incremental(filenames= combined_test_files, is_train=False)\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size= args.batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
    "        print('train files: \\t size: ', len(test_loader.dataset), ' , files: ', combined_test_files)\n",
    "\n",
    "        # initialize variables\n",
    "        lrc = args.lr\n",
    "        acc_training = []\n",
    "        print('current lr = %f' % (lrc))\n",
    "\n",
    "        # epoch training\n",
    "        for epoch in range(args.epoch_base):\n",
    "        \n",
    "            # load pretrained model\n",
    "            if period == 0 and flag_model:\n",
    "                print('load model: %s' % model_path)\n",
    "                model.load_state_dict(torch.load(model_path))\n",
    "            \n",
    "            if args.use_cbs:\n",
    "                model.module.get_new_kernels(epoch)\n",
    "                model_old.module.get_new_kernels(epoch)\n",
    "                model.cuda()\n",
    "                model_old.cuda()\n",
    "\n",
    "            ''' ====== training combined ======''' \n",
    "            # decaying learning rate\n",
    "            if epoch in schedules:\n",
    "                lrc *= args.gamma\n",
    "                print('current lr = %f' % (lrc))\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lrc, momentum=args.momentum, weight_decay=args.decay)\n",
    "\n",
    "            # train\n",
    "            tcost, loss_avg, acc_avg = train(args, period, model, model_old, combined_train_loader, \n",
    "                                             loss_criterion, optimizer, class_old, class_novel, False)\n",
    "\n",
    "            acc_training.append(acc_avg)\n",
    "            print('Training Period: %d \\t Epoch: %d \\t time = %.1f \\t loss = %.6f \\t acc = %.4f' % (period, epoch, tcost, loss_avg, acc_avg))\n",
    "            '''--------------------------------'''\n",
    "\n",
    "            ''' ====== Test combined ======'''\n",
    "            # test model\n",
    "            tcost, acc_avg = test(args, model, test_loader,class_old, class_novel)\n",
    "\n",
    "            acc_nvld_basic[period] = acc_avg\n",
    "            print('Test(n&o)Period: %d \\t Epoch: %d \\t time = %.1f \\t\\t\\t\\t acc = %.4f' % (period, epoch, tcost, acc_avg))\n",
    "\n",
    "            # exit if pre-trained model / loss converged\n",
    "            if period == 0 and flag_model: break\n",
    "            if len(acc_training)>20 and acc_training[-1]>args.stop_acc and acc_training[-5]>args.stop_acc:\n",
    "                print('training loss converged')\n",
    "                break\n",
    "            '''----------------------------'''\n",
    "\n",
    "        ''' copy net-old for finetuning '''\n",
    "        model_old = copy.deepcopy(model)\n",
    "        '''-----------------------------'''\n",
    "\n",
    "        ''' ===== Finetuning ====='''\n",
    "        if period > 0:\n",
    "            \n",
    "            acc_finetune_train = []\n",
    "            lrc = args.lr*args.ft_lr_factor # finetune lr\n",
    "            print('finetune current lr = %f' % (lrc))\n",
    "\n",
    "            for epoch in range(args.epoch_finetune):\n",
    "\n",
    "                # fine tune train_dataloaders\n",
    "                ft_size = (args.num_class_novel[period+1]-args.num_class_novel[period])*args.memory_size\n",
    "                ft_combined_train_dataset = SurgicalClassDataset18_incremental(filenames= combined_train_files, fine_tune_size = ft_size, is_train=True)\n",
    "                ft_combined_train_loader = DataLoader(dataset=ft_combined_train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
    "                if(epoch == 0):  print('finetune train size:', len(ft_combined_train_loader.dataset))\n",
    "\n",
    "                ''' ===== training combined =====''' \n",
    "                # learning rate\n",
    "                if epoch in schedules:\n",
    "                    lrc *= args.gamma\n",
    "                    print('current lr = %f'%(lrc))\n",
    "\n",
    "                # optimizer\n",
    "                # criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=lrc, momentum=args.momentum, weight_decay=args.decay)\n",
    "\n",
    "                # train\n",
    "                tcost, loss_avg, acc_avg = train(args, period, model, model_old, ft_combined_train_loader, \n",
    "                                                 loss_criterion, optimizer, class_old, class_novel, True)\n",
    "\n",
    "                acc_finetune_train.append(acc_avg)\n",
    "                print('Finetune Training Period: %d \\t Epoch: %d \\t time = %.1f \\t loss = %.6f \\t acc = %.4f'%(period, epoch, tcost, loss_avg, acc_avg))\n",
    "                '''------------------------------'''\n",
    "\n",
    "                ''' ===== Test combined ====='''\n",
    "                # test\n",
    "                tcost, acc_avg = test(args, model, test_loader, class_old, class_novel)\n",
    "\n",
    "                acc_nvld_finetune[period] = acc_avg\n",
    "                print('Finetune Test(n&o) Period: %d \\t Epoch: %d \\t time = %.1f \\t\\t\\t\\t acc = %.4f' % (period, epoch, tcost, acc_avg))\n",
    "\n",
    "                if len(acc_finetune_train) > 20 and acc_finetune_train[-1] > args.stop_acc and acc_finetune_train[-5] > args.stop_acc:\n",
    "                    print('finetune training loss converged')\n",
    "                    break\n",
    "                '''--------------------------'''\n",
    "                \n",
    "            print('------------------- result ------------------------')\n",
    "            print('Period: %d, basic acc = %.4f, finetune acc = %.4f' % (period, acc_nvld_basic[period], acc_nvld_finetune[period]))\n",
    "            print('---------------------------------------------------')\n",
    "\n",
    "        if period == args.period_train-1:\n",
    "            print('------------------- ave result ------------------------')\n",
    "            print('basic acc = %.4f, finetune acc = %.4f' % (np.mean(acc_nvld_basic[1:], keepdims=False), np.mean(acc_nvld_finetune[1:], keepdims=False)))\n",
    "            print('---------------------------------------------------')\n",
    "\n",
    "        print('===========================================================')\n",
    "\n",
    "        # save model\n",
    "        model_path = args.checkpointfile + '_%d_%s%s' % (0, ''.join(str(e) for e in class_order[args.num_class_novel[0]:args.num_class_novel[period+1]]), '.pkl')\n",
    "        print('save model: %s' % model_path)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        ''' ===== random images selection ====='''\n",
    "        #remove memory files from old runs\n",
    "        if os.path.exists(('data_files/memory_'+str(period)+'.txt')): \n",
    "            os.remove(('data_files/memory_'+str(period)+'.txt'))\n",
    "\n",
    "        curr_file = open((args.novel_train_files[period]), 'r') \n",
    "        memory_file = open(('data_files/memory_'+str(period)+'.txt'), 'a')\n",
    "        Lines = curr_file.readlines()        \n",
    "        indices = np.random.permutation(len(Lines))\n",
    "        Lines = [Lines[i] for i in indices[0:((args.num_class_novel[period+1]-args.num_class_novel[period])*args.memory_size)]]\n",
    "        for line in Lines: memory_file.write(line)\n",
    "        curr_file.close()\n",
    "        memory_file.close()\n",
    "\n",
    "        # add new memory file to memory train list\n",
    "        memory_train.append('data_files/memory_'+str(period)+'.txt')\n",
    "        print('memory_train', memory_train)\n",
    "        '''------------------------------------'''\n",
    "\n",
    "        #append new class images (create new)\n",
    "        class_old = np.append(class_old, class_novel, axis=0)\n",
    "    \n",
    "    print('acc_base    : ', acc_nvld_basic)     # accuracy list\n",
    "    print('acc_finetune: ', acc_nvld_finetune)\n",
    "\n",
    "    print('xxx')\n",
    "    \n",
    "#     if args.save_model:\n",
    "#         print('saving_model')\n",
    "#         save_model(args, best_model)\n",
    "#     else: print('save is disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls True ts False cbs True\n",
      "class order: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "device_ids: [0, 1, 2]\n",
      "===================== period = 0 =========================\n",
      "class_novel: [0 1 2 3 4 5 6 7 8]\n",
      "train files: \t size:  6785  , files:  ['data_files/class0_8_train.txt']\n",
      "train files: \t size:  1260  , files:  ['data_files/class0_8_test.txt']\n",
      "current lr = 0.001000\n",
      "Training Period: 0 \t Epoch: 0 \t time = 192.6 \t loss = 0.086081 \t acc = 0.4352\n",
      "Test(n&o)Period: 0 \t Epoch: 0 \t time = 12.7 \t\t\t\t acc = 0.2992\n",
      "Training Period: 0 \t Epoch: 1 \t time = 191.1 \t loss = 0.072062 \t acc = 0.5889\n",
      "Test(n&o)Period: 0 \t Epoch: 1 \t time = 12.7 \t\t\t\t acc = 0.2968\n",
      "Training Period: 0 \t Epoch: 2 \t time = 191.1 \t loss = 0.065662 \t acc = 0.6435\n",
      "Test(n&o)Period: 0 \t Epoch: 2 \t time = 12.7 \t\t\t\t acc = 0.2913\n",
      "current lr = 0.000800\n",
      "Training Period: 0 \t Epoch: 3 \t time = 191.2 \t loss = 0.061315 \t acc = 0.6865\n",
      "Test(n&o)Period: 0 \t Epoch: 3 \t time = 12.7 \t\t\t\t acc = 0.2817\n",
      "Training Period: 0 \t Epoch: 4 \t time = 190.3 \t loss = 0.058915 \t acc = 0.7105\n",
      "Test(n&o)Period: 0 \t Epoch: 4 \t time = 12.8 \t\t\t\t acc = 0.3159\n",
      "Training Period: 0 \t Epoch: 5 \t time = 191.1 \t loss = 0.056733 \t acc = 0.7307\n",
      "Test(n&o)Period: 0 \t Epoch: 5 \t time = 12.7 \t\t\t\t acc = 0.3421\n",
      "current lr = 0.000640\n",
      "Training Period: 0 \t Epoch: 6 \t time = 191.1 \t loss = 0.054454 \t acc = 0.7539\n",
      "Test(n&o)Period: 0 \t Epoch: 6 \t time = 12.7 \t\t\t\t acc = 0.3452\n",
      "Training Period: 0 \t Epoch: 7 \t time = 190.9 \t loss = 0.053363 \t acc = 0.7637\n",
      "Test(n&o)Period: 0 \t Epoch: 7 \t time = 12.7 \t\t\t\t acc = 0.3095\n",
      "Training Period: 0 \t Epoch: 8 \t time = 191.6 \t loss = 0.051769 \t acc = 0.7742\n",
      "Test(n&o)Period: 0 \t Epoch: 8 \t time = 12.7 \t\t\t\t acc = 0.3627\n",
      "current lr = 0.000512\n",
      "Training Period: 0 \t Epoch: 9 \t time = 190.4 \t loss = 0.050613 \t acc = 0.7901\n",
      "Test(n&o)Period: 0 \t Epoch: 9 \t time = 12.8 \t\t\t\t acc = 0.3246\n",
      "Training Period: 0 \t Epoch: 10 \t time = 191.4 \t loss = 0.049907 \t acc = 0.7929\n",
      "Test(n&o)Period: 0 \t Epoch: 10 \t time = 12.7 \t\t\t\t acc = 0.3452\n",
      "Training Period: 0 \t Epoch: 11 \t time = 191.0 \t loss = 0.048943 \t acc = 0.8060\n",
      "Test(n&o)Period: 0 \t Epoch: 11 \t time = 12.8 \t\t\t\t acc = 0.3476\n",
      "current lr = 0.000410\n",
      "Training Period: 0 \t Epoch: 12 \t time = 190.0 \t loss = 0.048554 \t acc = 0.8006\n",
      "Test(n&o)Period: 0 \t Epoch: 12 \t time = 12.7 \t\t\t\t acc = 0.3484\n",
      "Training Period: 0 \t Epoch: 13 \t time = 191.0 \t loss = 0.047575 \t acc = 0.8136\n",
      "Test(n&o)Period: 0 \t Epoch: 13 \t time = 12.8 \t\t\t\t acc = 0.3667\n",
      "Training Period: 0 \t Epoch: 14 \t time = 191.0 \t loss = 0.046968 \t acc = 0.8184\n",
      "Test(n&o)Period: 0 \t Epoch: 14 \t time = 12.8 \t\t\t\t acc = 0.3532\n",
      "current lr = 0.000328\n",
      "Training Period: 0 \t Epoch: 15 \t time = 190.8 \t loss = 0.047709 \t acc = 0.8192\n",
      "Test(n&o)Period: 0 \t Epoch: 15 \t time = 12.7 \t\t\t\t acc = 0.3563\n",
      "Training Period: 0 \t Epoch: 16 \t time = 191.1 \t loss = 0.046613 \t acc = 0.8268\n",
      "Test(n&o)Period: 0 \t Epoch: 16 \t time = 12.8 \t\t\t\t acc = 0.3460\n",
      "Training Period: 0 \t Epoch: 17 \t time = 190.9 \t loss = 0.045778 \t acc = 0.8305\n",
      "Test(n&o)Period: 0 \t Epoch: 17 \t time = 12.7 \t\t\t\t acc = 0.3675\n",
      "current lr = 0.000262\n",
      "Training Period: 0 \t Epoch: 18 \t time = 190.0 \t loss = 0.045260 \t acc = 0.8442\n",
      "Test(n&o)Period: 0 \t Epoch: 18 \t time = 12.8 \t\t\t\t acc = 0.3603\n",
      "Training Period: 0 \t Epoch: 19 \t time = 190.3 \t loss = 0.045125 \t acc = 0.8407\n",
      "Test(n&o)Period: 0 \t Epoch: 19 \t time = 12.8 \t\t\t\t acc = 0.3810\n",
      "Training Period: 0 \t Epoch: 20 \t time = 190.9 \t loss = 0.046789 \t acc = 0.8221\n",
      "Test(n&o)Period: 0 \t Epoch: 20 \t time = 12.7 \t\t\t\t acc = 0.3611\n",
      "current lr = 0.000210\n",
      "Training Period: 0 \t Epoch: 21 \t time = 191.4 \t loss = 0.045711 \t acc = 0.8357\n",
      "Test(n&o)Period: 0 \t Epoch: 21 \t time = 12.8 \t\t\t\t acc = 0.3325\n",
      "Training Period: 0 \t Epoch: 22 \t time = 191.1 \t loss = 0.045426 \t acc = 0.8355\n",
      "Test(n&o)Period: 0 \t Epoch: 22 \t time = 12.8 \t\t\t\t acc = 0.3690\n",
      "Training Period: 0 \t Epoch: 23 \t time = 191.4 \t loss = 0.044454 \t acc = 0.8433\n",
      "Test(n&o)Period: 0 \t Epoch: 23 \t time = 12.8 \t\t\t\t acc = 0.3643\n",
      "current lr = 0.000168\n",
      "Training Period: 0 \t Epoch: 24 \t time = 191.3 \t loss = 0.043961 \t acc = 0.8579\n",
      "Test(n&o)Period: 0 \t Epoch: 24 \t time = 12.7 \t\t\t\t acc = 0.3810\n",
      "Training Period: 0 \t Epoch: 25 \t time = 191.3 \t loss = 0.046470 \t acc = 0.8284\n",
      "Test(n&o)Period: 0 \t Epoch: 25 \t time = 12.6 \t\t\t\t acc = 0.3635\n",
      "Training Period: 0 \t Epoch: 26 \t time = 190.3 \t loss = 0.045679 \t acc = 0.8376\n",
      "Test(n&o)Period: 0 \t Epoch: 26 \t time = 12.8 \t\t\t\t acc = 0.3643\n",
      "current lr = 0.000134\n",
      "Training Period: 0 \t Epoch: 27 \t time = 190.5 \t loss = 0.045354 \t acc = 0.8361\n",
      "Test(n&o)Period: 0 \t Epoch: 27 \t time = 12.8 \t\t\t\t acc = 0.3706\n",
      "Training Period: 0 \t Epoch: 28 \t time = 191.1 \t loss = 0.044484 \t acc = 0.8479\n",
      "Test(n&o)Period: 0 \t Epoch: 28 \t time = 12.8 \t\t\t\t acc = 0.3849\n",
      "Training Period: 0 \t Epoch: 29 \t time = 191.1 \t loss = 0.044274 \t acc = 0.8470\n",
      "Test(n&o)Period: 0 \t Epoch: 29 \t time = 12.7 \t\t\t\t acc = 0.3730\n",
      "===========================================================\n",
      "save model: checkpoint/incremental/inc_ResNet18_cbs_ls_0_012345678.pkl\n",
      "memory_train ['data_files/memory_0.txt']\n",
      "===================== period = 1 =========================\n",
      "class_novel: [ 9 10]\n",
      "train files: \t size:  684  , files:  ['data_files/memory_0.txt', 'data_files/class9_10_train.txt']\n",
      "train files: \t size:  1460  , files:  ['data_files/class0_8_test.txt', 'data_files/class9_10_test.txt']\n",
      "current lr = 0.001000\n",
      "Training Period: 1 \t Epoch: 0 \t time = 26.2 \t loss = 0.085533 \t acc = 0.5570\n",
      "Test(n&o)Period: 1 \t Epoch: 0 \t time = 14.8 \t\t\t\t acc = 0.3068\n",
      "Training Period: 1 \t Epoch: 1 \t time = 26.3 \t loss = 0.078993 \t acc = 0.6111\n",
      "Test(n&o)Period: 1 \t Epoch: 1 \t time = 14.8 \t\t\t\t acc = 0.2945\n",
      "Training Period: 1 \t Epoch: 2 \t time = 26.3 \t loss = 0.075495 \t acc = 0.6301\n",
      "Test(n&o)Period: 1 \t Epoch: 2 \t time = 14.8 \t\t\t\t acc = 0.3137\n",
      "current lr = 0.000800\n",
      "Training Period: 1 \t Epoch: 3 \t time = 26.4 \t loss = 0.071320 \t acc = 0.7018\n",
      "Test(n&o)Period: 1 \t Epoch: 3 \t time = 14.7 \t\t\t\t acc = 0.3068\n",
      "Training Period: 1 \t Epoch: 4 \t time = 26.3 \t loss = 0.070728 \t acc = 0.6857\n",
      "Test(n&o)Period: 1 \t Epoch: 4 \t time = 14.8 \t\t\t\t acc = 0.3034\n",
      "Training Period: 1 \t Epoch: 5 \t time = 26.3 \t loss = 0.070010 \t acc = 0.6827\n",
      "Test(n&o)Period: 1 \t Epoch: 5 \t time = 14.7 \t\t\t\t acc = 0.3267\n",
      "current lr = 0.000640\n",
      "Training Period: 1 \t Epoch: 6 \t time = 26.4 \t loss = 0.068952 \t acc = 0.7076\n",
      "Test(n&o)Period: 1 \t Epoch: 6 \t time = 14.8 \t\t\t\t acc = 0.3185\n",
      "Training Period: 1 \t Epoch: 7 \t time = 26.4 \t loss = 0.066820 \t acc = 0.7105\n",
      "Test(n&o)Period: 1 \t Epoch: 7 \t time = 14.7 \t\t\t\t acc = 0.3329\n",
      "Training Period: 1 \t Epoch: 8 \t time = 26.5 \t loss = 0.067548 \t acc = 0.7076\n",
      "Test(n&o)Period: 1 \t Epoch: 8 \t time = 14.8 \t\t\t\t acc = 0.3192\n",
      "current lr = 0.000512\n",
      "Training Period: 1 \t Epoch: 9 \t time = 26.4 \t loss = 0.064909 \t acc = 0.7529\n",
      "Test(n&o)Period: 1 \t Epoch: 9 \t time = 14.7 \t\t\t\t acc = 0.3404\n",
      "Training Period: 1 \t Epoch: 10 \t time = 26.3 \t loss = 0.067702 \t acc = 0.7193\n",
      "Test(n&o)Period: 1 \t Epoch: 10 \t time = 14.8 \t\t\t\t acc = 0.3247\n",
      "Training Period: 1 \t Epoch: 11 \t time = 26.2 \t loss = 0.067626 \t acc = 0.7193\n",
      "Test(n&o)Period: 1 \t Epoch: 11 \t time = 14.8 \t\t\t\t acc = 0.3329\n",
      "current lr = 0.000410\n",
      "Training Period: 1 \t Epoch: 12 \t time = 26.3 \t loss = 0.066664 \t acc = 0.7456\n",
      "Test(n&o)Period: 1 \t Epoch: 12 \t time = 14.8 \t\t\t\t acc = 0.3315\n",
      "Training Period: 1 \t Epoch: 13 \t time = 26.3 \t loss = 0.066033 \t acc = 0.7281\n",
      "Test(n&o)Period: 1 \t Epoch: 13 \t time = 14.7 \t\t\t\t acc = 0.3356\n",
      "Training Period: 1 \t Epoch: 14 \t time = 26.2 \t loss = 0.065469 \t acc = 0.7427\n",
      "Test(n&o)Period: 1 \t Epoch: 14 \t time = 14.8 \t\t\t\t acc = 0.3349\n",
      "current lr = 0.000328\n",
      "Training Period: 1 \t Epoch: 15 \t time = 26.2 \t loss = 0.068695 \t acc = 0.7251\n",
      "Test(n&o)Period: 1 \t Epoch: 15 \t time = 14.8 \t\t\t\t acc = 0.3397\n",
      "Training Period: 1 \t Epoch: 16 \t time = 26.3 \t loss = 0.067579 \t acc = 0.7251\n",
      "Test(n&o)Period: 1 \t Epoch: 16 \t time = 14.8 \t\t\t\t acc = 0.3425\n",
      "Training Period: 1 \t Epoch: 17 \t time = 26.4 \t loss = 0.067804 \t acc = 0.7339\n",
      "Test(n&o)Period: 1 \t Epoch: 17 \t time = 14.8 \t\t\t\t acc = 0.3500\n",
      "current lr = 0.000262\n",
      "Training Period: 1 \t Epoch: 18 \t time = 26.3 \t loss = 0.066210 \t acc = 0.7558\n",
      "Test(n&o)Period: 1 \t Epoch: 18 \t time = 14.8 \t\t\t\t acc = 0.3390\n",
      "Training Period: 1 \t Epoch: 19 \t time = 26.3 \t loss = 0.066040 \t acc = 0.7558\n",
      "Test(n&o)Period: 1 \t Epoch: 19 \t time = 14.8 \t\t\t\t acc = 0.3411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Period: 1 \t Epoch: 20 \t time = 26.2 \t loss = 0.070720 \t acc = 0.7178\n",
      "Test(n&o)Period: 1 \t Epoch: 20 \t time = 14.8 \t\t\t\t acc = 0.3308\n",
      "current lr = 0.000210\n",
      "Training Period: 1 \t Epoch: 21 \t time = 26.3 \t loss = 0.070455 \t acc = 0.7105\n",
      "Test(n&o)Period: 1 \t Epoch: 21 \t time = 14.7 \t\t\t\t acc = 0.3247\n",
      "Training Period: 1 \t Epoch: 22 \t time = 26.3 \t loss = 0.069058 \t acc = 0.7164\n",
      "Test(n&o)Period: 1 \t Epoch: 22 \t time = 14.7 \t\t\t\t acc = 0.3315\n",
      "Training Period: 1 \t Epoch: 23 \t time = 26.3 \t loss = 0.069533 \t acc = 0.7281\n",
      "Test(n&o)Period: 1 \t Epoch: 23 \t time = 14.7 \t\t\t\t acc = 0.3329\n",
      "current lr = 0.000168\n",
      "Training Period: 1 \t Epoch: 24 \t time = 26.3 \t loss = 0.069579 \t acc = 0.7295\n",
      "Test(n&o)Period: 1 \t Epoch: 24 \t time = 14.8 \t\t\t\t acc = 0.3329\n",
      "Training Period: 1 \t Epoch: 25 \t time = 26.3 \t loss = 0.073448 \t acc = 0.7178\n",
      "Test(n&o)Period: 1 \t Epoch: 25 \t time = 14.8 \t\t\t\t acc = 0.3267\n",
      "Training Period: 1 \t Epoch: 26 \t time = 26.3 \t loss = 0.073862 \t acc = 0.7120\n",
      "Test(n&o)Period: 1 \t Epoch: 26 \t time = 14.8 \t\t\t\t acc = 0.3370\n",
      "current lr = 0.000134\n",
      "Training Period: 1 \t Epoch: 27 \t time = 26.3 \t loss = 0.072796 \t acc = 0.6988\n",
      "Test(n&o)Period: 1 \t Epoch: 27 \t time = 14.6 \t\t\t\t acc = 0.3308\n",
      "Training Period: 1 \t Epoch: 28 \t time = 26.3 \t loss = 0.073164 \t acc = 0.7018\n",
      "Test(n&o)Period: 1 \t Epoch: 28 \t time = 14.7 \t\t\t\t acc = 0.3342\n",
      "Training Period: 1 \t Epoch: 29 \t time = 26.3 \t loss = 0.072245 \t acc = 0.7061\n",
      "Test(n&o)Period: 1 \t Epoch: 29 \t time = 14.7 \t\t\t\t acc = 0.3342\n",
      "finetune current lr = 0.000100\n",
      "finetune train size: 550\n",
      "Finetune Training Period: 1 \t Epoch: 0 \t time = 21.0 \t loss = 0.068959 \t acc = 0.7236\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 0 \t time = 14.8 \t\t\t\t acc = 0.3233\n",
      "Finetune Training Period: 1 \t Epoch: 1 \t time = 21.0 \t loss = 0.067655 \t acc = 0.7455\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 1 \t time = 14.8 \t\t\t\t acc = 0.3281\n",
      "Finetune Training Period: 1 \t Epoch: 2 \t time = 21.0 \t loss = 0.068982 \t acc = 0.7091\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 2 \t time = 14.9 \t\t\t\t acc = 0.3390\n",
      "current lr = 0.000080\n",
      "Finetune Training Period: 1 \t Epoch: 3 \t time = 21.1 \t loss = 0.067306 \t acc = 0.7273\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 3 \t time = 14.7 \t\t\t\t acc = 0.3459\n",
      "Finetune Training Period: 1 \t Epoch: 4 \t time = 21.0 \t loss = 0.068955 \t acc = 0.7127\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 4 \t time = 14.7 \t\t\t\t acc = 0.3377\n",
      "Finetune Training Period: 1 \t Epoch: 5 \t time = 21.1 \t loss = 0.067281 \t acc = 0.7255\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 5 \t time = 14.8 \t\t\t\t acc = 0.3288\n",
      "current lr = 0.000064\n",
      "Finetune Training Period: 1 \t Epoch: 6 \t time = 21.0 \t loss = 0.067334 \t acc = 0.7273\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 6 \t time = 14.8 \t\t\t\t acc = 0.3459\n",
      "Finetune Training Period: 1 \t Epoch: 7 \t time = 21.0 \t loss = 0.066665 \t acc = 0.7400\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 7 \t time = 14.7 \t\t\t\t acc = 0.3342\n",
      "Finetune Training Period: 1 \t Epoch: 8 \t time = 21.0 \t loss = 0.066574 \t acc = 0.7327\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 8 \t time = 14.7 \t\t\t\t acc = 0.3397\n",
      "current lr = 0.000051\n",
      "Finetune Training Period: 1 \t Epoch: 9 \t time = 21.1 \t loss = 0.067039 \t acc = 0.7527\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 9 \t time = 14.8 \t\t\t\t acc = 0.3418\n",
      "Finetune Training Period: 1 \t Epoch: 10 \t time = 21.1 \t loss = 0.067124 \t acc = 0.7291\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 10 \t time = 14.8 \t\t\t\t acc = 0.3342\n",
      "Finetune Training Period: 1 \t Epoch: 11 \t time = 21.1 \t loss = 0.068009 \t acc = 0.7200\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 11 \t time = 14.8 \t\t\t\t acc = 0.3425\n",
      "current lr = 0.000041\n",
      "Finetune Training Period: 1 \t Epoch: 12 \t time = 21.0 \t loss = 0.067615 \t acc = 0.7418\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 12 \t time = 14.8 \t\t\t\t acc = 0.3432\n",
      "Finetune Training Period: 1 \t Epoch: 13 \t time = 21.0 \t loss = 0.067478 \t acc = 0.7273\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 13 \t time = 14.7 \t\t\t\t acc = 0.3397\n",
      "Finetune Training Period: 1 \t Epoch: 14 \t time = 21.1 \t loss = 0.066668 \t acc = 0.7273\n",
      "Finetune Test(n&o) Period: 1 \t Epoch: 14 \t time = 14.8 \t\t\t\t acc = 0.3329\n",
      "------------------- result ------------------------\n",
      "Period: 1, basic acc = 0.3342, finetune acc = 0.3329\n",
      "---------------------------------------------------\n",
      "------------------- ave result ------------------------\n",
      "basic acc = 0.3342, finetune acc = 0.3329\n",
      "---------------------------------------------------\n",
      "===========================================================\n",
      "save model: checkpoint/incremental/inc_ResNet18_cbs_ls_0_012345678910.pkl\n",
      "memory_train ['data_files/memory_0.txt', 'data_files/memory_1.txt']\n",
      "acc_base    :  [0.37301587 0.33424658]\n",
      "acc_finetune:  [0.         0.33287671]\n",
      "xxx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "\n",
    "#python3 main.py --dataset cifar10 --alg res --data ./data/\n",
    "\n",
    "def seed_everything(seed=27):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # File locations \n",
    "    novel_train_files = ['data_files/class0_8_train.txt', 'data_files/class9_10_train.txt']\n",
    "    novel_test_files = ['data_files/class0_8_test.txt', 'data_files/class9_10_test.txt']\n",
    "    #novel_train_files = ['data_files/class0_10_train.txt']\n",
    "    #novel_test_files = ['data_files/class0_10_test.txt']\n",
    "    #novel_train_files = ['data_files/class0_8_train.txt']\n",
    "    #novel_test_files = ['data_files/class0_8_test.txt']\n",
    "    \n",
    "    \n",
    "    '''--------------------------------------------------- Arguments ------------------------------------------------------------'''\n",
    "    parser = argparse.ArgumentParser(description='Incremental learning for feature extraction')\n",
    "\n",
    "    # incremental learning\n",
    "    parser.add_argument('--epoch_base',         type=int,       default=30,          help='30')\n",
    "    parser.add_argument('--epoch_finetune',     type=int,       default=15,          help='15')\n",
    "    parser.add_argument('--batch_size',         type=int,       default=20,          help='20')\n",
    "    parser.add_argument('--period_train',       type=int,       default=2,           help='2')\n",
    "    parser.add_argument('--num_classes',        type=int,       default=11,          help='11')\n",
    "    parser.add_argument('--num_class_novel',                    default=[0,9,11],    help='[0,9,11]')\n",
    "    parser.add_argument('--memory_size',                        default=50,          help='50')\n",
    "\n",
    "    parser.add_argument('--stop_acc',           type=float,     default=0.998,       help='number of epochs')\n",
    "\n",
    "    # model\n",
    "    parser.add_argument('--alg',                type=str,       default='res',       help='res')\n",
    "\n",
    "    # datasets\n",
    "    parser.add_argument('--novel_train_files',  default=novel_train_files,           help='list of train files')\n",
    "    parser.add_argument('--novel_test_files',   default=novel_test_files,            help='list of test files')\n",
    "\n",
    "    # learning rate\n",
    "    parser.add_argument('--schedule_interval',  type=int,       default=3,           help='decay epoch rate: 3')\n",
    "    parser.add_argument('--lr',                 type=float,     default=0.001,       help='learn rate: 0.001') \n",
    "    parser.add_argument('--gamma',              type=float,     default=0.8,         help='decay lr factor: 0.8')\n",
    "    parser.add_argument('--ft_lr_factor',       type=float,     default=0.1,         help='ft learn rate: 0.1')\n",
    "    \n",
    "    # loss\n",
    "    parser.add_argument('--dist_loss',          type=str,       default='ce',        help='dist_loss')\n",
    "    parser.add_argument('--dist_loss_act',      type=str,       default='softmax',   help='dist_loss_act')\n",
    "    parser.add_argument('--dist_ratio',         type=float,     default=0.5,         help='dist_loss_ratio')\n",
    "    \n",
    "    # optimizer\n",
    "    parser.add_argument('--momentum',           type=float,     default=0.6,         help='learning momentum') \n",
    "    parser.add_argument('--decay',              type=float,     default=0.0001,      help='learning rate')\n",
    "    \n",
    "    # Label smoothing\n",
    "    parser.add_argument('--use_ls',             type=bool,      default=True,        help='list of test files')\n",
    "\n",
    "    # Temperature scaling\n",
    "    parser.add_argument('--use_ts',             type=bool,      default=False,       help='use temp_scale')\n",
    "    parser.add_argument('--tscale',             type=float,     default=3.0,         help='Temp scaling')\n",
    "    \n",
    "    # CBS ARGS\n",
    "    parser.add_argument('--use_cbs',            type=bool,      default=True,       help='use CBS')\n",
    "    parser.add_argument('--std',                type=float,     default=1.0,         help='')\n",
    "    parser.add_argument('--std_factor',         type=float,     default=0.9,         help='')\n",
    "    parser.add_argument('--cbs_epoch',          type=int,       default=5,           help='')\n",
    "    parser.add_argument('--kernel_size',        type=int,       default=3,           help='')\n",
    "    parser.add_argument('--fil1',               type=str,       default='LOG',       help='gau, LOG')\n",
    "    parser.add_argument('--fil2',               type=str,       default='gau',       help='gau, LOG')\n",
    "    parser.add_argument('--fil3',               type=str,       default='gau',       help='gau, LOG')\n",
    "    \n",
    "    parser.add_argument('--save_model',         type=bool,      default=False,       help='store_true')\n",
    "    parser.add_argument('--checkpointfile',     type=str,       default='checkpoint/incremental/inc_ResNet18_cbs_ls')\n",
    "   \n",
    "    args = parser.parse_args(args=[])\n",
    "    '''-------------------------------------------------------------------------------------------------------------------------'''\n",
    "    \n",
    "    if torch.cuda.is_available(): args.cuda = True\n",
    "    \n",
    "    seed_everything()\n",
    "    main(args)\n",
    "\n",
    "#python3 main.py --dataset cifar10 --alg res --data ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "if args.use_ts: print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch count: 0 \t accuracy: 42.86\n",
    "best acc: 0 \t best acc: 42.86\n",
    "epoch count: 1 \t accuracy: 48.08\n",
    "best acc: 1 \t best acc: 48.08\n",
    "epoch count: 2 \t accuracy: 50.50\n",
    "best acc: 2 \t best acc: 50.50\n",
    "epoch count: 3 \t accuracy: 57.78\n",
    "best acc: 3 \t best acc: 57.78\n",
    "epoch count: 4 \t accuracy: 58.14\n",
    "best acc: 4 \t best acc: 58.14\n",
    "epoch count: 5 \t accuracy: 63.66\n",
    "best acc: 5 \t best acc: 63.66\n",
    "epoch count: 6 \t accuracy: 63.52\n",
    "best acc: 5 \t best acc: 63.66\n",
    "epoch count: 7 \t accuracy: 62.68\n",
    "best acc: 5 \t best acc: 63.66\n",
    "epoch count: 8 \t accuracy: 67.59\n",
    "best acc: 8 \t best acc: 67.59\n",
    "epoch count: 9 \t accuracy: 62.43\n",
    "best acc: 8 \t best acc: 67.59\n",
    "epoch count: 10 \t accuracy: 68.83\n",
    "best acc: 10 \t best acc: 68.83\n",
    "epoch count: 11 \t accuracy: 70.76\n",
    "best acc: 11 \t best acc: 70.76\n",
    "epoch count: 12 \t accuracy: 70.52\n",
    "best acc: 11 \t best acc: 70.76\n",
    "epoch count: 13 \t accuracy: 66.54\n",
    "best acc: 11 \t best acc: 70.76\n",
    "epoch count: 14 \t accuracy: 67.69\n",
    "best acc: 11 \t best acc: 70.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
