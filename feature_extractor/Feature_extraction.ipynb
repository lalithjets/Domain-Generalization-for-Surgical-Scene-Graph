{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for Graph-based Surgical Scene Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Learning Domain Generaliazation with Graph Neural Network for Surgical Scene Understanding. Lab:MMLAB, National University of Singapore. Contributors: Lalith, Mobarak. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features used by Graph Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {}\n",
    "\n",
    "# data['global_id'] = global_id                                       # image name\n",
    "# data['img_name']     = global_id + '.jpg'                           # image name\n",
    "# data['node_num']    = single_app_data['node_num'].value             # total node number\n",
    "\n",
    "# data['roi_labels']      = single_app_data['classes'][:]             # node labels\n",
    "# data['edge_labels'] = single_app_data['edge_labels'][:]             # edge  labels\n",
    "\n",
    "# data['det_boxes'] = single_app_data['boxes'][:]                     # box\n",
    "# data['roi_scores'] = single_app_data['scores'][:]                   # detection score\n",
    "\n",
    "# data['edge_num']    = data['edge_labels'].shape[0]                  # edge number\n",
    "# data['features']        = single_app_data['feature'][:]             # features\n",
    "# data['spatial_feat'] = single_spatial_data[:]                       # spatial features\n",
    "\n",
    "# data['word2vec']     = self._get_word2vec(data['roi_labels'])       # word2vec, from roi_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def center_offset(box1, box2, im_wh):\n",
    "    '''\n",
    "    '''\n",
    "    c1 = [(box1[2]+box1[0])/2, (box1[3]+box1[1])/2]\n",
    "    c2 = [(box2[2]+box2[0])/2, (box2[3]+box2[1])/2]\n",
    "    offset = np.array(c1)-np.array(c2)/np.array(im_wh)\n",
    "    return offset\n",
    "\n",
    "\n",
    "def box_with_respect_to_img(box, im_wh):\n",
    "    '''\n",
    "        To get [x1/W, y1/H, x2/W, y2/H, A_box/A_img]\n",
    "    '''\n",
    "    # ipdb.set_trace()\n",
    "    feats = [box[0]/(im_wh[0]+ 1e-6), box[1]/(im_wh[1]+ 1e-6), box[2]/(im_wh[0]+ 1e-6), box[3]/(im_wh[1]+ 1e-6)]\n",
    "    box_area = (box[2]-box[0])*(box[3]-box[1])\n",
    "    img_area = im_wh[0]*im_wh[1]\n",
    "    feats +=[ box_area/(img_area+ 1e-6) ]\n",
    "    return feats\n",
    "\n",
    "\n",
    "def box1_with_respect_to_box2(box1, box2):\n",
    "    '''\n",
    "    '''\n",
    "    feats = [ (box1[0]-box2[0])/(box2[2]-box2[0]+1e-6),\n",
    "              (box1[1]-box2[1])/(box2[3]-box2[1]+ 1e-6),\n",
    "              np.log((box1[2]-box1[0])/(box2[2]-box2[0]+ 1e-6)),\n",
    "              np.log((box1[3]-box1[1])/(box2[3]-box2[1]+ 1e-6))   \n",
    "            ]\n",
    "    return feats\n",
    "\n",
    "\n",
    "def calculate_spatial_feats(det_boxes, im_wh):\n",
    "    '''\n",
    "    stand-alone extract spatial features\n",
    "    size = node x (node-1), 16 (5 + 5 + 4 + 2)\n",
    "    '''\n",
    "    spatial_feats = []\n",
    "    for i in range(det_boxes.shape[0]):\n",
    "        for j in range(det_boxes.shape[0]):\n",
    "            if j == i: continue\n",
    "            single_feat = []\n",
    "            # features 5, 5, 4, 2\n",
    "            box1_wrt_img = box_with_respect_to_img(det_boxes[i], im_wh)\n",
    "            box2_wrt_img = box_with_respect_to_img(det_boxes[j], im_wh)\n",
    "            box1_wrt_box2 = box1_with_respect_to_box2(det_boxes[i], det_boxes[j])\n",
    "            offset = center_offset(det_boxes[i], det_boxes[j], im_wh)\n",
    "            \n",
    "            single_feat = single_feat + box1_wrt_img + box2_wrt_img + box1_wrt_box2 + offset.tolist()\n",
    "            spatial_feats.append(single_feat)\n",
    "    \n",
    "    spatial_feats = np.array(spatial_feats)\n",
    "    return spatial_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument Segmentation challange dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import h5py\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torchvision.models\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from models.resnet import *\n",
    "\n",
    "if sys.version_info[0] == 2: import xml.etree.cElementTree as ET\n",
    "else: import xml.etree.ElementTree as ET\n",
    "\n",
    "# input data and IO folder location\n",
    "mlist = [1,2,3,4,5,6,7,9,10,11,12,14,15,16]\n",
    "dir_root_gt = '../datasets/instruments18/seq_'\n",
    "\n",
    "xml_dir_list = []\n",
    "for i in mlist:\n",
    "    xml_dir_temp = dir_root_gt + str(i) + '/xml/'\n",
    "    seq_list_each = glob(xml_dir_temp + '/*.xml')\n",
    "    xml_dir_list = xml_dir_list + seq_list_each\n",
    "    \n",
    "# global variables\n",
    "INSTRUMENT_CLASSES = ('kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "\n",
    "ACTION_CLASSES = (  'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                    'Tool_Manipulation', 'Cutting', 'Cauterization',\n",
    "                    'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing')\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "# arguments\n",
    "parser = argparse.ArgumentParser(description='feature extractor')\n",
    "parser.add_argument('--use_cbs',            type=bool,      default=True,        help='use CBS')\n",
    "parser.add_argument('--std',                type=float,     default=1.0,         help='')\n",
    "parser.add_argument('--std_factor',         type=float,     default=0.9,         help='')\n",
    "parser.add_argument('--cbs_epoch',          type=int,       default=5,           help='')\n",
    "parser.add_argument('--kernel_size',        type=int,       default=3,           help='')\n",
    "parser.add_argument('--fil1',               type=str,       default='LOG',       help='gau, LOG')\n",
    "parser.add_argument('--fil2',               type=str,       default='gau',       help='gau, LOG')\n",
    "parser.add_argument('--fil3',               type=str,       default='gau',       help='gau, LOG')\n",
    "\n",
    "# for 9 class\n",
    "#parser.add_argument('--savedir',            type=str,       default='vsgat/resnet18_09_cbs_ls')\n",
    "#parser.add_argument('--num_classes',        type=int,       default=9,           help='11')\n",
    "#parser.add_argument('--modelpath',           type=str,       default='checkpoint/base/ResNet18_cbs_ls_0_012345678.pkl')\n",
    "\n",
    "# for 11 class\n",
    "parser.add_argument('--savedir',            type=str,       default='vsgat/resnet18_11_cbs_ts_test')\n",
    "parser.add_argument('--num_classes',        type=int,       default=11,           help='11')\n",
    "parser.add_argument('--modelpath',          type=str,       default='checkpoint/incremental/inc_ResNet18_cbs_ts_0_012345678910.pkl')\n",
    "\n",
    "# vanilla\n",
    "# parser.add_argument('--savedir',            type=str,       default='vsgat/resnet18_vanilla')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# declare fearure extraction model\n",
    "vanilla_model = False\n",
    "if vanilla_model:\n",
    "    feature_network = ResNet18_vanilla()\n",
    "else:\n",
    "    feature_network = ResNet18(args)\n",
    "\n",
    "# Set data parallel based on GPU\n",
    "num_gpu = torch.cuda.device_count()\n",
    "if num_gpu > 0:\n",
    "    device_ids = np.arange(num_gpu).tolist()\n",
    "    feature_network = nn.DataParallel(feature_network, device_ids=device_ids)\n",
    "\n",
    "# remove the last linear layer for feature extraction\n",
    "if not vanilla_model:\n",
    "    if args.use_cbs: feature_network.module.get_new_kernels(0)\n",
    "    feature_network.load_state_dict(torch.load(args.modelpath))\n",
    "    if args.use_cbs: feature_network = nn.Sequential(*list(feature_network.module.children())[:-2])\n",
    "    else: feature_network = nn.Sequential(*list(feature_network.module.children())[:-1])\n",
    "\n",
    "# Use Cuda\n",
    "feature_network = feature_network.cuda()\n",
    "\n",
    "        \n",
    "for index, _xml_dir in  enumerate(xml_dir_list):\n",
    "    img_name = os.path.basename(xml_dir_list[index][:-4])\n",
    "    _img_dir = os.path.dirname(os.path.dirname(xml_dir_list[index])) + '/left_frames/' + img_name + '.png'\n",
    "    save_data_path = os.path.join(os.path.dirname(os.path.dirname(xml_dir_list[index])),args.savedir)\n",
    "\n",
    "    if not os.path.exists(save_data_path):\n",
    "        os.makedirs(save_data_path)\n",
    "    \n",
    "    _xml = ET.parse(_xml_dir).getroot()\n",
    "    \n",
    "    det_classes = []\n",
    "    act_classes = []\n",
    "    det_boxes_all = []\n",
    "    c_flag = False\n",
    "    \n",
    "    for obj in _xml.iter('objects'):\n",
    "        # object name and interaction type\n",
    "        name = obj.find('name').text.strip()\n",
    "        interact = obj.find('interaction').text.strip()\n",
    "        det_classes.append(INSTRUMENT_CLASSES.index(str(name)))\n",
    "        act_classes.append(ACTION_CLASSES.index(str(interact)))\n",
    "        \n",
    "        # bounding box\n",
    "        bndbox = []\n",
    "        bbox = obj.find('bndbox') \n",
    "        for i, pt in enumerate(['xmin', 'ymin', 'xmax', 'ymax']):         \n",
    "            bndbox.append(int(bbox.find(pt).text))\n",
    "        det_boxes_all.append(np.array(bndbox))\n",
    "        \n",
    "    if c_flag: continue\n",
    "        \n",
    "    tissue_num = len(np.where(np.array(det_classes)==0)[0])\n",
    "    node_num = len(det_classes)\n",
    "    if tissue_num > 0: edges = np.cumsum(node_num - np.arange(tissue_num) -1)[-1]\n",
    "    else: edges = 0\n",
    "\n",
    "    # parse the original data to get node labels\n",
    "    edge_labels = np.zeros((edges, len(ACTION_CLASSES)))\n",
    "    edge_index = 0\n",
    "    for tissue in range (tissue_num):\n",
    "        for obj_index in range(tissue+1, node_num):\n",
    "            edge_labels[edge_index, act_classes[tissue_num+edge_index]] = 1 \n",
    "            edge_index += 1\n",
    "\n",
    "    # node features\n",
    "    node_features = np.zeros((node_num, 512))\n",
    "    _img = Image.open(_img_dir).convert('RGB')\n",
    "    _img = np.array(_img)\n",
    "    for idx, bndbox in enumerate(det_boxes_all):\n",
    "        roi = np.array(bndbox).astype(int)\n",
    "        roi_image = _img[roi[1]:roi[3] + 1, roi[0]:roi[2] + 1, :]\n",
    "        roi_image = transform(cv2.resize(roi_image, (224, 224), interpolation=cv2.INTER_LINEAR))\n",
    "        roi_image = torch.autograd.Variable(roi_image.unsqueeze(0)).cuda()\n",
    "        feature = feature_network(roi_image)\n",
    "        feature = feature.view(feature.size(0), -1)\n",
    "        node_features[idx] = feature.data.cpu().numpy()\n",
    "\n",
    "    # spatial_features\n",
    "    spatial_features = np.array(calculate_spatial_feats(np.array(det_boxes_all), [1024, 1280]))\n",
    "\n",
    "    # save to file\n",
    "    hdf5_file = h5py.File(os.path.join(save_data_path, '{}_features.hdf5'.format(img_name)),'w')\n",
    "    hdf5_file.create_dataset('img_name', data=img_name)\n",
    "    hdf5_file.create_dataset('node_num', data=node_num)\n",
    "    hdf5_file.create_dataset('classes', data=det_classes)\n",
    "    hdf5_file.create_dataset('boxes', data=det_boxes_all)\n",
    "    hdf5_file.create_dataset('edge_labels', data=edge_labels)\n",
    "    hdf5_file.create_dataset('node_features', data=node_features)\n",
    "    hdf5_file.create_dataset('spatial_features', data=spatial_features)\n",
    "    hdf5_file.close()\n",
    "    print('edges', edge_labels.shape, 'node_feat', node_features.shape, 'spatial_feat', spatial_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGT TORS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import h5py\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torchvision.models\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "if sys.version_info[0] == 2: import xml.etree.cElementTree as ET\n",
    "else: import xml.etree.ElementTree as ET\n",
    "\n",
    "# input data and IO folder location\n",
    "mlist = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]\n",
    "dir_root_gt = '../datasets/SGH_dataset_2020/'\n",
    "\n",
    "xml_dir_list = []\n",
    "for i in mlist:\n",
    "    xml_dir_temp = dir_root_gt + str(i) + '/xml/'\n",
    "    seq_list_each = glob(xml_dir_temp + '/*.xml')\n",
    "    xml_dir_list = xml_dir_list + seq_list_each\n",
    "    \n",
    "# global variables\n",
    "INSTRUMENT_CLASSES = ('tissue', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "\n",
    "ACTION_CLASSES = (  'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                    'Tool_Manipulation', 'Cutting', 'Cauterization',\n",
    "                    'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing')\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    ])\n",
    "\n",
    "# arguments\n",
    "parser = argparse.ArgumentParser(description='feature extractor')\n",
    "parser.add_argument('--use_cbs',            type=bool,      default=True,        help='use CBS')\n",
    "parser.add_argument('--std',                type=float,     default=1.0,         help='')\n",
    "parser.add_argument('--std_factor',         type=float,     default=0.9,         help='')\n",
    "parser.add_argument('--cbs_epoch',          type=int,       default=5,           help='')\n",
    "parser.add_argument('--kernel_size',        type=int,       default=3,           help='')\n",
    "parser.add_argument('--fil1',               type=str,       default='LOG',       help='gau, LOG')\n",
    "parser.add_argument('--fil2',               type=str,       default='gau',       help='gau, LOG')\n",
    "parser.add_argument('--fil3',               type=str,       default='gau',       help='gau, LOG')\n",
    "\n",
    "# for 9 class\n",
    "#parser.add_argument('--savedir',            type=str,       default='vsgat/resnet18_09_cbs_ls')\n",
    "#parser.add_argument('--num_classes',        type=int,       default=9,           help='11')\n",
    "#parser.add_argument('--modelpath',           type=str,       default='checkpoint/base/ResNet18_cbs_ls_0_012345678.pkl')\n",
    "\n",
    "# for 11 class\n",
    "parser.add_argument('--savedir',            type=str,       default='vsgat/resnet18_11_cbs_ts')\n",
    "parser.add_argument('--num_classes',        type=int,       default=11,           help='11')\n",
    "parser.add_argument('--modelpath',          type=str,       default='checkpoint/incremental/inc_ResNet18_cbs_ts_0_012345678910.pkl')\n",
    "\n",
    "# vanilla\n",
    "#parser.add_argument('--savedir',            type=str,       default='vsgat/resnet18_vanilla')\n",
    "args = parser.parse_args(args=[])\n",
    "    \n",
    "# Feature extraction network\n",
    "vanilla_model = False\n",
    "if vanilla_model:\n",
    "    feature_network = ResNet18_vanilla()\n",
    "else:\n",
    "    feature_network = ResNet18(args)\n",
    "\n",
    "# Use dataparallel for GPU\n",
    "num_gpu = torch.cuda.device_count()\n",
    "if num_gpu > 0:\n",
    "    device_ids = np.arange(num_gpu).tolist()\n",
    "    feature_network = nn.DataParallel(feature_network, device_ids=device_ids)\n",
    "    \n",
    "# Remove last layer from feature extraction network\n",
    "if not vanilla_model:\n",
    "    if args.use_cbs: feature_network.module.get_new_kernels(0)\n",
    "    feature_network.load_state_dict(torch.load(args.modelpath))\n",
    "    if args.use_cbs: feature_network = nn.Sequential(*list(feature_network.module.children())[:-2])\n",
    "    else: feature_network = nn.Sequential(*list(feature_network.module.children())[:-1])\n",
    "\n",
    "# Use Cuda\n",
    "feature_network = feature_network.cuda()\n",
    "\n",
    "\n",
    "for index, _xml_dir in  enumerate(xml_dir_list):\n",
    "    img_name = os.path.basename(xml_dir_list[index][:-4])\n",
    "    _img_dir = os.path.dirname(os.path.dirname(xml_dir_list[index])) + '/resized_frames/' + img_name + '.png'\n",
    "    save_data_path = os.path.join(os.path.dirname(os.path.dirname(xml_dir_list[index])),args.savedir)\n",
    "    if not os.path.exists(save_data_path):\n",
    "        os.makedirs(save_data_path)\n",
    "    \n",
    "    _xml = ET.parse(_xml_dir).getroot()\n",
    "    \n",
    "    det_classes = []\n",
    "    act_classes = []\n",
    "    det_boxes_all = []\n",
    "    c_flag = False\n",
    "    \n",
    "    for obj in _xml.iter('objects'):\n",
    "        # object name and interaction type\n",
    "        name = obj.find('name').text.strip()\n",
    "        interact = obj.find('interaction').text.strip()\n",
    "        det_classes.append(INSTRUMENT_CLASSES.index(str(name)))\n",
    "        act_classes.append(ACTION_CLASSES.index(str(interact)))\n",
    "        \n",
    "        # bounding box\n",
    "        bndbox = []\n",
    "        bbox = obj.find('bndbox') \n",
    "        for i, pt in enumerate(['xmin', 'ymin', 'xmax', 'ymax']):         \n",
    "            bndbox.append(int(bbox.find(pt).text))\n",
    "        det_boxes_all.append(np.array(bndbox))\n",
    "        \n",
    "    if c_flag: continue\n",
    "        \n",
    "    tissue_num = len(np.where(np.array(det_classes)==0)[0])\n",
    "    node_num = len(det_classes)\n",
    "    if tissue_num > 0: edges = np.cumsum(node_num - np.arange(tissue_num) -1)[-1]\n",
    "    else: edges = 0\n",
    "\n",
    "    # parse the original data to get node labels\n",
    "    edge_labels = np.zeros((edges, len(ACTION_CLASSES)))\n",
    "    edge_index = 0\n",
    "    for tissue in range (tissue_num):\n",
    "        for obj_index in range(tissue+1, node_num):\n",
    "            edge_labels[edge_index, act_classes[tissue_num+edge_index]] = 1 \n",
    "            edge_index += 1\n",
    "\n",
    "    # roi features extraction\n",
    "    # node features\n",
    "    node_features = np.zeros((node_num, 512))\n",
    "    _img = Image.open(_img_dir).convert('RGB')\n",
    "    _img = np.array(_img)\n",
    "    for idx, bndbox in enumerate(det_boxes_all):\n",
    "        roi = np.array(bndbox).astype(int)\n",
    "        roi_image = _img[roi[1]:roi[3] + 1, roi[0]:roi[2] + 1, :]\n",
    "        roi_image = transform(cv2.resize(roi_image, (224, 224), interpolation=cv2.INTER_LINEAR))\n",
    "        roi_image = torch.autograd.Variable(roi_image.unsqueeze(0)).cuda()\n",
    "        feature = feature_network(roi_image)\n",
    "        feature = feature.view(feature.size(0), -1)\n",
    "        node_features[idx] = feature.data.cpu().numpy()\n",
    "\n",
    "    # spatial_features\n",
    "    spatial_features = np.array(calculate_spatial_feats(np.array(det_boxes_all), [1024, 1280]))\n",
    "\n",
    "    # save to file\n",
    "    hdf5_file = h5py.File(os.path.join(save_data_path, '{}_features.hdf5'.format(img_name)),'w')\n",
    "    hdf5_file.create_dataset('img_name', data=img_name)\n",
    "    hdf5_file.create_dataset('node_num', data=node_num)\n",
    "    hdf5_file.create_dataset('classes', data=det_classes)\n",
    "    hdf5_file.create_dataset('boxes', data=det_boxes_all)\n",
    "    hdf5_file.create_dataset('edge_labels', data=edge_labels)\n",
    "    hdf5_file.create_dataset('node_features', data=node_features)\n",
    "    hdf5_file.create_dataset('spatial_features', data=spatial_features)\n",
    "    hdf5_file.close()\n",
    "    print('edges', edge_labels.shape, 'node_feat', node_features.shape, 'spatial_feat', spatial_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import gensim\n",
    "\n",
    "#Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../datasets/word2vec/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "original_keys = list(model.vocab.keys())\n",
    "upper_keys = [str.upper(x) for x in original_keys]\n",
    "\n",
    "# class names\n",
    "INSTRUMENT_CLASSES = ('kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery', 'tissue')\n",
    "\n",
    "instrument_class_to_w2v = ['kidney', 'bipolar', 'grasp', 'needle', \n",
    "                           'scissors', 'ultrasound', 'suction', 'clipper', \n",
    "                           'stapler', 'dissector', 'cautery','tissue']\n",
    "\n",
    "hico_word2vec = os.path.join('../datasets/','surgicalscene_word2vec.hdf5')\n",
    "file = h5py.File(hico_word2vec, 'w')\n",
    "\n",
    "for i, name in enumerate(INSTRUMENT_CLASSES):\n",
    "    print(name, ':', instrument_class_to_w2v[i])\n",
    "    if name == '': continue\n",
    "    else: \n",
    "        index = upper_keys.index(str.upper(instrument_class_to_w2v[i]))\n",
    "        data = data=model[original_keys[index]]\n",
    "        print(data.shape)\n",
    "        file.create_dataset(name, data=model[original_keys[index]])\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
