{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "arguments\n",
    "    Hyperparameters, file location, optimizer, network, data_processing\n",
    "'''\n",
    "\n",
    "class arguments():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # hyper parameters\n",
    "        self.lr = 0.00001\n",
    "        self.epoch = 351 \n",
    "        self.start_epoch = 0\n",
    "        self.batch_size = 32\n",
    "        self.gpu = True\n",
    "        self.print_every = 10 \n",
    "        self.train_model = 'epoch'\n",
    "        self.exp_ver='g_resnet18_09_ls'\n",
    "\n",
    "        # file locations\n",
    "        self.log_dir = './log/g_resnet18_09_ls'\n",
    "        self.save_dir = './checkpoints/g_resnet18_09_ls'\n",
    "        self.output_img_dir = './results/g_resnet18_09_ls'\n",
    "        self.save_every = 10\n",
    "        self.pretrained = None                 \n",
    "\n",
    "        # optimizer\n",
    "        self.optim='adam' # choices=['sgd', 'adam']\n",
    "\n",
    "        # network\n",
    "        self.layers= 1\n",
    "        self.bn = False\n",
    "        self.drop_prob = 0.3\n",
    "        self.bias = True\n",
    "        self.multi_attn = False\n",
    "        self.diff_edge = False\n",
    "\n",
    "        # data_processing\n",
    "        self.sampler = 0\n",
    "        self.data_aug = False\n",
    "        self.feature_extractor = 'resnet18_09_ls'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "configurations of the network\n",
    "    \n",
    "    readout: G_ER_L_S = [1024+300+16+300+1024,  1024, 117]\n",
    "\n",
    "    node_func: G_N_L_S = [1024+1024, 1024]\n",
    "    node_lang_func: G_N_L_S2 = [300+300+300]\n",
    "    \n",
    "    edge_func : G_E_L_S = [1024*2+16, 1024]\n",
    "    edge_lang_func: [300*2, 1024]\n",
    "    \n",
    "    attn: [1024, 1]\n",
    "    attn_lang: [1024, 1]\n",
    "'''\n",
    "\n",
    "class CONFIGURATION(object):\n",
    "    '''\n",
    "    Configuration arguments: feature type, layer, bias, batch normalization, dropout, multi-attn\n",
    "    \n",
    "    readout           : fc_size, activation, bias, bn, droupout\n",
    "    gnn_node          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_node_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    gnn_edge          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_edge_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    gnn_attn          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_attn_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    '''\n",
    "    def __init__(self, layer=1, bias=True, bn=False, dropout=0.2, multi_attn=False):\n",
    "        \n",
    "        # if multi_attn:\n",
    "        if True:\n",
    "            if layer==1:\n",
    "                feature_size = 512\n",
    "                # readout\n",
    "                self.G_ER_L_S = [feature_size+300+16+300+feature_size, feature_size, 13]\n",
    "                self.G_ER_A   = ['ReLU', 'Identity']\n",
    "                self.G_ER_B   = bias    #true\n",
    "                self.G_ER_BN  = bn      #false\n",
    "                self.G_ER_D   = dropout #0.3\n",
    "                # self.G_ER_GRU = feature_size\n",
    "\n",
    "                # # gnn node function\n",
    "                self.G_N_L_S = [feature_size+feature_size, feature_size]\n",
    "                self.G_N_A   = ['ReLU']\n",
    "                self.G_N_B   = bias #true\n",
    "                self.G_N_BN  = bn      #false\n",
    "                self.G_N_D   = dropout #0.3\n",
    "                # self.G_N_GRU = feature_size\n",
    "\n",
    "                # # gnn node function for language\n",
    "                self.G_N_L_S2 = [300+300, 300]\n",
    "                self.G_N_A2   = ['ReLU']\n",
    "                self.G_N_B2   = bias    #true\n",
    "                self.G_N_BN2  = bn      #false\n",
    "                self.G_N_D2   = dropout #0.3\n",
    "                # self.G_N_GRU2 = feature_size\n",
    "\n",
    "                # gnn edge function1\n",
    "                self.G_E_L_S = [feature_size*2+16, feature_size]\n",
    "                self.G_E_A   = ['ReLU']\n",
    "                self.G_E_B   = bias     #true\n",
    "                self.G_E_BN  = bn       #false\n",
    "                self.G_E_D   = dropout  #0.3\n",
    "\n",
    "                # gnn edge function2 for language\n",
    "                self.G_E_L_S2 = [300*2, feature_size]\n",
    "                self.G_E_A2   = ['ReLU']\n",
    "                self.G_E_B2   = bias     #true\n",
    "                self.G_E_BN2  = bn       #false\n",
    "                self.G_E_D2   = dropout  #0.3\n",
    "\n",
    "                # gnn attention mechanism\n",
    "                self.G_A_L_S = [feature_size, 1]\n",
    "                self.G_A_A   = ['LeakyReLU']\n",
    "                self.G_A_B   = bias     #true\n",
    "                self.G_A_BN  = bn       #false\n",
    "                self.G_A_D   = dropout  #0.3\n",
    "\n",
    "                # gnn attention mechanism2 for language\n",
    "                self.G_A_L_S2 = [feature_size, 1]\n",
    "                self.G_A_A2   = ['LeakyReLU']\n",
    "                self.G_A_B2   = bias    #true\n",
    "                self.G_A_BN2  = bn      #false\n",
    "                self.G_A_D2   = dropout #0.3\n",
    "                    \n",
    "    def save_config(self):\n",
    "        model_config = {'graph_head':{}, 'graph_node':{}, 'graph_edge':{}, 'graph_attn':{}}\n",
    "        CONFIG=self.__dict__\n",
    "        for k, v in CONFIG.items():\n",
    "            if 'G_H' in k:\n",
    "                model_config['graph_head'][k]=v\n",
    "            elif 'G_N' in k:\n",
    "                model_config['graph_node'][k]=v\n",
    "            elif 'G_E' in k:\n",
    "                model_config['graph_edge'][k]=v\n",
    "            elif 'G_A' in k:\n",
    "                model_config['graph_attn'][k]=v\n",
    "            else:\n",
    "                model_config[k]=v\n",
    "        \n",
    "        return model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primary activation and MLP layer\n",
    "acivation:\n",
    "    Identity\n",
    "    ReLU\n",
    "    LeakyReLU\n",
    "MLP:\n",
    "    init: layer size, activation, bias, use_BN, dropout_probability\n",
    "    forward: x\n",
    "'''\n",
    "\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    '''\n",
    "    Identity class activation layer\n",
    "    x = x\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Identity,self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def get_activation(name):\n",
    "    '''\n",
    "    get_activation sub-function\n",
    "    argument: activatoin name (eg. ReLU, Identity, LeakyReLU)\n",
    "    '''\n",
    "    if name=='ReLU': return nn.ReLU(inplace=True)\n",
    "    elif name=='Identity': return Identity()\n",
    "    elif name=='LeakyReLU': return nn.LeakyReLU(0.2,inplace=True)\n",
    "    else: assert(False), 'Not Implemented'\n",
    "    #elif name=='Tanh': return nn.Tanh()\n",
    "    #elif name=='Sigmoid': return nn.Sigmoid()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Args:\n",
    "        layer_sizes: a list, [1024,1024,...]\n",
    "        activation: a list, ['ReLU', 'Tanh',...]\n",
    "        bias : bool\n",
    "        use_bn: bool\n",
    "        drop_prob: default is None, use drop out layer or not\n",
    "    '''\n",
    "    def __init__(self, layer_sizes, activation, bias=True, use_bn=False, drop_prob=None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.bn = use_bn\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=bias)\n",
    "            activate = get_activation(activation[i])\n",
    "            block = nn.Sequential(OrderedDict([(f'L{i}', layer), ]))\n",
    "            \n",
    "            # !NOTE:# Actually, it is inappropriate to use batch-normalization here\n",
    "            if use_bn:                                  \n",
    "                bn = nn.BatchNorm1d(layer_sizes[i+1])\n",
    "                block.add_module(f'B{i}', bn)\n",
    "            \n",
    "            # batch normalization is put before activation function \n",
    "            block.add_module(f'A{i}', activate)\n",
    "\n",
    "            # dropout probablility\n",
    "            if drop_prob:\n",
    "                block.add_module(f'D{i}', nn.Dropout(drop_prob))\n",
    "            \n",
    "            self.layers.append(block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # !NOTE: sometime the shape of x will be [1,N], and we cannot use batch-normailzation in that situation\n",
    "            if self.bn and x.shape[0]==1:\n",
    "                x = layer[0](x)\n",
    "                x = layer[:-1](x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "H_H_EdgeApplyModule\n",
    "    init    : config, multi_attn \n",
    "    forward : edge\n",
    "    \n",
    "H_NodeApplyModule\n",
    "    init    : config\n",
    "    forward : node\n",
    "    \n",
    "E_AttentionModule1\n",
    "    init    : config\n",
    "    forward : edge\n",
    "    \n",
    "GNN\n",
    "    init    : config, multi_attn, diff_edge\n",
    "    forward : g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_features\n",
    "    \n",
    "GRNN\n",
    "    init    : config, multi_attn, diff_edge\n",
    "    forward : b_graph, b_h_node_list, b_o_node_list, b_h_h_e_list, b_o_o_e_list, b_h_o_e_list, features, spatial_features, word2vec, valid, pop_features, initial_features\n",
    "'''\n",
    "\n",
    "import ipdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class H_H_EdgeApplyModule(nn.Module): #human to human edge\n",
    "    '''\n",
    "        init    : config, multi_attn \n",
    "        forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False):\n",
    "        super(H_H_EdgeApplyModule, self).__init__()\n",
    "        self.edge_fc = MLP(CONFIG.G_E_L_S, CONFIG.G_E_A, CONFIG.G_E_B, CONFIG.G_E_BN, CONFIG.G_E_D)\n",
    "        self.edge_fc_lang = MLP(CONFIG.G_E_L_S2, CONFIG.G_E_A2, CONFIG.G_E_B2, CONFIG.G_E_BN2, CONFIG.G_E_D2)\n",
    "    \n",
    "    def forward(self, edge):\n",
    "        feat = torch.cat([edge.src['n_f'], edge.data['s_f'], edge.dst['n_f']], dim=1)\n",
    "        feat_lang = torch.cat([edge.src['word2vec'], edge.dst['word2vec']], dim=1)\n",
    "        e_feat = self.edge_fc(feat)\n",
    "        e_feat_lang = self.edge_fc_lang(feat_lang)\n",
    "  \n",
    "        return {'e_f': e_feat, 'e_f_lang': e_feat_lang}\n",
    "\n",
    "class H_NodeApplyModule(nn.Module): #human node\n",
    "    '''\n",
    "        init    : config\n",
    "        forward : node\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(H_NodeApplyModule, self).__init__()\n",
    "        self.node_fc = MLP(CONFIG.G_N_L_S, CONFIG.G_N_A, CONFIG.G_N_B, CONFIG.G_N_BN, CONFIG.G_N_D)\n",
    "        self.node_fc_lang = MLP(CONFIG.G_N_L_S2, CONFIG.G_N_A2, CONFIG.G_N_B2, CONFIG.G_N_BN2, CONFIG.G_N_D2)\n",
    "    \n",
    "    def forward(self, node):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        feat = torch.cat([node.data['n_f'], node.data['z_f']], dim=1)\n",
    "        feat_lang = torch.cat([node.data['word2vec'], node.data['z_f_lang']], dim=1)\n",
    "        n_feat = self.node_fc(feat)\n",
    "        n_feat_lang = self.node_fc_lang(feat_lang)\n",
    "\n",
    "        return {'new_n_f': n_feat, 'new_n_f_lang': n_feat_lang}\n",
    "\n",
    "class E_AttentionModule1(nn.Module): #edge attention\n",
    "    '''\n",
    "        init    : config\n",
    "        forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(E_AttentionModule1, self).__init__()\n",
    "        self.attn_fc = MLP(CONFIG.G_A_L_S, CONFIG.G_A_A, CONFIG.G_A_B, CONFIG.G_A_BN, CONFIG.G_A_D)\n",
    "        self.attn_fc_lang = MLP(CONFIG.G_A_L_S2, CONFIG.G_A_A2, CONFIG.G_A_B2, CONFIG.G_A_BN2, CONFIG.G_A_D2)\n",
    "\n",
    "    def forward(self, edge):\n",
    "        a_feat = self.attn_fc(edge.data['e_f'])\n",
    "        a_feat_lang = self.attn_fc_lang(edge.data['e_f_lang'])\n",
    "        return {'a_feat': a_feat, 'a_feat_lang': a_feat_lang}\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    '''\n",
    "        init    : config, multi_attn, diff_edge\n",
    "        forward : g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_features\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, diff_edge=True):\n",
    "        super(GNN, self).__init__()\n",
    "        self.diff_edge = diff_edge # false\n",
    "        self.apply_h_h_edge = H_H_EdgeApplyModule(CONFIG, multi_attn)\n",
    "        self.apply_edge_attn1 = E_AttentionModule1(CONFIG)  \n",
    "        self.apply_h_node = H_NodeApplyModule(CONFIG)\n",
    "\n",
    "    def _message_func(self, edges):\n",
    "        return {'nei_n_f': edges.src['n_f'], 'nei_n_w': edges.src['word2vec'], 'e_f': edges.data['e_f'], 'e_f_lang': edges.data['e_f_lang'], 'a_feat': edges.data['a_feat'], 'a_feat_lang': edges.data['a_feat_lang']}\n",
    "\n",
    "    def _reduce_func(self, nodes):\n",
    "        alpha = F.softmax(nodes.mailbox['a_feat'], dim=1)\n",
    "        alpha_lang = F.softmax(nodes.mailbox['a_feat_lang'], dim=1)\n",
    "\n",
    "        z_raw_f = nodes.mailbox['nei_n_f']+nodes.mailbox['e_f']\n",
    "        z_f = torch.sum( alpha * z_raw_f, dim=1)\n",
    "\n",
    "        z_raw_f_lang = nodes.mailbox['nei_n_w']\n",
    "        z_f_lang = torch.sum(alpha_lang * z_raw_f_lang, dim=1)\n",
    "         \n",
    "        # we cannot return 'alpha' for the different dimension \n",
    "        if self.training or validation: return {'z_f': z_f, 'z_f_lang': z_f_lang}\n",
    "        else: return {'z_f': z_f, 'z_f_lang': z_f_lang, 'alpha': alpha, 'alpha_lang': alpha_lang}\n",
    "\n",
    "    def forward(self, g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_feat=False):\n",
    "        \n",
    "        g.apply_edges(self.apply_h_h_edge, g.edges())\n",
    "        g.apply_edges(self.apply_edge_attn1)\n",
    "        g.update_all(self._message_func, self._reduce_func)\n",
    "        g.apply_nodes(self.apply_h_node, h_node+o_node)\n",
    "\n",
    "        # !NOTE:PAY ATTENTION WHEN ADDING MORE FEATURE\n",
    "        g.ndata.pop('n_f')\n",
    "        g.ndata.pop('word2vec')\n",
    "\n",
    "        g.ndata.pop('z_f')\n",
    "        g.edata.pop('e_f')\n",
    "        g.edata.pop('a_feat')\n",
    "\n",
    "        g.ndata.pop('z_f_lang')\n",
    "        g.edata.pop('e_f_lang')\n",
    "        g.edata.pop('a_feat_lang')\n",
    "\n",
    "class GRNN(nn.Module):\n",
    "    '''\n",
    "    init: \n",
    "        config, multi_attn, diff_edge\n",
    "    forward: \n",
    "        batch_graph, batch_h_node_list, batch_obj_node_list,\n",
    "        batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list,\n",
    "        features, spatial_features, word2vec,\n",
    "        valid, pop_features, initial_features\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, diff_edge=True):\n",
    "        super(GRNN, self).__init__()\n",
    "        self.multi_attn = multi_attn #false\n",
    "        self.gnn = GNN(CONFIG, multi_attn, diff_edge)\n",
    "\n",
    "    def forward(self, batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec, valid=False, pop_feat=False, initial_feat=False):\n",
    "        \n",
    "        # !NOTE: if node_num==1, there will be something wrong to forward the attention mechanism\n",
    "        global validation \n",
    "        validation = valid\n",
    "\n",
    "        # initialize the graph with some datas\n",
    "        batch_graph.ndata['n_f'] = feat           # node: features \n",
    "        batch_graph.ndata['word2vec'] = word2vec  # node: words\n",
    "        batch_graph.edata['s_f'] = spatial_feat   # edge: spatial features\n",
    "\n",
    "        try:\n",
    "            self.gnn(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            ipdb.set_trace()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Predictor \n",
    "    init    : config\n",
    "    forward : edge\n",
    "\n",
    "AGRNN\n",
    "    init    : bias, bn, dropout, multi_attn, layer, diff_edge\n",
    "    forward : node_num, feat, spatial_feat, word2vec, roi_label, validation, choose_nodes, remove_nodes\n",
    "'''\n",
    "\n",
    "import dgl\n",
    "import ipdb\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchvision\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    '''\n",
    "    init    : config\n",
    "    forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.classifier = MLP(CONFIG.G_ER_L_S, CONFIG.G_ER_A, CONFIG.G_ER_B, CONFIG.G_ER_BN, CONFIG.G_ER_D)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, edge):\n",
    "        feat = torch.cat([edge.dst['new_n_f'], edge.dst['new_n_f_lang'], edge.data['s_f'], edge.src['new_n_f_lang'], edge.src['new_n_f']], dim=1)\n",
    "        pred = self.classifier(feat)\n",
    "        # if the criterion is BCELoss, you need to uncomment the following code\n",
    "        # output = self.sigmoid(output)\n",
    "        return {'pred': pred}\n",
    "\n",
    "class AGRNN(nn.Module):\n",
    "    '''\n",
    "    init    : \n",
    "        feature_type, bias, bn, dropout, multi_attn, layer, diff_edge\n",
    "        \n",
    "    forward : \n",
    "        node_num, features, spatial_features, word2vec, roi_label,\n",
    "        validation, choose_nodes, remove_nodes\n",
    "    '''\n",
    "    def __init__(self, bias=True, bn=True, dropout=None, multi_attn=False, layer=1, diff_edge=True):\n",
    "        super(AGRNN, self).__init__()\n",
    " \n",
    "        self.multi_attn = multi_attn # false\n",
    "        self.layer = layer           # 1 layer\n",
    "        self.diff_edge = diff_edge   # false\n",
    "        \n",
    "        self.CONFIG1 = CONFIGURATION(layer=1, bias=bias, bn=bn, dropout=dropout, multi_attn=multi_attn)\n",
    "\n",
    "        self.grnn1 = GRNN(self.CONFIG1, multi_attn=multi_attn, diff_edge=diff_edge)\n",
    "        self.edge_readout = Predictor(self.CONFIG1)\n",
    "        \n",
    "    def _collect_edge(self, node_num, roi_label, node_space, diff_edge):\n",
    "        '''\n",
    "        arguments: node_num, roi_label, node_space, diff_edge\n",
    "        '''\n",
    "        \n",
    "        # get human nodes && object nodes\n",
    "        h_node_list = np.where(roi_label == 0)[0]\n",
    "        obj_node_list = np.where(roi_label != 0)[0]\n",
    "        edge_list = []\n",
    "        \n",
    "        h_h_e_list = []\n",
    "        o_o_e_list = []\n",
    "        h_o_e_list = []\n",
    "        \n",
    "        readout_edge_list = []\n",
    "        readout_h_h_e_list = []\n",
    "        readout_h_o_e_list = []\n",
    "        \n",
    "        # get all edge in the fully-connected graph, edge_list, For node_num = 2, edge_list = [(0, 1), (1, 0)]\n",
    "        for src in range(node_num):\n",
    "            for dst in range(node_num):\n",
    "                if src == dst:\n",
    "                    continue\n",
    "                else:\n",
    "                    edge_list.append((src, dst))\n",
    "        \n",
    "        # readout_edge_list, get corresponding readout edge in the graph\n",
    "        src_box_list = np.arange(roi_label.shape[0])\n",
    "        for dst in h_node_list:\n",
    "            if dst == roi_label.shape[0]-1:\n",
    "                continue\n",
    "            src_box_list = src_box_list[1:]\n",
    "            for src in src_box_list:\n",
    "                readout_edge_list.append((src, dst))\n",
    "        \n",
    "        # readout h_h_e_list, get corresponding readout h_h edges && h_o edges\n",
    "        temp_h_node_list = h_node_list[:]\n",
    "        for dst in h_node_list:\n",
    "            if dst == h_node_list.shape[0]-1:\n",
    "                continue\n",
    "            temp_h_node_list = temp_h_node_list[1:]\n",
    "            for src in temp_h_node_list:\n",
    "                if src == dst: continue\n",
    "                readout_h_h_e_list.append((src, dst))\n",
    "\n",
    "        # readout h_o_e_list\n",
    "        readout_h_o_e_list = [x for x in readout_edge_list if x not in readout_h_h_e_list]\n",
    "\n",
    "        # add node space to match the batch graph\n",
    "        h_node_list = (np.array(h_node_list)+node_space).tolist()\n",
    "        obj_node_list = (np.array(obj_node_list)+node_space).tolist()\n",
    "        \n",
    "        h_h_e_list = (np.array(h_h_e_list)+node_space).tolist() #empty no diff_edge\n",
    "        o_o_e_list = (np.array(o_o_e_list)+node_space).tolist() #empty no diff_edge\n",
    "        h_o_e_list = (np.array(h_o_e_list)+node_space).tolist() #empty no diff_edge\n",
    "\n",
    "        readout_h_h_e_list = (np.array(readout_h_h_e_list)+node_space).tolist()\n",
    "        readout_h_o_e_list = (np.array(readout_h_o_e_list)+node_space).tolist()   \n",
    "        readout_edge_list = (np.array(readout_edge_list)+node_space).tolist()\n",
    "\n",
    "        return edge_list, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list\n",
    "    \n",
    "    def _build_graph(self, node_num, roi_label, node_space, diff_edge):\n",
    "        '''\n",
    "        Declare graph, add_nodes, collect edges, add_edges\n",
    "        '''\n",
    "        graph = dgl.DGLGraph()\n",
    "        graph.add_nodes(node_num)\n",
    "\n",
    "        edge_list, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list = self._collect_edge(node_num, roi_label, node_space, diff_edge)\n",
    "        src, dst = tuple(zip(*edge_list))\n",
    "        graph.add_edges(src, dst)   # make the graph bi-directional\n",
    "\n",
    "        return graph, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list\n",
    "\n",
    "    def forward(self, node_num=None, feat=None, spatial_feat=None, word2vec=None, roi_label=None, validation=False, choose_nodes=None, remove_nodes=None):\n",
    "        \n",
    "        batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, batch_readout_edge_list, batch_readout_h_h_e_list, batch_readout_h_o_e_list = [], [], [], [], [], [], [], [], []\n",
    "        node_num_cum = np.cumsum(node_num) # !IMPORTANT\n",
    "        \n",
    "        for i in range(len(node_num)):\n",
    "            # set node space\n",
    "            node_space = 0\n",
    "            if i != 0:\n",
    "                node_space = node_num_cum[i-1]\n",
    "            graph, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list = self._build_graph(node_num[i], roi_label[i], node_space, diff_edge=self.diff_edge)\n",
    "            \n",
    "            # updata batch\n",
    "            batch_graph.append(graph)\n",
    "            batch_h_node_list += h_node_list\n",
    "            batch_obj_node_list += obj_node_list\n",
    "            \n",
    "            batch_h_h_e_list += h_h_e_list\n",
    "            batch_o_o_e_list += o_o_e_list\n",
    "            batch_h_o_e_list += h_o_e_list\n",
    "            \n",
    "            batch_readout_edge_list += readout_edge_list\n",
    "            batch_readout_h_h_e_list += readout_h_h_e_list\n",
    "            batch_readout_h_o_e_list += readout_h_o_e_list\n",
    "        \n",
    "        batch_graph = dgl.batch(batch_graph)\n",
    "        \n",
    "        # GRNN\n",
    "        self.grnn1(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec, validation, initial_feat=True)\n",
    "        batch_graph.apply_edges(self.edge_readout, tuple(zip(*(batch_readout_h_o_e_list+batch_readout_h_h_e_list))))\n",
    "        \n",
    "        if self.training or validation:\n",
    "            # !NOTE: cannot use \"batch_readout_h_o_e_list+batch_readout_h_h_e_list\" because of the wrong order\n",
    "            return batch_graph.edges[tuple(zip(*batch_readout_edge_list))].data['pred']\n",
    "        else:\n",
    "            return batch_graph.edges[tuple(zip(*batch_readout_edge_list))].data['pred'], \\\n",
    "                   batch_graph.nodes[batch_h_node_list].data['alpha'], \\\n",
    "                   batch_graph.nodes[batch_h_node_list].data['alpha_lang'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils.io as io\n",
    "\n",
    "class SurgicalSceneConstants():\n",
    "    def __init__( self):\n",
    "        self.instrument_classes = ('kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "        \n",
    "        #self.instrument_classes = ( 'kidney', 'bipolar_forceps', 'fenestrated_bipolar', \n",
    "        #                             'prograsp_forceps', 'large_needle_driver', 'vessel_sealer',\n",
    "        #                             'grasping_retractor', 'monopolar_curved_scissors', \n",
    "        #                             'ultrasound_probe', 'suction', 'clip_applier', 'stapler')\n",
    "        \n",
    "        self.action_classes = ( 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                                'Tool_Manipulation', 'Cutting', 'Cauterization', \n",
    "                                'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', \n",
    "                                'Ultrasound_Sensing')\n",
    "        self.xml_data_dir = 'datasets/instruments18/seq_'\n",
    "        self.word2vec_loc = 'datasets/surgicalscene_word2vec.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "    \n",
    "class SurgicalSceneDataset(Dataset):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, seq_set, dataconst, feature_extractor):\n",
    "        self.dataconst = dataconst\n",
    "        self.dir_root_gt = 'datasets/instruments18/seq_'\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        self.xml_dir_list = []\n",
    "        for i in seq_set:\n",
    "            xml_dir_temp = self.dir_root_gt + str(i) + '/xml/'\n",
    "            self.xml_dir_list = self.xml_dir_list + glob(xml_dir_temp + '/*.xml')\n",
    "        \n",
    "        self.word2vec = h5py.File('datasets/surgicalscene_word2vec.hdf5', 'r')\n",
    "    \n",
    "    # word2vec\n",
    "    def _get_word2vec(self,node_ids):\n",
    "        word2vec = np.empty((0,300))\n",
    "        for node_id in node_ids:\n",
    "            vec = self.word2vec[self.dataconst.instrument_classes[node_id]]\n",
    "            word2vec = np.vstack((word2vec, vec))\n",
    "        return word2vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xml_dir_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        file_name = os.path.splitext(os.path.basename(self.xml_dir_list[idx]))[0]\n",
    "        file_root = os.path.dirname(os.path.dirname(self.xml_dir_list[idx]))\n",
    "        _img_loc = os.path.join(file_root+'/left_frames/'+ file_name + '.png')\n",
    "        \n",
    "        frame_data = h5py.File(os.path.join(file_root+'/vsgat/'+self.feature_extractor+'/'+ file_name + '_features.hdf5'), 'r')    \n",
    "        data = {}\n",
    "        data['img_name'] = frame_data['img_name'].value[:] + '.jpg'\n",
    "        data['img_loc'] = _img_loc\n",
    "        \n",
    "        data['node_num'] = frame_data['node_num'].value\n",
    "        data['roi_labels'] = frame_data['classes'][:]\n",
    "        data['det_boxes'] = frame_data['boxes'][:]\n",
    "        \n",
    "        \n",
    "        data['edge_labels'] = frame_data['edge_labels'][:]\n",
    "        data['edge_num'] = data['edge_labels'].shape[0]\n",
    "        \n",
    "        data['features'] = frame_data['node_features'][:]\n",
    "        data['spatial_feat'] = frame_data['spatial_features'][:]\n",
    "        data['word2vec'] = self._get_word2vec(data['roi_labels'])\n",
    "        return data\n",
    "\n",
    "# for DatasetLoader\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "        Default collate_fn(): https://github.com/pytorch/pytorch/blob/1d53d0756668ce641e4f109200d9c65b003d05fa/torch/utils/data/_utils/collate.py#L43\n",
    "    '''\n",
    "    batch_data = {}\n",
    "    batch_data['img_name'] = []\n",
    "    batch_data['img_loc'] = []\n",
    "    batch_data['node_num'] = []\n",
    "    batch_data['roi_labels'] = []\n",
    "    batch_data['det_boxes'] = []\n",
    "    batch_data['edge_labels'] = []\n",
    "    batch_data['edge_num'] = []\n",
    "    batch_data['features'] = []\n",
    "    batch_data['spatial_feat'] = []\n",
    "    batch_data['word2vec'] = []\n",
    "    \n",
    "    for data in batch:\n",
    "        batch_data['img_name'].append(data['img_name'])\n",
    "        batch_data['img_loc'].append(data['img_loc'])\n",
    "        batch_data['node_num'].append(data['node_num'])\n",
    "        batch_data['roi_labels'].append(data['roi_labels'])\n",
    "        batch_data['det_boxes'].append(data['det_boxes'])\n",
    "        batch_data['edge_labels'].append(data['edge_labels'])\n",
    "        batch_data['edge_num'].append(data['edge_num'])\n",
    "        batch_data['features'].append(data['features'])\n",
    "        batch_data['spatial_feat'].append(data['spatial_feat'])\n",
    "        batch_data['word2vec'].append(data['word2vec'])\n",
    "        \n",
    "    batch_data['edge_labels'] = torch.FloatTensor(np.concatenate(batch_data['edge_labels'], axis=0))\n",
    "    batch_data['features'] = torch.FloatTensor(np.concatenate(batch_data['features'], axis=0))\n",
    "    batch_data['spatial_feat'] = torch.FloatTensor(np.concatenate(batch_data['spatial_feat'], axis=0))\n",
    "    batch_data['word2vec'] = torch.FloatTensor(np.concatenate(batch_data['word2vec'], axis=0))\n",
    "    \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch as t\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plot\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def vis_img(img, node_classes, bboxs,  det_action, score_thresh = 0.7):\n",
    "    \n",
    "    Drawer = ImageDraw.Draw(img)\n",
    "    line_width = 3\n",
    "    outline = '#FF0000'\n",
    "    font = ImageFont.truetype(font='/usr/share/fonts/truetype/freefont/FreeMono.ttf', size=25)\n",
    "    \n",
    "    im_w,im_h = img.size\n",
    "    node_num = len(node_classes)\n",
    "    edge_num = len(det_action)\n",
    "    tissue_num = len(np.where(node_classes == 1)[0])\n",
    "    \n",
    "    for node in range(node_num):\n",
    "        \n",
    "        r_color = random.choice(np.arange(256))\n",
    "        g_color = random.choice(np.arange(256))\n",
    "        b_color = random.choice(np.arange(256))\n",
    "        \n",
    "        text = data_const.instrument_classes[node_classes[node]]\n",
    "        h, w = font.getsize(text)\n",
    "        Drawer.rectangle(list(bboxs[node]), outline=outline, width=line_width)\n",
    "        Drawer.text(xy=(bboxs[node][0], bboxs[node][1]-w-1), text=text, font=font, fill=(r_color,g_color,b_color))\n",
    "  \n",
    "    edge_idx = 0\n",
    "    \n",
    "    for tissue in range(tissue_num):\n",
    "        for instrument in range(tissue+1, node_num):\n",
    "            \n",
    "            #action_idx = np.where(det_action[edge_idx] > score_thresh)[0]\n",
    "            action_idx = np.argmax(det_action[edge_idx])\n",
    "#             print('det_action', det_action[edge_idx])\n",
    "#             print('action_idx',action_idx)\n",
    "            \n",
    "            text = data_const.action_classes[action_idx]\n",
    "            r_color = random.choice(np.arange(256))\n",
    "            g_color = random.choice(np.arange(256))\n",
    "            b_color = random.choice(np.arange(256))\n",
    "        \n",
    "            x1,y1,x2,y2 = bboxs[tissue]\n",
    "            x1_,y1_,x2_,y2_ = bboxs[instrument]\n",
    "            \n",
    "            c0 = int(0.5*x1)+int(0.5*x2)\n",
    "            c0 = max(0,min(c0,im_w-1))\n",
    "            r0 = int(0.5*y1)+int(0.5*y2)\n",
    "            r0 = max(0,min(r0,im_h-1))\n",
    "            c1 = int(0.5*x1_)+int(0.5*x2_)\n",
    "            c1 = max(0,min(c1,im_w-1))\n",
    "            r1 = int(0.5*y1_)+int(0.5*y2_)\n",
    "            r1 = max(0,min(r1,im_h-1))\n",
    "            Drawer.line(((c0,r0),(c1,r1)), fill=(r_color,g_color,b_color), width=3)\n",
    "            Drawer.text(xy=(c1, r1), text=text, font=font, fill=(r_color,g_color,b_color))\n",
    "\n",
    "            edge_idx +=1\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import utils.io as io\n",
    "#from utils.vis_tool import vis_img\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def run_model(args, data_const):\n",
    "    '''\n",
    "    a) set dataset and dataloader for train and validate set\n",
    "    b) set model\n",
    "    c) optimizer\n",
    "    d) loss\n",
    "    e) learning rate scheduler\n",
    "    '''\n",
    "\n",
    "    # set up dataset variable\n",
    "    train_dataset = SurgicalSceneDataset(seq_set = [2,3,4,6,7,9,10,11,12,14,15], dataconst = data_const, feature_extractor = args.feature_extractor)\n",
    "    val_dataset = SurgicalSceneDataset(seq_set= [1,5,16], dataconst = data_const, feature_extractor = args.feature_extractor)\n",
    "    dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "    print('set up dataset variable successfully')\n",
    "   \n",
    "    # use default DataLoader() to load the data. \n",
    "    train_dataloader = DataLoader(dataset=dataset['train'], batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(dataset=dataset['val'], batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "    dataloader = {'train': train_dataloader, 'val': val_dataloader}\n",
    "\n",
    "    print('set up dataloader successfully')\n",
    "\n",
    "    # use cpu or cuda\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.gpu else 'cpu')\n",
    "    print('training on {}...'.format(device))\n",
    "\n",
    "    # model\n",
    "    model = AGRNN(bias=args.bias, bn=args.bn, dropout=args.drop_prob, multi_attn=args.multi_attn, layer=args.layers, diff_edge=args.diff_edge)\n",
    "\n",
    "    # calculate the amount of all the learned parameters\n",
    "    parameter_num = 0\n",
    "    for param in model.parameters(): parameter_num += param.numel()\n",
    "    print(f'The parameters number of the model is {parameter_num / 1e6} million')\n",
    "\n",
    "    # load pretrained model\n",
    "    if args.pretrained:\n",
    "        print(f\"loading pretrained model {args.pretrained}\")\n",
    "        checkpoints = torch.load(args.pretrained, map_location=device)\n",
    "        model.load_state_dict(checkpoints['state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # build optimizer  \n",
    "    if args.optim == 'sgd': optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0)\n",
    "    else: optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=0)\n",
    "    \n",
    "    # criterion and scheduler\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.3) #the scheduler divides the lr by 10 every 150 epochs\n",
    "\n",
    "    # get the configuration of the model and save some key configurations\n",
    "    io.mkdir_if_not_exists(os.path.join(args.save_dir, args.exp_ver), recursive=True)\n",
    "    for i in range(args.layers):\n",
    "        if i==0:\n",
    "            model_config = model.CONFIG1.save_config()\n",
    "            model_config['lr'] = args.lr\n",
    "            model_config['bs'] = args.batch_size\n",
    "            model_config['layers'] = args.layers\n",
    "            model_config['multi_attn'] = args.multi_attn\n",
    "            model_config['data_aug'] = args.data_aug\n",
    "            model_config['drop_out'] = args.drop_prob\n",
    "            model_config['optimizer'] = args.optim\n",
    "            model_config['diff_edge'] = args.diff_edge\n",
    "            model_config['model_parameters'] = parameter_num\n",
    "            io.dump_json_object(model_config, os.path.join(args.save_dir, args.exp_ver, 'l1_config.json'))\n",
    "    print('save key configurations successfully...')\n",
    "\n",
    "    epoch_train(model, dataloader, dataset, criterion, optimizer, scheduler, device, data_const)\n",
    "\n",
    "def epoch_train(model, dataloader, dataset, criterion, optimizer, scheduler, device, data_const):\n",
    "    '''\n",
    "    input: model, dataloader, dataset, criterain, optimizer, scheduler, device, data_const\n",
    "    data: \n",
    "        img_name, node_num, roi_labels, det_boxes, edge_labels,\n",
    "        edge_num, features, spatial_features, word2vec\n",
    "    '''\n",
    "    print('epoch training...')\n",
    "    \n",
    "    # set visualization and create folder to save checkpoints\n",
    "    writer = SummaryWriter(log_dir=args.log_dir + '/' + args.exp_ver + '/' + 'epoch_train')\n",
    "    io.mkdir_if_not_exists(os.path.join(args.save_dir, args.exp_ver, 'epoch_train'), recursive=True)\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        # each epoch has a training and validation step\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for phase in ['train', 'val']:\n",
    "            start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            running_edge_count = 0\n",
    "            idx = 0\n",
    "            \n",
    "            #for data in tqdm(dataloader[phase]):\n",
    "            for data in dataloader[phase]:\n",
    "                train_data = data\n",
    "                img_name = train_data['img_name']\n",
    "                img_loc = train_data['img_loc']\n",
    "                node_num = train_data['node_num']\n",
    "                roi_labels = train_data['roi_labels']\n",
    "                det_boxes = train_data['det_boxes']\n",
    "                edge_labels = train_data['edge_labels']\n",
    "                edge_num = train_data['edge_num']\n",
    "                features = train_data['features']\n",
    "                spatial_feat = train_data['spatial_feat']\n",
    "                word2vec = train_data['word2vec']\n",
    "                features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "                \n",
    "                #if idx == 2: break\n",
    "                    \n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                    model.zero_grad()\n",
    "                    outputs = model(node_num, features, spatial_feat, word2vec, roi_labels)\n",
    "                    \n",
    "                    # loss and accuracy\n",
    "                    loss = criterion(outputs, edge_labels.float())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    acc = np.sum(np.equal(np.argmax(outputs.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    # turn off the gradients for validation, save memory and computations\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(node_num, features, spatial_feat, word2vec, roi_labels, validation=True)\n",
    "                        \n",
    "                        # loss and accuracy\n",
    "                        loss = criterion(outputs, edge_labels.float())\n",
    "                        acc = np.sum(np.equal(np.argmax(outputs.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "                    \n",
    "                        # print result every 1000 iteration during validation\n",
    "                        if idx == 10:\n",
    "                            #print(img_loc[0])\n",
    "                            io.mkdir_if_not_exists(os.path.join(args.output_img_dir, ('epoch_'+str(epoch))), recursive=True)\n",
    "                            image = Image.open(img_loc[0]).convert('RGB')\n",
    "                            det_actions = nn.Sigmoid()(outputs[0:int(edge_num[0])])\n",
    "                            det_actions = det_actions.cpu().detach().numpy()\n",
    "                            action_img = vis_img(image, roi_labels[0], det_boxes[0],  det_actions, score_thresh = 0.7)\n",
    "                            image = image.save(os.path.join(args.output_img_dir, ('epoch_'+str(epoch)),img_name[0]))\n",
    "\n",
    "                idx+=1\n",
    "                # accumulate loss of each batch\n",
    "                running_loss += loss.item() * edge_labels.shape[0]\n",
    "                running_acc += acc\n",
    "                running_edge_count += edge_labels.shape[0]\n",
    "                \n",
    "            # calculate the loss and accuracy of each epoch\n",
    "            epoch_loss = running_loss / len(dataset[phase])\n",
    "            epoch_acc = running_acc / running_edge_count\n",
    "            \n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            # log trainval datas, and visualize them in the same graph\n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss \n",
    "            else:\n",
    "                writer.add_scalars('trainval_loss_epoch', {'train': train_loss, 'val': epoch_loss}, epoch)\n",
    "            \n",
    "            # print data\n",
    "            if (epoch % args.print_every) == 0:\n",
    "                end_time = time.time()\n",
    "                print(\"[{}] Epoch: {}/{} Acc: {:0.6f} Loss: {:0.6f} Execution time: {:0.6f}\".format(\\\n",
    "                        phase, epoch+1, args.epoch, epoch_acc, epoch_loss, (end_time-start_time)))\n",
    "                        \n",
    "        # scheduler.step()\n",
    "        # save model\n",
    "        if epoch_loss<0.0405 or epoch % args.save_every == (args.save_every - 1) and epoch >= (50-1):\n",
    "            checkpoint = { \n",
    "                            'lr': args.lr,\n",
    "                           'b_s': args.batch_size,\n",
    "                          'bias': args.bias, \n",
    "                            'bn': args.bn, \n",
    "                       'dropout': args.drop_prob,\n",
    "                        'layers': args.layers,\n",
    "                    'multi_head': args.multi_attn,\n",
    "                     'diff_edge': args.diff_edge,\n",
    "                    'state_dict': model.state_dict()\n",
    "            }\n",
    "            save_name = \"checkpoint_\" + str(epoch+1) + '_epoch.pth'\n",
    "            torch.save(checkpoint, os.path.join(args.save_dir, args.exp_ver, 'epoch_train', save_name))\n",
    "\n",
    "    writer.close()\n",
    "    print('Finishing training!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18_09_ls\n",
      "set up dataset variable successfully\n",
      "set up dataloader successfully\n",
      "training on cuda...\n",
      "The parameters number of the model is 2.393691 million\n",
      "save key configurations successfully...\n",
      "epoch training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Epoch: 1/351 Acc: 0.156426 Loss: 4.218132 Execution time: 5.587665\n",
      "[val] Epoch: 1/351 Acc: 0.242033 Loss: 1.726557 Execution time: 1.531619\n",
      "[train] Epoch: 11/351 Acc: 0.286558 Loss: 1.249767 Execution time: 5.501142\n",
      "[val] Epoch: 11/351 Acc: 0.421189 Loss: 0.599684 Execution time: 1.512062\n",
      "[train] Epoch: 21/351 Acc: 0.327609 Loss: 0.971461 Execution time: 5.477361\n",
      "[val] Epoch: 21/351 Acc: 0.517657 Loss: 0.483978 Execution time: 1.503312\n",
      "[train] Epoch: 31/351 Acc: 0.328683 Loss: 0.848509 Execution time: 5.497762\n",
      "[val] Epoch: 31/351 Acc: 0.517657 Loss: 0.465106 Execution time: 1.497632\n",
      "[train] Epoch: 41/351 Acc: 0.344781 Loss: 0.822351 Execution time: 5.484414\n",
      "[val] Epoch: 41/351 Acc: 0.529716 Loss: 0.469336 Execution time: 1.511727\n",
      "[train] Epoch: 51/351 Acc: 0.351757 Loss: 0.812516 Execution time: 5.499628\n",
      "[val] Epoch: 51/351 Acc: 0.560724 Loss: 0.457132 Execution time: 1.526616\n",
      "[train] Epoch: 61/351 Acc: 0.375101 Loss: 0.794345 Execution time: 5.505145\n",
      "[val] Epoch: 61/351 Acc: 0.555556 Loss: 0.451513 Execution time: 1.505860\n",
      "[train] Epoch: 71/351 Acc: 0.380467 Loss: 0.777839 Execution time: 5.445204\n",
      "[val] Epoch: 71/351 Acc: 0.563307 Loss: 0.443266 Execution time: 1.493992\n",
      "[train] Epoch: 81/351 Acc: 0.405152 Loss: 0.759116 Execution time: 5.166170\n",
      "[val] Epoch: 81/351 Acc: 0.564169 Loss: 0.433476 Execution time: 1.517248\n",
      "[train] Epoch: 91/351 Acc: 0.416421 Loss: 0.756352 Execution time: 5.178767\n",
      "[val] Epoch: 91/351 Acc: 0.558140 Loss: 0.430241 Execution time: 1.506351\n",
      "[train] Epoch: 101/351 Acc: 0.408908 Loss: 0.751056 Execution time: 5.496544\n",
      "[val] Epoch: 101/351 Acc: 0.559001 Loss: 0.428776 Execution time: 1.516280\n",
      "[train] Epoch: 111/351 Acc: 0.440837 Loss: 0.740566 Execution time: 5.491242\n",
      "[val] Epoch: 111/351 Acc: 0.567614 Loss: 0.424317 Execution time: 1.509278\n",
      "[train] Epoch: 121/351 Acc: 0.444325 Loss: 0.738257 Execution time: 5.478829\n",
      "[val] Epoch: 121/351 Acc: 0.572782 Loss: 0.422300 Execution time: 1.504546\n",
      "[train] Epoch: 131/351 Acc: 0.444862 Loss: 0.728942 Execution time: 5.571443\n",
      "[val] Epoch: 131/351 Acc: 0.583979 Loss: 0.412007 Execution time: 1.505591\n",
      "[train] Epoch: 141/351 Acc: 0.486182 Loss: 0.716628 Execution time: 5.475541\n",
      "[val] Epoch: 141/351 Acc: 0.594315 Loss: 0.407745 Execution time: 1.489142\n",
      "[train] Epoch: 151/351 Acc: 0.470620 Loss: 0.722495 Execution time: 5.150313\n",
      "[val] Epoch: 151/351 Acc: 0.593454 Loss: 0.405490 Execution time: 1.496171\n",
      "[train] Epoch: 161/351 Acc: 0.493963 Loss: 0.714845 Execution time: 5.511823\n",
      "[val] Epoch: 161/351 Acc: 0.596899 Loss: 0.408471 Execution time: 1.521047\n",
      "[train] Epoch: 171/351 Acc: 0.483230 Loss: 0.708202 Execution time: 5.495463\n",
      "[val] Epoch: 171/351 Acc: 0.602929 Loss: 0.407316 Execution time: 1.512162\n",
      "[train] Epoch: 181/351 Acc: 0.494231 Loss: 0.704865 Execution time: 5.533938\n",
      "[val] Epoch: 181/351 Acc: 0.601206 Loss: 0.405659 Execution time: 1.513614\n",
      "[train] Epoch: 191/351 Acc: 0.504964 Loss: 0.698832 Execution time: 5.505324\n",
      "[val] Epoch: 191/351 Acc: 0.613264 Loss: 0.398686 Execution time: 1.495808\n",
      "[train] Epoch: 201/351 Acc: 0.507647 Loss: 0.700065 Execution time: 5.501303\n",
      "[val] Epoch: 201/351 Acc: 0.588286 Loss: 0.403775 Execution time: 1.520534\n",
      "[train] Epoch: 211/351 Acc: 0.522672 Loss: 0.699103 Execution time: 5.480974\n",
      "[val] Epoch: 211/351 Acc: 0.586563 Loss: 0.404131 Execution time: 1.498590\n",
      "[train] Epoch: 221/351 Acc: 0.529380 Loss: 0.689504 Execution time: 5.478854\n",
      "[val] Epoch: 221/351 Acc: 0.582257 Loss: 0.404477 Execution time: 1.501834\n",
      "[train] Epoch: 231/351 Acc: 0.539308 Loss: 0.688400 Execution time: 5.457323\n",
      "[val] Epoch: 231/351 Acc: 0.592593 Loss: 0.401156 Execution time: 1.502802\n",
      "[train] Epoch: 241/351 Acc: 0.527502 Loss: 0.692292 Execution time: 5.473851\n",
      "[val] Epoch: 241/351 Acc: 0.583979 Loss: 0.401884 Execution time: 1.498307\n",
      "[train] Epoch: 251/351 Acc: 0.542796 Loss: 0.685818 Execution time: 5.504332\n",
      "[val] Epoch: 251/351 Acc: 0.577950 Loss: 0.398327 Execution time: 1.511808\n",
      "[train] Epoch: 261/351 Acc: 0.551918 Loss: 0.688338 Execution time: 5.508261\n",
      "[val] Epoch: 261/351 Acc: 0.578811 Loss: 0.406471 Execution time: 1.512966\n",
      "[train] Epoch: 271/351 Acc: 0.551650 Loss: 0.675679 Execution time: 5.483851\n",
      "[val] Epoch: 271/351 Acc: 0.594315 Loss: 0.394777 Execution time: 1.506322\n",
      "[train] Epoch: 281/351 Acc: 0.559163 Loss: 0.681374 Execution time: 5.466148\n",
      "[val] Epoch: 281/351 Acc: 0.592593 Loss: 0.399146 Execution time: 1.520399\n",
      "[train] Epoch: 291/351 Acc: 0.565602 Loss: 0.673687 Execution time: 5.494796\n",
      "[val] Epoch: 291/351 Acc: 0.577950 Loss: 0.399893 Execution time: 1.508196\n",
      "[train] Epoch: 301/351 Acc: 0.566944 Loss: 0.669161 Execution time: 5.505293\n",
      "[val] Epoch: 301/351 Acc: 0.582257 Loss: 0.395730 Execution time: 1.514373\n",
      "[train] Epoch: 311/351 Acc: 0.579286 Loss: 0.666797 Execution time: 5.465517\n",
      "[val] Epoch: 311/351 Acc: 0.595177 Loss: 0.394984 Execution time: 1.489057\n",
      "[train] Epoch: 321/351 Acc: 0.569359 Loss: 0.668792 Execution time: 5.479174\n",
      "[val] Epoch: 321/351 Acc: 0.579673 Loss: 0.396900 Execution time: 1.502407\n",
      "[train] Epoch: 331/351 Acc: 0.580628 Loss: 0.674424 Execution time: 5.452379\n",
      "[val] Epoch: 331/351 Acc: 0.577089 Loss: 0.396683 Execution time: 1.519227\n",
      "[train] Epoch: 341/351 Acc: 0.579018 Loss: 0.664785 Execution time: 5.515769\n",
      "[val] Epoch: 341/351 Acc: 0.590009 Loss: 0.392194 Execution time: 1.513196\n",
      "[train] Epoch: 351/351 Acc: 0.594848 Loss: 0.657520 Execution time: 5.497670\n",
      "[val] Epoch: 351/351 Acc: 0.573643 Loss: 0.394290 Execution time: 1.491670\n",
      "Finishing training!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = arguments()\n",
    "    print(args.feature_extractor)\n",
    "    data_const = SurgicalSceneConstants()\n",
    "    run_model(args, data_const)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
