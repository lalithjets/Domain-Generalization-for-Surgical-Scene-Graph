{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "arguments\n",
    "    Hyperparameters, file location, optimizer, network, data_processing\n",
    "'''\n",
    "ver = 'd2g_ecbs_t_resnet18_09'\n",
    "f_e = 'resnet18_09'\n",
    "\n",
    "class arguments():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # hyper parameters\n",
    "        self.lr = 0.00001\n",
    "        self.epoch = 251\n",
    "        self.ft_epoch = 81\n",
    "        self.start_epoch = 0\n",
    "        self.batch_size = 32\n",
    "        self.gpu = True\n",
    "        self.print_every = 10 \n",
    "        self.train_model = 'epoch'\n",
    "        self.exp_ver= ver\n",
    "\n",
    "        # file locations\n",
    "        self.log_dir = './log/' + ver\n",
    "        self.save_dir = './checkpoints/' + ver\n",
    "        self.output_img_dir = './results/' + ver\n",
    "        self.save_every = 10\n",
    "        self.pretrained = None                 \n",
    "\n",
    "        # optimizer\n",
    "        self.optim='adam' # choices=['sgd', 'adam']\n",
    "\n",
    "        # network\n",
    "        self.layers= 1\n",
    "        self.bn = False\n",
    "        self.drop_prob = 0.3\n",
    "        self.bias = True\n",
    "        self.multi_attn = False\n",
    "        self.diff_edge = False\n",
    "\n",
    "        # data_processing\n",
    "        self.sampler = 0\n",
    "        self.data_aug = False\n",
    "        self.feature_extractor = f_e\n",
    "        \n",
    "        # CBS\n",
    "        self.use_cbs = True\n",
    "        \n",
    "        # temperature_scaling\n",
    "        self.use_t = True\n",
    "        self.t_scale = 1.5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "configurations of the network\n",
    "    \n",
    "    readout: G_ER_L_S = [1024+300+16+300+1024,  1024, 117]\n",
    "\n",
    "    node_func: G_N_L_S = [1024+1024, 1024]\n",
    "    node_lang_func: G_N_L_S2 = [300+300+300]\n",
    "    \n",
    "    edge_func : G_E_L_S = [1024*2+16, 1024]\n",
    "    edge_lang_func: [300*2, 1024]\n",
    "    \n",
    "    attn: [1024, 1]\n",
    "    attn_lang: [1024, 1]\n",
    "'''\n",
    "\n",
    "class CONFIGURATION(object):\n",
    "    '''\n",
    "    Configuration arguments: feature type, layer, bias, batch normalization, dropout, multi-attn\n",
    "    \n",
    "    readout           : fc_size, activation, bias, bn, droupout\n",
    "    gnn_node          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_node_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    gnn_edge          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_edge_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    gnn_attn          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_attn_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    '''\n",
    "    def __init__(self, layer=1, bias=True, bn=False, dropout=0.2, multi_attn=False):\n",
    "        \n",
    "        # if multi_attn:\n",
    "        if True:\n",
    "            if layer==1:\n",
    "                feature_size = 512\n",
    "                # readout\n",
    "                self.G_ER_L_S = [feature_size+300+16+300+feature_size, feature_size, 13]\n",
    "                self.G_ER_A   = ['ReLU', 'Identity']\n",
    "                self.G_ER_B   = bias    #true\n",
    "                self.G_ER_BN  = bn      #false\n",
    "                self.G_ER_D   = dropout #0.3\n",
    "                # self.G_ER_GRU = feature_size\n",
    "\n",
    "                # # gnn node function\n",
    "                self.G_N_L_S = [feature_size+feature_size, feature_size]\n",
    "                self.G_N_A   = ['ReLU']\n",
    "                self.G_N_B   = bias #true\n",
    "                self.G_N_BN  = bn      #false\n",
    "                self.G_N_D   = dropout #0.3\n",
    "                # self.G_N_GRU = feature_size\n",
    "\n",
    "                # # gnn node function for language\n",
    "                self.G_N_L_S2 = [300+300, 300]\n",
    "                self.G_N_A2   = ['ReLU']\n",
    "                self.G_N_B2   = bias    #true\n",
    "                self.G_N_BN2  = bn      #false\n",
    "                self.G_N_D2   = dropout #0.3\n",
    "                # self.G_N_GRU2 = feature_size\n",
    "\n",
    "                # gnn edge function1\n",
    "                self.G_E_L_S           = [feature_size*2+16, feature_size]\n",
    "                self.G_E_A             = ['ReLU']\n",
    "                self.G_E_B             = bias     # true\n",
    "                self.G_E_BN            = bn       # false\n",
    "                self.G_E_D             = dropout  # 0.3\n",
    "                self.G_E_c_std         = 1.0\n",
    "                self.G_E_c_std_factor  = 0.9      # 0.985 (LOG), 0.95 (gau)\n",
    "                self.G_E_c_epoch       = 20\n",
    "                self.G_E_c_kernel_size = 3\n",
    "                self.G_E_c_filter      = 'LOG' # 'gau', 'LOG'\n",
    "\n",
    "                # gnn edge function2 for language\n",
    "                self.G_E_L_S2 = [300*2, feature_size]\n",
    "                self.G_E_A2   = ['ReLU']\n",
    "                self.G_E_B2   = bias     #true\n",
    "                self.G_E_BN2  = bn       #false\n",
    "                self.G_E_D2   = dropout  #0.3\n",
    "\n",
    "                # gnn attention mechanism\n",
    "                self.G_A_L_S = [feature_size, 1]\n",
    "                self.G_A_A   = ['LeakyReLU']\n",
    "                self.G_A_B   = bias     #true\n",
    "                self.G_A_BN  = bn       #false\n",
    "                self.G_A_D   = dropout  #0.3\n",
    "\n",
    "                # gnn attention mechanism2 for language\n",
    "                self.G_A_L_S2 = [feature_size, 1]\n",
    "                self.G_A_A2   = ['LeakyReLU']\n",
    "                self.G_A_B2   = bias    #true\n",
    "                self.G_A_BN2  = bn      #false\n",
    "                self.G_A_D2   = dropout #0.3\n",
    "                    \n",
    "    def save_config(self):\n",
    "        model_config = {'graph_head':{}, 'graph_node':{}, 'graph_edge':{}, 'graph_attn':{}}\n",
    "        CONFIG=self.__dict__\n",
    "        for k, v in CONFIG.items():\n",
    "            if 'G_H' in k:\n",
    "                model_config['graph_head'][k]=v\n",
    "            elif 'G_N' in k:\n",
    "                model_config['graph_node'][k]=v\n",
    "            elif 'G_E' in k:\n",
    "                model_config['graph_edge'][k]=v\n",
    "            elif 'G_A' in k:\n",
    "                model_config['graph_attn'][k]=v\n",
    "            else:\n",
    "                model_config[k]=v\n",
    "        \n",
    "        return model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_gaussian_filter_1D(kernel_size=3, sigma=2, channels=3):\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    \n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "    mean = (kernel_size - 1)/2.\n",
    "    variance = sigma**2.\n",
    "    xy_grid = torch.sum((xy_grid[:kernel_size,:kernel_size,:] - mean)**2., dim=-1)\n",
    "\n",
    "    # Calculate the 1-dimensional gaussian kernel\n",
    "    gaussian_kernel = (1./((math.sqrt(2.*math.pi)*sigma))) * \\\n",
    "                        torch.exp(-1* (xy_grid[int(kernel_size/2)]) / (2*variance))\n",
    "\n",
    "    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size)\n",
    "    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1)\n",
    "\n",
    "    padding = 1 if kernel_size==3 else 2 if kernel_size == 5 else 0\n",
    "    gaussian_filter = nn.Conv1d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels,\n",
    "                                bias=False, padding=padding)\n",
    "    gaussian_filter.weight.data = gaussian_kernel\n",
    "    gaussian_filter.weight.requires_grad = False \n",
    "    return gaussian_filter\n",
    "\n",
    "def get_laplaceOfGaussian_filter_1D(kernel_size=3, sigma=2, channels=3):\n",
    "    \n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "    mean = (kernel_size - 1)/2.\n",
    "\n",
    "    used_sigma = sigma\n",
    "    # Calculate the 2-dimensional gaussian kernel which is\n",
    "    log_kernel = (-1./(math.pi*(used_sigma**4))) \\\n",
    "                  * (1-(torch.sum((xy_grid[int(kernel_size/2)] - mean)**2., dim=-1) / (2*(used_sigma**2)))) \\\n",
    "                  * torch.exp(-torch.sum((xy_grid[int(kernel_size/2)] - mean)**2., dim=-1) / (2*(used_sigma**2)))\n",
    "    \n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    log_kernel = log_kernel / torch.sum(log_kernel)\n",
    "    log_kernel = log_kernel.view(1, 1, kernel_size)\n",
    "    log_kernel = log_kernel.repeat(channels, 1, 1)\n",
    "\n",
    "    padding = 1 if kernel_size==3 else 2 if kernel_size == 5 else 0\n",
    "    log_filter = nn.Conv1d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels,\n",
    "                                bias=False, padding=padding)\n",
    "    log_filter.weight.data = log_kernel\n",
    "    log_filter.weight.requires_grad = False\n",
    "    return log_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primary activation and MLP layer\n",
    "acivation:\n",
    "    Identity\n",
    "    ReLU\n",
    "    LeakyReLU\n",
    "MLP:\n",
    "    init: layer size, activation, bias, use_BN, dropout_probability\n",
    "    forward: x\n",
    "'''\n",
    "\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    '''\n",
    "    Identity class activation layer\n",
    "    x = x\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Identity,self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def get_activation(name):\n",
    "    '''\n",
    "    get_activation sub-function\n",
    "    argument: activatoin name (eg. ReLU, Identity, LeakyReLU)\n",
    "    '''\n",
    "    if name=='ReLU': return nn.ReLU(inplace=True)\n",
    "    elif name=='Identity': return Identity()\n",
    "    elif name=='LeakyReLU': return nn.LeakyReLU(0.2,inplace=True)\n",
    "    else: assert(False), 'Not Implemented'\n",
    "    #elif name=='Tanh': return nn.Tanh()\n",
    "    #elif name=='Sigmoid': return nn.Sigmoid()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Args:\n",
    "        layer_sizes: a list, [1024,1024,...]\n",
    "        activation: a list, ['ReLU', 'Tanh',...]\n",
    "        bias : bool\n",
    "        use_bn: bool\n",
    "        drop_prob: default is None, use drop out layer or not\n",
    "    '''\n",
    "    def __init__(self, layer_sizes, activation, bias=True, use_bn=False, drop_prob=None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.bn = use_bn\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=bias)\n",
    "            activate = get_activation(activation[i])\n",
    "            block = nn.Sequential(OrderedDict([(f'L{i}', layer), ]))\n",
    "            \n",
    "            # !NOTE:# Actually, it is inappropriate to use batch-normalization here\n",
    "            if use_bn:                                  \n",
    "                bn = nn.BatchNorm1d(layer_sizes[i+1])\n",
    "                block.add_module(f'B{i}', bn)\n",
    "            \n",
    "            # batch normalization is put before activation function \n",
    "            block.add_module(f'A{i}', activate)\n",
    "\n",
    "            # dropout probablility\n",
    "            if drop_prob:\n",
    "                block.add_module(f'D{i}', nn.Dropout(drop_prob))\n",
    "            \n",
    "            self.layers.append(block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # !NOTE: sometime the shape of x will be [1,N], and we cannot use batch-normailzation in that situation\n",
    "            if self.bn and x.shape[0]==1:\n",
    "                x = layer[0](x)\n",
    "                x = layer[:-1](x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "H_H_EdgeApplyModule\n",
    "    init    : config, multi_attn \n",
    "    forward : edge\n",
    "    \n",
    "H_NodeApplyModule\n",
    "    init    : config\n",
    "    forward : node\n",
    "    \n",
    "E_AttentionModule1\n",
    "    init    : config\n",
    "    forward : edge\n",
    "    \n",
    "GNN\n",
    "    init    : config, multi_attn, diff_edge\n",
    "    forward : g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_features\n",
    "    \n",
    "GRNN\n",
    "    init    : config, multi_attn, diff_edge\n",
    "    forward : b_graph, b_h_node_list, b_o_node_list, b_h_h_e_list, b_o_o_e_list, b_h_o_e_list, features, spatial_features, word2vec, valid, pop_features, initial_features\n",
    "'''\n",
    "\n",
    "import ipdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class H_H_EdgeApplyModule(nn.Module): #human to human edge\n",
    "    '''\n",
    "        init    : config, multi_attn \n",
    "        forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, use_cbs = False):\n",
    "        super(H_H_EdgeApplyModule, self).__init__()\n",
    "        self.use_cbs = use_cbs\n",
    "        if use_cbs:\n",
    "            self.init_std = CONFIG.G_E_c_std \n",
    "            self.cbs_std = CONFIG.G_E_c_std\n",
    "            self.cbs_std_factor = CONFIG.G_E_c_std_factor\n",
    "            self.cbs_epoch = CONFIG.G_E_c_epoch\n",
    "            self.cbs_kernel_size = CONFIG.G_E_c_kernel_size\n",
    "            self.cbs_filter = CONFIG.G_E_c_filter\n",
    "        \n",
    "        self.edge_fc = MLP(CONFIG.G_E_L_S, CONFIG.G_E_A, CONFIG.G_E_B, CONFIG.G_E_BN, CONFIG.G_E_D)\n",
    "        self.edge_fc_lang = MLP(CONFIG.G_E_L_S2, CONFIG.G_E_A2, CONFIG.G_E_B2, CONFIG.G_E_BN2, CONFIG.G_E_D2)\n",
    "    \n",
    "    def forward(self, edge):\n",
    "        feat = torch.cat([edge.src['n_f'], edge.data['s_f'], edge.dst['n_f']], dim=1)\n",
    "        feat_lang = torch.cat([edge.src['word2vec'], edge.dst['word2vec']], dim=1)\n",
    "        if self.use_cbs:\n",
    "            feat = self.kernel1(feat[:,None,:])\n",
    "            feat = torch.squeeze(feat, 1)\n",
    "        e_feat = self.edge_fc(feat)\n",
    "        e_feat_lang = self.edge_fc_lang(feat_lang)\n",
    "  \n",
    "        return {'e_f': e_feat, 'e_f_lang': e_feat_lang}\n",
    "\n",
    "    def get_new_kernels(self, epoch_count):\n",
    "        if self.use_cbs:\n",
    "            if epoch_count == 0:\n",
    "                self.cbs_std = self.init_std\n",
    "                \n",
    "            if epoch_count % self.cbs_epoch == 0 and epoch_count is not 0:\n",
    "                self.cbs_std *= self.cbs_std_factor\n",
    "            \n",
    "            if (self.cbs_filter == 'gau'): \n",
    "                self.kernel1 = get_gaussian_filter_1D(kernel_size=self.cbs_kernel_size, sigma= self.cbs_std, channels= 1)\n",
    "            elif (self.cbs_filter == 'LOG'): \n",
    "                self.kernel1 = get_laplaceOfGaussian_filter_1D(kernel_size=self.cbs_kernel_size, sigma= self.cbs_std, channels= 1)\n",
    "\n",
    "class H_NodeApplyModule(nn.Module): #human node\n",
    "    '''\n",
    "        init    : config\n",
    "        forward : node\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(H_NodeApplyModule, self).__init__()\n",
    "        self.node_fc = MLP(CONFIG.G_N_L_S, CONFIG.G_N_A, CONFIG.G_N_B, CONFIG.G_N_BN, CONFIG.G_N_D)\n",
    "        self.node_fc_lang = MLP(CONFIG.G_N_L_S2, CONFIG.G_N_A2, CONFIG.G_N_B2, CONFIG.G_N_BN2, CONFIG.G_N_D2)\n",
    "    \n",
    "    def forward(self, node):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        feat = torch.cat([node.data['n_f'], node.data['z_f']], dim=1)\n",
    "        feat_lang = torch.cat([node.data['word2vec'], node.data['z_f_lang']], dim=1)\n",
    "        n_feat = self.node_fc(feat)\n",
    "        n_feat_lang = self.node_fc_lang(feat_lang)\n",
    "\n",
    "        return {'new_n_f': n_feat, 'new_n_f_lang': n_feat_lang}\n",
    "\n",
    "class E_AttentionModule1(nn.Module): #edge attention\n",
    "    '''\n",
    "        init    : config\n",
    "        forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(E_AttentionModule1, self).__init__()\n",
    "        self.attn_fc = MLP(CONFIG.G_A_L_S, CONFIG.G_A_A, CONFIG.G_A_B, CONFIG.G_A_BN, CONFIG.G_A_D)\n",
    "        self.attn_fc_lang = MLP(CONFIG.G_A_L_S2, CONFIG.G_A_A2, CONFIG.G_A_B2, CONFIG.G_A_BN2, CONFIG.G_A_D2)\n",
    "\n",
    "    def forward(self, edge):\n",
    "        a_feat = self.attn_fc(edge.data['e_f'])\n",
    "        a_feat_lang = self.attn_fc_lang(edge.data['e_f_lang'])\n",
    "        return {'a_feat': a_feat, 'a_feat_lang': a_feat_lang}\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    '''\n",
    "        init    : config, multi_attn, diff_edge\n",
    "        forward : g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_features\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, diff_edge=True, use_cbs = False):\n",
    "        super(GNN, self).__init__()\n",
    "        self.diff_edge = diff_edge # false\n",
    "        self.apply_h_h_edge = H_H_EdgeApplyModule(CONFIG, multi_attn, use_cbs)\n",
    "        self.apply_edge_attn1 = E_AttentionModule1(CONFIG)  \n",
    "        self.apply_h_node = H_NodeApplyModule(CONFIG)\n",
    "\n",
    "    def _message_func(self, edges):\n",
    "        return {'nei_n_f': edges.src['n_f'], 'nei_n_w': edges.src['word2vec'], 'e_f': edges.data['e_f'], 'e_f_lang': edges.data['e_f_lang'], 'a_feat': edges.data['a_feat'], 'a_feat_lang': edges.data['a_feat_lang']}\n",
    "\n",
    "    def _reduce_func(self, nodes):\n",
    "        alpha = F.softmax(nodes.mailbox['a_feat'], dim=1)\n",
    "        alpha_lang = F.softmax(nodes.mailbox['a_feat_lang'], dim=1)\n",
    "\n",
    "        z_raw_f = nodes.mailbox['nei_n_f']+nodes.mailbox['e_f']\n",
    "        z_f = torch.sum( alpha * z_raw_f, dim=1)\n",
    "\n",
    "        z_raw_f_lang = nodes.mailbox['nei_n_w']\n",
    "        z_f_lang = torch.sum(alpha_lang * z_raw_f_lang, dim=1)\n",
    "         \n",
    "        # we cannot return 'alpha' for the different dimension \n",
    "        if self.training or validation: return {'z_f': z_f, 'z_f_lang': z_f_lang}\n",
    "        else: return {'z_f': z_f, 'z_f_lang': z_f_lang, 'alpha': alpha, 'alpha_lang': alpha_lang}\n",
    "\n",
    "    def forward(self, g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_feat=False):\n",
    "        \n",
    "        g.apply_edges(self.apply_h_h_edge, g.edges())\n",
    "        g.apply_edges(self.apply_edge_attn1)\n",
    "        g.update_all(self._message_func, self._reduce_func)\n",
    "        g.apply_nodes(self.apply_h_node, h_node+o_node)\n",
    "\n",
    "        # !NOTE:PAY ATTENTION WHEN ADDING MORE FEATURE\n",
    "        g.ndata.pop('n_f')\n",
    "        g.ndata.pop('word2vec')\n",
    "\n",
    "        g.ndata.pop('z_f')\n",
    "        g.edata.pop('e_f')\n",
    "        g.edata.pop('a_feat')\n",
    "\n",
    "        g.ndata.pop('z_f_lang')\n",
    "        g.edata.pop('e_f_lang')\n",
    "        g.edata.pop('a_feat_lang')\n",
    "\n",
    "class GRNN(nn.Module):\n",
    "    '''\n",
    "    init: \n",
    "        config, multi_attn, diff_edge\n",
    "    forward: \n",
    "        batch_graph, batch_h_node_list, batch_obj_node_list,\n",
    "        batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list,\n",
    "        features, spatial_features, word2vec,\n",
    "        valid, pop_features, initial_features\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, diff_edge=True, use_cbs = False):\n",
    "        super(GRNN, self).__init__()\n",
    "        self.multi_attn = multi_attn #false\n",
    "        self.gnn = GNN(CONFIG, multi_attn, diff_edge, use_cbs)\n",
    "\n",
    "    def forward(self, batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec, valid=False, pop_feat=False, initial_feat=False):\n",
    "        \n",
    "        # !NOTE: if node_num==1, there will be something wrong to forward the attention mechanism\n",
    "        global validation \n",
    "        validation = valid\n",
    "\n",
    "        # initialize the graph with some datas\n",
    "        batch_graph.ndata['n_f'] = feat           # node: features \n",
    "        batch_graph.ndata['word2vec'] = word2vec  # node: words\n",
    "        batch_graph.edata['s_f'] = spatial_feat   # edge: spatial features\n",
    "\n",
    "        try:\n",
    "            self.gnn(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            ipdb.set_trace()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Predictor \n",
    "    init    : config\n",
    "    forward : edge\n",
    "\n",
    "AGRNN\n",
    "    init    : bias, bn, dropout, multi_attn, layer, diff_edge\n",
    "    forward : node_num, feat, spatial_feat, word2vec, roi_label, validation, choose_nodes, remove_nodes\n",
    "'''\n",
    "\n",
    "import dgl\n",
    "import ipdb\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchvision\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    '''\n",
    "    init    : config\n",
    "    forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.classifier = MLP(CONFIG.G_ER_L_S, CONFIG.G_ER_A, CONFIG.G_ER_B, CONFIG.G_ER_BN, CONFIG.G_ER_D)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, edge):\n",
    "        feat = torch.cat([edge.dst['new_n_f'], edge.dst['new_n_f_lang'], edge.data['s_f'], edge.src['new_n_f_lang'], edge.src['new_n_f']], dim=1)\n",
    "        pred = self.classifier(feat)\n",
    "        # if the criterion is BCELoss, you need to uncomment the following code\n",
    "        # output = self.sigmoid(output)\n",
    "        return {'pred': pred}\n",
    "\n",
    "class AGRNN(nn.Module):\n",
    "    '''\n",
    "    init    : \n",
    "        feature_type, bias, bn, dropout, multi_attn, layer, diff_edge\n",
    "        \n",
    "    forward : \n",
    "        node_num, features, spatial_features, word2vec, roi_label,\n",
    "        validation, choose_nodes, remove_nodes\n",
    "    '''\n",
    "    def __init__(self, bias=True, bn=True, dropout=None, multi_attn=False, layer=1, diff_edge=True, use_cbs = False):\n",
    "        super(AGRNN, self).__init__()\n",
    " \n",
    "        self.multi_attn = multi_attn # false\n",
    "        self.layer = layer           # 1 layer\n",
    "        self.diff_edge = diff_edge   # false\n",
    "        \n",
    "        self.CONFIG1 = CONFIGURATION(layer=1, bias=bias, bn=bn, dropout=dropout, multi_attn=multi_attn)\n",
    "\n",
    "        self.grnn1 = GRNN(self.CONFIG1, multi_attn=multi_attn, diff_edge=diff_edge, use_cbs = use_cbs)\n",
    "        self.edge_readout = Predictor(self.CONFIG1)\n",
    "        \n",
    "    def _collect_edge(self, node_num, roi_label, node_space, diff_edge):\n",
    "        '''\n",
    "        arguments: node_num, roi_label, node_space, diff_edge\n",
    "        '''\n",
    "        \n",
    "        # get human nodes && object nodes\n",
    "        h_node_list = np.where(roi_label == 0)[0]\n",
    "        obj_node_list = np.where(roi_label != 0)[0]\n",
    "        edge_list = []\n",
    "        \n",
    "        h_h_e_list = []\n",
    "        o_o_e_list = []\n",
    "        h_o_e_list = []\n",
    "        \n",
    "        readout_edge_list = []\n",
    "        readout_h_h_e_list = []\n",
    "        readout_h_o_e_list = []\n",
    "        \n",
    "        # get all edge in the fully-connected graph, edge_list, For node_num = 2, edge_list = [(0, 1), (1, 0)]\n",
    "        for src in range(node_num):\n",
    "            for dst in range(node_num):\n",
    "                if src == dst:\n",
    "                    continue\n",
    "                else:\n",
    "                    edge_list.append((src, dst))\n",
    "        \n",
    "        # readout_edge_list, get corresponding readout edge in the graph\n",
    "        src_box_list = np.arange(roi_label.shape[0])\n",
    "        for dst in h_node_list:\n",
    "            # if dst == roi_label.shape[0]-1:\n",
    "            #    continue\n",
    "            # src_box_list = src_box_list[1:]\n",
    "            for src in src_box_list:\n",
    "                if src not in h_node_list:\n",
    "                    readout_edge_list.append((src, dst))\n",
    "        \n",
    "        # readout h_h_e_list, get corresponding readout h_h edges && h_o edges\n",
    "        temp_h_node_list = h_node_list[:]\n",
    "        for dst in h_node_list:\n",
    "            if dst == h_node_list.shape[0]-1:\n",
    "                continue\n",
    "            temp_h_node_list = temp_h_node_list[1:]\n",
    "            for src in temp_h_node_list:\n",
    "                if src == dst: continue\n",
    "                readout_h_h_e_list.append((src, dst))\n",
    "\n",
    "        # readout h_o_e_list\n",
    "        readout_h_o_e_list = [x for x in readout_edge_list if x not in readout_h_h_e_list]\n",
    "\n",
    "        # add node space to match the batch graph\n",
    "        h_node_list = (np.array(h_node_list)+node_space).tolist()\n",
    "        obj_node_list = (np.array(obj_node_list)+node_space).tolist()\n",
    "        \n",
    "        h_h_e_list = (np.array(h_h_e_list)+node_space).tolist() #empty no diff_edge\n",
    "        o_o_e_list = (np.array(o_o_e_list)+node_space).tolist() #empty no diff_edge\n",
    "        h_o_e_list = (np.array(h_o_e_list)+node_space).tolist() #empty no diff_edge\n",
    "\n",
    "        readout_h_h_e_list = (np.array(readout_h_h_e_list)+node_space).tolist()\n",
    "        readout_h_o_e_list = (np.array(readout_h_o_e_list)+node_space).tolist()   \n",
    "        readout_edge_list = (np.array(readout_edge_list)+node_space).tolist()\n",
    "\n",
    "        return edge_list, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list\n",
    "    \n",
    "    def _build_graph(self, node_num, roi_label, node_space, diff_edge):\n",
    "        '''\n",
    "        Declare graph, add_nodes, collect edges, add_edges\n",
    "        '''\n",
    "        graph = dgl.DGLGraph()\n",
    "        graph.add_nodes(node_num)\n",
    "\n",
    "        edge_list, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list = self._collect_edge(node_num, roi_label, node_space, diff_edge)\n",
    "        src, dst = tuple(zip(*edge_list))\n",
    "        graph.add_edges(src, dst)   # make the graph bi-directional\n",
    "\n",
    "        return graph, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list\n",
    "\n",
    "    def forward(self, node_num=None, feat=None, spatial_feat=None, word2vec=None, roi_label=None, validation=False, choose_nodes=None, remove_nodes=None):\n",
    "        \n",
    "        batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, batch_readout_edge_list, batch_readout_h_h_e_list, batch_readout_h_o_e_list = [], [], [], [], [], [], [], [], []\n",
    "        node_num_cum = np.cumsum(node_num) # !IMPORTANT\n",
    "        \n",
    "        for i in range(len(node_num)):\n",
    "            # set node space\n",
    "            node_space = 0\n",
    "            if i != 0:\n",
    "                node_space = node_num_cum[i-1]\n",
    "            graph, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list = self._build_graph(node_num[i], roi_label[i], node_space, diff_edge=self.diff_edge)\n",
    "            \n",
    "            # updata batch\n",
    "            batch_graph.append(graph)\n",
    "            batch_h_node_list += h_node_list\n",
    "            batch_obj_node_list += obj_node_list\n",
    "            \n",
    "            batch_h_h_e_list += h_h_e_list\n",
    "            batch_o_o_e_list += o_o_e_list\n",
    "            batch_h_o_e_list += h_o_e_list\n",
    "            \n",
    "            batch_readout_edge_list += readout_edge_list\n",
    "            batch_readout_h_h_e_list += readout_h_h_e_list\n",
    "            batch_readout_h_o_e_list += readout_h_o_e_list\n",
    "        \n",
    "        batch_graph = dgl.batch(batch_graph)\n",
    "        \n",
    "        # GRNN\n",
    "        self.grnn1(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec, validation, initial_feat=True)\n",
    "        batch_graph.apply_edges(self.edge_readout, tuple(zip(*(batch_readout_h_o_e_list+batch_readout_h_h_e_list))))\n",
    "        \n",
    "        if self.training or validation:\n",
    "            # !NOTE: cannot use \"batch_readout_h_o_e_list+batch_readout_h_h_e_list\" because of the wrong order\n",
    "            return batch_graph.edges[tuple(zip(*batch_readout_edge_list))].data['pred']\n",
    "        else:\n",
    "            return batch_graph.edges[tuple(zip(*batch_readout_edge_list))].data['pred'], \\\n",
    "                   batch_graph.nodes[batch_h_node_list].data['alpha'], \\\n",
    "                   batch_graph.nodes[batch_h_node_list].data['alpha_lang'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils.io as io\n",
    "\n",
    "class SurgicalSceneConstants():\n",
    "    def __init__( self):\n",
    "        self.instrument_classes = ('kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "        \n",
    "        #self.instrument_classes = ( 'kidney', 'bipolar_forceps', 'fenestrated_bipolar', \n",
    "        #                             'prograsp_forceps', 'large_needle_driver', 'vessel_sealer',\n",
    "        #                             'grasping_retractor', 'monopolar_curved_scissors', \n",
    "        #                             'ultrasound_probe', 'suction', 'clip_applier', 'stapler')\n",
    "        \n",
    "        self.action_classes = ( 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                                'Tool_Manipulation', 'Cutting', 'Cauterization', \n",
    "                                'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', \n",
    "                                'Ultrasound_Sensing')\n",
    "        self.xml_data_dir = 'datasets/instruments18/seq_'\n",
    "        self.word2vec_loc = 'datasets/surgicalscene_word2vec.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "    \n",
    "class SurgicalSceneDataset(Dataset):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, seq_set, data_dir, img_dir, dset, dataconst, feature_extractor, reduce_size = False):\n",
    "        \n",
    "        \n",
    "        self.data_size = 143\n",
    "        self.dataconst = dataconst\n",
    "        self.img_dir = img_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.reduce_size = reduce_size\n",
    "        \n",
    "        self.xml_dir_list = []\n",
    "        self.dset = []\n",
    "        \n",
    "        for domain in range(len(seq_set)):\n",
    "            domain_dir_list = []\n",
    "            for i in seq_set[domain]:\n",
    "                xml_dir_temp = data_dir[domain] + str(i) + '/xml/'\n",
    "                domain_dir_list = domain_dir_list + glob(xml_dir_temp + '/*.xml')\n",
    "            if self.reduce_size:\n",
    "                indices = np.random.permutation(len(domain_dir_list))\n",
    "                domain_dir_list = [domain_dir_list[j] for j in indices[0:self.data_size]]\n",
    "            for file in domain_dir_list: \n",
    "                self.xml_dir_list.append(file)\n",
    "                self.dset.append(dset[domain])\n",
    "        self.word2vec = h5py.File('datasets/surgicalscene_word2vec.hdf5', 'r')\n",
    "    \n",
    "    # word2vec\n",
    "    def _get_word2vec(self,node_ids, sgh = 0):\n",
    "        word2vec = np.empty((0,300))\n",
    "        for node_id in node_ids:\n",
    "            if sgh == 1 and node_id == 0:\n",
    "                vec = self.word2vec['tissue']\n",
    "            else:\n",
    "                vec = self.word2vec[self.dataconst.instrument_classes[node_id]]\n",
    "            word2vec = np.vstack((word2vec, vec))\n",
    "        return word2vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xml_dir_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        file_name = os.path.splitext(os.path.basename(self.xml_dir_list[idx]))[0]\n",
    "        file_root = os.path.dirname(os.path.dirname(self.xml_dir_list[idx]))\n",
    "        if len(self.img_dir) == 1:\n",
    "            _img_loc = os.path.join(file_root+self.img_dir[0]+ file_name + '.png')\n",
    "        else:\n",
    "            _img_loc = os.path.join(file_root+self.img_dir[self.dset[idx]]+ file_name + '.png')\n",
    "        frame_data = h5py.File(os.path.join(file_root+'/vsgat/'+self.feature_extractor+'/'+ file_name + '_features.hdf5'), 'r')    \n",
    "        data = {}\n",
    "        data['img_name'] = frame_data['img_name'].value[:] + '.jpg'\n",
    "        data['img_loc'] = _img_loc\n",
    "        \n",
    "        data['node_num'] = frame_data['node_num'].value\n",
    "        data['roi_labels'] = frame_data['classes'][:]\n",
    "        data['det_boxes'] = frame_data['boxes'][:]\n",
    "        \n",
    "        \n",
    "        data['edge_labels'] = frame_data['edge_labels'][:]\n",
    "        data['edge_num'] = data['edge_labels'].shape[0]\n",
    "        \n",
    "        data['features'] = frame_data['node_features'][:]\n",
    "        data['spatial_feat'] = frame_data['spatial_features'][:]\n",
    "        \n",
    "        \n",
    "        data['word2vec'] = self._get_word2vec(data['roi_labels'], self.dset[idx])\n",
    "        return data\n",
    "\n",
    "# for DatasetLoader\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "        Default collate_fn(): https://github.com/pytorch/pytorch/blob/1d53d0756668ce641e4f109200d9c65b003d05fa/torch/utils/data/_utils/collate.py#L43\n",
    "    '''\n",
    "    batch_data = {}\n",
    "    batch_data['img_name'] = []\n",
    "    batch_data['img_loc'] = []\n",
    "    batch_data['node_num'] = []\n",
    "    batch_data['roi_labels'] = []\n",
    "    batch_data['det_boxes'] = []\n",
    "    batch_data['edge_labels'] = []\n",
    "    batch_data['edge_num'] = []\n",
    "    batch_data['features'] = []\n",
    "    batch_data['spatial_feat'] = []\n",
    "    batch_data['word2vec'] = []\n",
    "    \n",
    "    for data in batch:\n",
    "        batch_data['img_name'].append(data['img_name'])\n",
    "        batch_data['img_loc'].append(data['img_loc'])\n",
    "        batch_data['node_num'].append(data['node_num'])\n",
    "        batch_data['roi_labels'].append(data['roi_labels'])\n",
    "        batch_data['det_boxes'].append(data['det_boxes'])\n",
    "        batch_data['edge_labels'].append(data['edge_labels'])\n",
    "        batch_data['edge_num'].append(data['edge_num'])\n",
    "        batch_data['features'].append(data['features'])\n",
    "        batch_data['spatial_feat'].append(data['spatial_feat'])\n",
    "        batch_data['word2vec'].append(data['word2vec'])\n",
    "        \n",
    "    batch_data['edge_labels'] = torch.FloatTensor(np.concatenate(batch_data['edge_labels'], axis=0))\n",
    "    batch_data['features'] = torch.FloatTensor(np.concatenate(batch_data['features'], axis=0))\n",
    "    batch_data['spatial_feat'] = torch.FloatTensor(np.concatenate(batch_data['spatial_feat'], axis=0))\n",
    "    batch_data['word2vec'] = torch.FloatTensor(np.concatenate(batch_data['word2vec'], axis=0))\n",
    "    \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch as t\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plot\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def vis_img(img, node_classes, bboxs,  det_action, score_thresh = 0.7):\n",
    "    \n",
    "    Drawer = ImageDraw.Draw(img)\n",
    "    line_width = 3\n",
    "    outline = '#FF0000'\n",
    "    font = ImageFont.truetype(font='/usr/share/fonts/truetype/freefont/FreeMono.ttf', size=25)\n",
    "    \n",
    "    im_w,im_h = img.size\n",
    "    node_num = len(node_classes)\n",
    "    edge_num = len(det_action)\n",
    "    tissue_num = len(np.where(node_classes == 1)[0])\n",
    "    \n",
    "    for node in range(node_num):\n",
    "        \n",
    "        r_color = random.choice(np.arange(256))\n",
    "        g_color = random.choice(np.arange(256))\n",
    "        b_color = random.choice(np.arange(256))\n",
    "        \n",
    "        text = data_const.instrument_classes[node_classes[node]]\n",
    "        h, w = font.getsize(text)\n",
    "        Drawer.rectangle(list(bboxs[node]), outline=outline, width=line_width)\n",
    "        Drawer.text(xy=(bboxs[node][0], bboxs[node][1]-w-1), text=text, font=font, fill=(r_color,g_color,b_color))\n",
    "  \n",
    "    edge_idx = 0\n",
    "    \n",
    "    for tissue in range(tissue_num):\n",
    "        for instrument in range(tissue+1, node_num):\n",
    "            \n",
    "            #action_idx = np.where(det_action[edge_idx] > score_thresh)[0]\n",
    "            action_idx = np.argmax(det_action[edge_idx])\n",
    "#             print('det_action', det_action[edge_idx])\n",
    "#             print('action_idx',action_idx)\n",
    "            \n",
    "            text = data_const.action_classes[action_idx]\n",
    "            r_color = random.choice(np.arange(256))\n",
    "            g_color = random.choice(np.arange(256))\n",
    "            b_color = random.choice(np.arange(256))\n",
    "        \n",
    "            x1,y1,x2,y2 = bboxs[tissue]\n",
    "            x1_,y1_,x2_,y2_ = bboxs[instrument]\n",
    "            \n",
    "            c0 = int(0.5*x1)+int(0.5*x2)\n",
    "            c0 = max(0,min(c0,im_w-1))\n",
    "            r0 = int(0.5*y1)+int(0.5*y2)\n",
    "            r0 = max(0,min(r0,im_h-1))\n",
    "            c1 = int(0.5*x1_)+int(0.5*x2_)\n",
    "            c1 = max(0,min(c1,im_w-1))\n",
    "            r1 = int(0.5*y1_)+int(0.5*y2_)\n",
    "            r1 = max(0,min(r1,im_h-1))\n",
    "            Drawer.line(((c0,r0),(c1,r1)), fill=(r_color,g_color,b_color), width=3)\n",
    "            Drawer.text(xy=(c1, r1), text=text, font=font, fill=(r_color,g_color,b_color))\n",
    "\n",
    "            edge_idx +=1\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import utils.io as io\n",
    "#from utils.vis_tool import vis_img\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def run_model(args, data_const):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # use cpu or cuda\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.gpu else 'cpu')\n",
    "    print('training on {}...'.format(device))\n",
    "\n",
    "    # model\n",
    "    model = AGRNN(bias=args.bias, bn=args.bn, dropout=args.drop_prob, multi_attn=args.multi_attn, layer=args.layers, diff_edge=args.diff_edge, use_cbs = args.use_cbs)\n",
    "    if args.use_cbs: model.grnn1.gnn.apply_h_h_edge.get_new_kernels(0)\n",
    "    \n",
    "    # calculate the amount of all the learned parameters\n",
    "    parameter_num = 0\n",
    "    for param in model.parameters(): parameter_num += param.numel()\n",
    "    print(f'The parameters number of the model is {parameter_num / 1e6} million')\n",
    "\n",
    "    # load pretrained model\n",
    "    if args.pretrained:\n",
    "        print(f\"loading pretrained model {args.pretrained}\")\n",
    "        checkpoints = torch.load(args.pretrained, map_location=device)\n",
    "        model.load_state_dict(checkpoints['state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.3) #the scheduler divides the lr by 10 every 150 epochs\n",
    "\n",
    "    # get the configuration of the model and save some key configurations\n",
    "    io.mkdir_if_not_exists(os.path.join(args.save_dir, args.exp_ver), recursive=True)\n",
    "    for i in range(args.layers):\n",
    "        if i==0:\n",
    "            model_config = model.CONFIG1.save_config()\n",
    "            model_config['lr'] = args.lr\n",
    "            model_config['bs'] = args.batch_size\n",
    "            model_config['layers'] = args.layers\n",
    "            model_config['multi_attn'] = args.multi_attn\n",
    "            model_config['data_aug'] = args.data_aug\n",
    "            model_config['drop_out'] = args.drop_prob\n",
    "            model_config['optimizer'] = args.optim\n",
    "            model_config['diff_edge'] = args.diff_edge\n",
    "            model_config['model_parameters'] = parameter_num\n",
    "            io.dump_json_object(model_config, os.path.join(args.save_dir, args.exp_ver, 'l1_config.json'))\n",
    "    print('save key configurations successfully...')\n",
    "\n",
    "    # domain 1\n",
    "    train_seq = [[2,3,4,6,7,9,10,11,12,14,15]]\n",
    "    val_seq = [[1,5,16]]\n",
    "    data_dir = ['datasets/instruments18/seq_']\n",
    "    img_dir = ['/left_frames/']\n",
    "    dset = [0] # 0 for ISC, 1 for SGH\n",
    "    seq = {'train_seq': train_seq, 'val_seq': val_seq, 'data_dir': data_dir, 'img_dir':img_dir, 'dset': dset}\n",
    "    print('======================== Domain 1 ==============================')\n",
    "    epoch_train(args, model,seq, device, \"D1\")\n",
    "    \n",
    "    # domain 2\n",
    "    train_seq = [[2,3,4,6,7,9,10,11,12,14,15], [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]]\n",
    "    val_seq = [[1,5,16],[16,17,18,19,20,21,22]]\n",
    "    data_dir = ['datasets/instruments18/seq_', 'datasets/SGH_dataset_2020/']\n",
    "    img_dir = ['/left_frames/', '/resized_frames/']\n",
    "    dset = [0, 1]\n",
    "    seq = {'train_seq': train_seq, 'val_seq': val_seq, 'data_dir': data_dir, 'img_dir':img_dir, 'dset': dset}\n",
    "    print('======================== Domain 2 ==============================')\n",
    "    epoch_train(args, model,seq, device, \"D2\")\n",
    "    print('======================== Domain 1-2 FT =========================')\n",
    "    epoch_train(args, model,seq, device, \"D2F\", finetune = True)\n",
    "    \n",
    "\n",
    "def epoch_train(args, model, seq, device, dname, finetune = False):\n",
    "    '''\n",
    "    input: model, dataloader, dataset, criterain, optimizer, scheduler, device, data_const\n",
    "    data: \n",
    "        img_name, node_num, roi_labels, det_boxes, edge_labels,\n",
    "        edge_num, features, spatial_features, word2vec\n",
    "    '''\n",
    "    \n",
    "    new_domain = False\n",
    "    stop_epoch = args.epoch\n",
    "    \n",
    "    if finetune:\n",
    "        stop_epoch = args.ft_epoch\n",
    "        train_dataset = SurgicalSceneDataset(seq_set = seq['train_seq'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = True)\n",
    "        val_dataset = SurgicalSceneDataset(seq_set = seq['val_seq'], dset = seq['dset'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "        dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "        model_old = None\n",
    "    \n",
    "    # train and test dataset for one domain\n",
    "    elif (len(seq['train_seq']) == 1):\n",
    "        # set up dataset variable\n",
    "        train_dataset = SurgicalSceneDataset(seq_set = seq['train_seq'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "        val_dataset = SurgicalSceneDataset(seq_set = seq['val_seq'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "        dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "        model_old = None\n",
    "   \n",
    "    # train and test for multiple domain\n",
    "    elif (len(seq['train_seq']) > 1):\n",
    "        # set up dataset variable\n",
    "        new_domain = True\n",
    "        curr_tr_seq = seq['train_seq'][len(seq['train_seq'])-1:]\n",
    "        curr_tr_data_dir = seq['data_dir'][len(seq['data_dir'])-1:]\n",
    "        curr_tr_img_dir = seq['img_dir'][len(seq['img_dir'])-1:]\n",
    "        curr_dset = seq['dset'][len(seq['dset'])-1:]\n",
    "        #print(curr_tr_seq, curr_tr_data_dir, curr_tr_img_dir, curr_dset)\n",
    "        train_dataset = SurgicalSceneDataset(seq_set = curr_tr_seq, data_dir = curr_tr_data_dir, \\\n",
    "                            img_dir = curr_tr_img_dir, dset = curr_dset, dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "        val_dataset = SurgicalSceneDataset(seq_set = seq['val_seq'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "        dataset = {'train': train_dataset, 'val': val_dataset}\n",
    "        model_old = copy.deepcopy(model)\n",
    "    \n",
    "    # use default DataLoader() to load the data. \n",
    "    train_dataloader = DataLoader(dataset=dataset['train'], batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(dataset=dataset['val'], batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "    dataloader = {'train': train_dataloader, 'val': val_dataloader}\n",
    "    \n",
    "    # criterion and scheduler\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # set visualization and create folder to save checkpoints\n",
    "    writer = SummaryWriter(log_dir=args.log_dir + '/' + args.exp_ver + '/' + 'epoch_train')\n",
    "    io.mkdir_if_not_exists(os.path.join(args.save_dir, args.exp_ver, 'epoch_train'), recursive=True)\n",
    "\n",
    "    for epoch in range(args.start_epoch, stop_epoch):\n",
    "        \n",
    "        # each epoch has a training and validation step\n",
    "        epoch_acc = 0\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # finetune\n",
    "        if finetune:\n",
    "            train_dataset = SurgicalSceneDataset(seq_set = seq['train_seq'], data_dir = seq['data_dir'], \\\n",
    "                                img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                                feature_extractor = args.feature_extractor, reduce_size = True)\n",
    "            dataset['train'] = train_dataset\n",
    "            train_dataloader = DataLoader(dataset=dataset['train'], batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "            dataloader['train'] = train_dataloader\n",
    "\n",
    "        # build optimizer  \n",
    "        if finetune: lrc = args.lr / 10\n",
    "        else: lrc = args.lr\n",
    "        \n",
    "        if args.optim == 'sgd': \n",
    "            optimizer = optim.SGD(model.parameters(), lr= lrc, momentum=0.9, weight_decay=0)\n",
    "        else: \n",
    "            optimizer = optim.Adam(model.parameters(), lr= lrc, weight_decay=0)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            idx = 0\n",
    "            running_acc = 0.0\n",
    "            running_loss = 0.0\n",
    "            running_edge_count = 0\n",
    "            \n",
    "            if phase == 'train' and args.use_cbs:\n",
    "                model.grnn1.gnn.apply_h_h_edge.get_new_kernels(epoch)\n",
    "                model.to(device)\n",
    "            \n",
    "            #print(len(dataloader[phase]))\n",
    "            #for data in tqdm(dataloader[phase]):\n",
    "            for data in dataloader[phase]:\n",
    "                train_data = data\n",
    "                img_name = train_data['img_name']\n",
    "                img_loc = train_data['img_loc']\n",
    "                node_num = train_data['node_num']\n",
    "                roi_labels = train_data['roi_labels']\n",
    "                det_boxes = train_data['det_boxes']\n",
    "                edge_labels = train_data['edge_labels']\n",
    "                edge_num = train_data['edge_num']\n",
    "                features = train_data['features']\n",
    "                spatial_feat = train_data['spatial_feat']\n",
    "                word2vec = train_data['word2vec']\n",
    "                features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "                \n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                    model.zero_grad()\n",
    "                    outputs = model(node_num, features, spatial_feat, word2vec, roi_labels)\n",
    "                    \n",
    "                    # loss and accuracy\n",
    "                    if args.use_t: outputs = outputs / args.t_scale\n",
    "                    loss = criterion(outputs, edge_labels.float())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    acc = np.sum(np.equal(np.argmax(outputs.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    # turn off the gradients for validation, save memory and computations\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(node_num, features, spatial_feat, word2vec, roi_labels, validation=True)\n",
    "                        \n",
    "                        # loss and accuracy\n",
    "                        loss = criterion(outputs, edge_labels.float())\n",
    "                        acc = np.sum(np.equal(np.argmax(outputs.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "                    \n",
    "                        # print result every 1000 iteration during validation\n",
    "                        if idx == 10:\n",
    "                            #print(img_loc[0])\n",
    "                            io.mkdir_if_not_exists(os.path.join(args.output_img_dir, ('epoch_'+str(epoch))), recursive=True)\n",
    "                            image = Image.open(img_loc[0]).convert('RGB')\n",
    "                            det_actions = nn.Sigmoid()(outputs[0:int(edge_num[0])])\n",
    "                            det_actions = det_actions.cpu().detach().numpy()\n",
    "                            action_img = vis_img(image, roi_labels[0], det_boxes[0],  det_actions, score_thresh = 0.7)\n",
    "                            image = image.save(os.path.join(args.output_img_dir, ('epoch_'+str(epoch)),img_name[0]))\n",
    "\n",
    "                idx+=1\n",
    "                # accumulate loss of each batch\n",
    "                running_loss += loss.item() * edge_labels.shape[0]\n",
    "                running_acc += acc\n",
    "                running_edge_count += edge_labels.shape[0]\n",
    "            \n",
    "            # distillation learning\n",
    "            if phase == 'train' and new_domain:\n",
    "                \n",
    "                # distillation loss activation\n",
    "                dist_loss_act = nn.Softmax(dim=1)\n",
    "                dist_loss_act = dist_loss_act.to(device)\n",
    "            \n",
    "                dis_seq = seq['train_seq'][:-1]\n",
    "                dis_data_dir = seq['data_dir'][:-1]\n",
    "                dis_img_dir = seq['img_dir'][:-1]\n",
    "                dis_dset = seq['dset'][:-1]\n",
    "                dis_train_dataset = SurgicalSceneDataset(seq_set =  dis_seq, data_dir = dis_data_dir, \\\n",
    "                                        img_dir = dis_img_dir, dset = dis_dset, dataconst = data_const, \\\n",
    "                                        feature_extractor = args.feature_extractor, reduce_size = True)\n",
    "                dis_train_dataloader = DataLoader(dataset=dis_train_dataset, batch_size=args.batch_size, shuffle= True, collate_fn=collate_fn)\n",
    "                \n",
    "#                 if args.use_cbs:\n",
    "#                     model_old.grnn1.gnn.apply_h_h_edge.get_new_kernels(epoch)\n",
    "#                     model_old.to(device)\n",
    "        \n",
    "                #print(len(dis_train_dataloader))\n",
    "                #for data in tqdm(dataloader[phase]):\n",
    "                for data in dis_train_dataloader:\n",
    "                    train_data = data\n",
    "                    img_name = train_data['img_name']\n",
    "                    img_loc = train_data['img_loc']\n",
    "                    node_num = train_data['node_num']\n",
    "                    roi_labels = train_data['roi_labels']\n",
    "                    det_boxes = train_data['det_boxes']\n",
    "                    edge_labels = train_data['edge_labels']\n",
    "                    edge_num = train_data['edge_num']\n",
    "                    features = train_data['features']\n",
    "                    spatial_feat = train_data['spatial_feat']\n",
    "                    word2vec = train_data['word2vec']\n",
    "                    features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "                    \n",
    "                    model.train()\n",
    "                    model_old.train()\n",
    "                    model.zero_grad()\n",
    "                    outputs = model(node_num, features, spatial_feat, word2vec, roi_labels)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # old network output\n",
    "                        output_old = model_old(node_num, features, spatial_feat, word2vec, roi_labels)\n",
    "                        output_old = Variable(output_old, requires_grad=False)\n",
    "                    \n",
    "                    if args.use_t:\n",
    "                        outputs = outputs/args.t_scale\n",
    "                        output_old = output_old/args.t_scale\n",
    "                    d_loss = F.binary_cross_entropy(dist_loss_act(outputs), dist_loss_act(output_old))\n",
    "                    loss = criterion(outputs, edge_labels.float()) + 0.5* d_loss\n",
    "                    \n",
    "                    # loss and accuracy\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # calculate the loss and accuracy of each epoch\n",
    "            epoch_loss = running_loss / len(dataset[phase])\n",
    "            epoch_acc = running_acc / running_edge_count\n",
    "            \n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            # log trainval datas, and visualize them in the same graph\n",
    "            if phase == 'train':\n",
    "                train_loss = epoch_loss \n",
    "            else:\n",
    "                writer.add_scalars('trainval_loss_epoch', {'train': train_loss, 'val': epoch_loss}, epoch)\n",
    "            \n",
    "            # print data\n",
    "            if (epoch % args.print_every) == 0:\n",
    "                end_time = time.time()\n",
    "                print(\"[{}] Epoch: {}/{} Acc: {:0.6f} Loss: {:0.6f} Execution time: {:0.6f}\".format(\\\n",
    "                        phase, epoch+1, args.epoch, epoch_acc, epoch_loss, (end_time-start_time)))\n",
    "                        \n",
    "        # scheduler.step()\n",
    "        # save model\n",
    "        if epoch_loss<0.0405 or epoch % args.save_every == (args.save_every - 1) and epoch >= (20-1):\n",
    "            checkpoint = { \n",
    "                            'lr': args.lr,\n",
    "                           'b_s': args.batch_size,\n",
    "                          'bias': args.bias, \n",
    "                            'bn': args.bn, \n",
    "                       'dropout': args.drop_prob,\n",
    "                        'layers': args.layers,\n",
    "                    'multi_head': args.multi_attn,\n",
    "                     'diff_edge': args.diff_edge,\n",
    "                    'state_dict': model.state_dict()\n",
    "            }\n",
    "            save_name = \"checkpoint_\" + dname + str(epoch+1) + '_epoch.pth'\n",
    "            torch.save(checkpoint, os.path.join(args.save_dir, args.exp_ver, 'epoch_train', save_name))\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18_09\n",
      "training on cuda...\n",
      "The parameters number of the model is 2.393694 million\n",
      "save key configurations successfully...\n",
      "======================== Domain 1 ==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Epoch: 1/251 Acc: 0.057687 Loss: 3.139412 Execution time: 5.549406\n",
      "[val] Epoch: 1/251 Acc: 0.003445 Loss: 1.609188 Execution time: 1.497931\n",
      "[train] Epoch: 11/251 Acc: 0.291924 Loss: 0.988341 Execution time: 5.452029\n",
      "[val] Epoch: 11/251 Acc: 0.412575 Loss: 0.609609 Execution time: 1.501824\n",
      "[train] Epoch: 21/251 Acc: 0.305876 Loss: 0.878071 Execution time: 5.837291\n",
      "[val] Epoch: 21/251 Acc: 0.431525 Loss: 0.491744 Execution time: 1.577329\n",
      "[train] Epoch: 31/251 Acc: 0.310437 Loss: 0.824750 Execution time: 5.934913\n",
      "[val] Epoch: 31/251 Acc: 0.475452 Loss: 0.456061 Execution time: 1.623235\n",
      "[train] Epoch: 41/251 Acc: 0.345318 Loss: 0.799243 Execution time: 5.986010\n",
      "[val] Epoch: 41/251 Acc: 0.541774 Loss: 0.436969 Execution time: 1.616955\n",
      "[train] Epoch: 51/251 Acc: 0.344781 Loss: 0.786648 Execution time: 6.172334\n",
      "[val] Epoch: 51/251 Acc: 0.549526 Loss: 0.436317 Execution time: 1.680262\n",
      "[train] Epoch: 61/251 Acc: 0.378320 Loss: 0.773164 Execution time: 6.091772\n",
      "[val] Epoch: 61/251 Acc: 0.567614 Loss: 0.436064 Execution time: 1.638522\n",
      "[train] Epoch: 71/251 Acc: 0.396566 Loss: 0.759327 Execution time: 6.193539\n",
      "[val] Epoch: 71/251 Acc: 0.596899 Loss: 0.431889 Execution time: 1.641760\n",
      "[train] Epoch: 81/251 Acc: 0.417494 Loss: 0.748864 Execution time: 6.200411\n",
      "[val] Epoch: 81/251 Acc: 0.559862 Loss: 0.428819 Execution time: 1.657348\n",
      "[train] Epoch: 91/251 Acc: 0.429300 Loss: 0.743653 Execution time: 6.174615\n",
      "[val] Epoch: 91/251 Acc: 0.561585 Loss: 0.433330 Execution time: 1.659589\n",
      "[train] Epoch: 101/251 Acc: 0.450496 Loss: 0.732975 Execution time: 6.242721\n",
      "[val] Epoch: 101/251 Acc: 0.585702 Loss: 0.430673 Execution time: 1.809069\n",
      "[train] Epoch: 111/251 Acc: 0.453448 Loss: 0.730898 Execution time: 6.119711\n",
      "[val] Epoch: 111/251 Acc: 0.602929 Loss: 0.427026 Execution time: 1.687601\n",
      "[train] Epoch: 121/251 Acc: 0.474108 Loss: 0.725921 Execution time: 6.391497\n",
      "[val] Epoch: 121/251 Acc: 0.606374 Loss: 0.433110 Execution time: 1.746665\n",
      "[train] Epoch: 131/251 Acc: 0.469547 Loss: 0.717834 Execution time: 6.171740\n",
      "[val] Epoch: 131/251 Acc: 0.624462 Loss: 0.422954 Execution time: 1.760393\n",
      "[train] Epoch: 141/251 Acc: 0.482157 Loss: 0.705893 Execution time: 6.422949\n",
      "[val] Epoch: 141/251 Acc: 0.617571 Loss: 0.424069 Execution time: 1.669138\n",
      "[train] Epoch: 151/251 Acc: 0.496914 Loss: 0.711895 Execution time: 6.299215\n",
      "[val] Epoch: 151/251 Acc: 0.619294 Loss: 0.417692 Execution time: 1.754543\n",
      "[train] Epoch: 161/251 Acc: 0.504159 Loss: 0.705870 Execution time: 6.274662\n",
      "[val] Epoch: 161/251 Acc: 0.621016 Loss: 0.416590 Execution time: 1.700911\n",
      "[train] Epoch: 171/251 Acc: 0.503622 Loss: 0.702271 Execution time: 6.234191\n",
      "[val] Epoch: 171/251 Acc: 0.612403 Loss: 0.417107 Execution time: 1.773550\n",
      "[train] Epoch: 181/251 Acc: 0.503086 Loss: 0.702199 Execution time: 6.174884\n",
      "[val] Epoch: 181/251 Acc: 0.627046 Loss: 0.408868 Execution time: 1.700244\n",
      "[train] Epoch: 191/251 Acc: 0.539308 Loss: 0.699263 Execution time: 6.283080\n",
      "[val] Epoch: 191/251 Acc: 0.621878 Loss: 0.415722 Execution time: 1.642167\n",
      "[train] Epoch: 201/251 Acc: 0.540649 Loss: 0.693871 Execution time: 6.402154\n",
      "[val] Epoch: 201/251 Acc: 0.633936 Loss: 0.407241 Execution time: 1.707143\n",
      "[train] Epoch: 211/251 Acc: 0.531258 Loss: 0.691574 Execution time: 6.481525\n",
      "[val] Epoch: 211/251 Acc: 0.636520 Loss: 0.410611 Execution time: 1.712983\n",
      "[train] Epoch: 221/251 Acc: 0.543869 Loss: 0.686672 Execution time: 6.202893\n",
      "[val] Epoch: 221/251 Acc: 0.625323 Loss: 0.405891 Execution time: 1.630350\n",
      "[train] Epoch: 231/251 Acc: 0.540113 Loss: 0.686860 Execution time: 6.439115\n",
      "[val] Epoch: 231/251 Acc: 0.628768 Loss: 0.416704 Execution time: 1.802234\n",
      "[train] Epoch: 241/251 Acc: 0.558895 Loss: 0.682701 Execution time: 6.112708\n",
      "[val] Epoch: 241/251 Acc: 0.631352 Loss: 0.409653 Execution time: 1.720420\n",
      "[train] Epoch: 251/251 Acc: 0.565066 Loss: 0.678783 Execution time: 6.373732\n",
      "[val] Epoch: 251/251 Acc: 0.627046 Loss: 0.412654 Execution time: 1.652452\n",
      "======================== Domain 2 ==============================\n",
      "[train] Epoch: 1/251 Acc: 0.182051 Loss: 1.249401 Execution time: 1.223494\n",
      "[val] Epoch: 1/251 Acc: 0.555021 Loss: 0.472484 Execution time: 2.069274\n",
      "[train] Epoch: 11/251 Acc: 0.207692 Loss: 0.970667 Execution time: 1.284925\n",
      "[val] Epoch: 11/251 Acc: 0.539890 Loss: 0.427362 Execution time: 2.091057\n",
      "[train] Epoch: 21/251 Acc: 0.256410 Loss: 0.920663 Execution time: 1.396314\n",
      "[val] Epoch: 21/251 Acc: 0.508941 Loss: 0.430409 Execution time: 2.294362\n",
      "[train] Epoch: 31/251 Acc: 0.251282 Loss: 0.919067 Execution time: 1.223573\n",
      "[val] Epoch: 31/251 Acc: 0.474553 Loss: 0.435138 Execution time: 2.103469\n",
      "[train] Epoch: 41/251 Acc: 0.271795 Loss: 0.893347 Execution time: 1.257446\n",
      "[val] Epoch: 41/251 Acc: 0.488996 Loss: 0.437978 Execution time: 2.219038\n",
      "[train] Epoch: 51/251 Acc: 0.335897 Loss: 0.884950 Execution time: 1.194802\n",
      "[val] Epoch: 51/251 Acc: 0.500688 Loss: 0.436214 Execution time: 2.178469\n",
      "[train] Epoch: 61/251 Acc: 0.276923 Loss: 0.880135 Execution time: 1.308353\n",
      "[val] Epoch: 61/251 Acc: 0.490371 Loss: 0.439272 Execution time: 2.144594\n",
      "[train] Epoch: 71/251 Acc: 0.300000 Loss: 0.879200 Execution time: 1.389995\n",
      "[val] Epoch: 71/251 Acc: 0.507565 Loss: 0.439388 Execution time: 2.074594\n",
      "[train] Epoch: 81/251 Acc: 0.292308 Loss: 0.873807 Execution time: 1.254751\n",
      "[val] Epoch: 81/251 Acc: 0.486245 Loss: 0.437753 Execution time: 2.157048\n",
      "[train] Epoch: 91/251 Acc: 0.343590 Loss: 0.862338 Execution time: 1.344871\n",
      "[val] Epoch: 91/251 Acc: 0.476616 Loss: 0.443949 Execution time: 2.151217\n",
      "[train] Epoch: 101/251 Acc: 0.343590 Loss: 0.854252 Execution time: 1.277244\n",
      "[val] Epoch: 101/251 Acc: 0.484869 Loss: 0.440867 Execution time: 2.145203\n",
      "[train] Epoch: 111/251 Acc: 0.351282 Loss: 0.846059 Execution time: 1.276243\n",
      "[val] Epoch: 111/251 Acc: 0.488996 Loss: 0.443756 Execution time: 2.097938\n",
      "[train] Epoch: 121/251 Acc: 0.312821 Loss: 0.860492 Execution time: 1.288543\n",
      "[val] Epoch: 121/251 Acc: 0.478680 Loss: 0.444842 Execution time: 2.305611\n",
      "[train] Epoch: 131/251 Acc: 0.376923 Loss: 0.845507 Execution time: 1.223048\n",
      "[val] Epoch: 131/251 Acc: 0.491747 Loss: 0.442740 Execution time: 2.078933\n",
      "[train] Epoch: 141/251 Acc: 0.430769 Loss: 0.856396 Execution time: 1.230811\n",
      "[val] Epoch: 141/251 Acc: 0.491059 Loss: 0.443608 Execution time: 2.154172\n",
      "[train] Epoch: 151/251 Acc: 0.374359 Loss: 0.859557 Execution time: 1.192747\n",
      "[val] Epoch: 151/251 Acc: 0.492435 Loss: 0.440234 Execution time: 2.120605\n",
      "[train] Epoch: 161/251 Acc: 0.417949 Loss: 0.828418 Execution time: 1.342818\n",
      "[val] Epoch: 161/251 Acc: 0.485557 Loss: 0.442154 Execution time: 2.120387\n",
      "[train] Epoch: 171/251 Acc: 0.417949 Loss: 0.842377 Execution time: 1.337763\n",
      "[val] Epoch: 171/251 Acc: 0.493122 Loss: 0.443372 Execution time: 2.140503\n",
      "[train] Epoch: 181/251 Acc: 0.358974 Loss: 0.829663 Execution time: 1.281638\n",
      "[val] Epoch: 181/251 Acc: 0.497249 Loss: 0.443572 Execution time: 2.081077\n",
      "[train] Epoch: 191/251 Acc: 0.402564 Loss: 0.850560 Execution time: 1.175085\n",
      "[val] Epoch: 191/251 Acc: 0.495186 Loss: 0.441696 Execution time: 2.051933\n",
      "[train] Epoch: 201/251 Acc: 0.428205 Loss: 0.839599 Execution time: 1.276995\n",
      "[val] Epoch: 201/251 Acc: 0.493122 Loss: 0.442476 Execution time: 2.079312\n",
      "[train] Epoch: 211/251 Acc: 0.423077 Loss: 0.818031 Execution time: 1.386011\n",
      "[val] Epoch: 211/251 Acc: 0.493810 Loss: 0.441846 Execution time: 2.091028\n",
      "[train] Epoch: 221/251 Acc: 0.420513 Loss: 0.840345 Execution time: 1.323966\n",
      "[val] Epoch: 221/251 Acc: 0.496561 Loss: 0.441668 Execution time: 2.154329\n",
      "[train] Epoch: 231/251 Acc: 0.397436 Loss: 0.833101 Execution time: 1.295133\n",
      "[val] Epoch: 231/251 Acc: 0.493122 Loss: 0.444387 Execution time: 2.290020\n",
      "[train] Epoch: 241/251 Acc: 0.400000 Loss: 0.817541 Execution time: 1.294506\n",
      "[val] Epoch: 241/251 Acc: 0.500000 Loss: 0.443777 Execution time: 2.155104\n",
      "[train] Epoch: 251/251 Acc: 0.451282 Loss: 0.832043 Execution time: 1.276813\n",
      "[val] Epoch: 251/251 Acc: 0.497249 Loss: 0.447104 Execution time: 2.152739\n",
      "======================== Domain 1-2 FT =========================\n",
      "[train] Epoch: 1/251 Acc: 0.402473 Loss: 0.777786 Execution time: 1.142382\n",
      "[val] Epoch: 1/251 Acc: 0.464237 Loss: 0.450560 Execution time: 2.077708\n",
      "[train] Epoch: 11/251 Acc: 0.408967 Loss: 0.801271 Execution time: 1.215239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] Epoch: 11/251 Acc: 0.485557 Loss: 0.447473 Execution time: 2.170462\n",
      "[train] Epoch: 21/251 Acc: 0.437586 Loss: 0.782782 Execution time: 1.170640\n",
      "[val] Epoch: 21/251 Acc: 0.493810 Loss: 0.443743 Execution time: 2.080318\n",
      "[train] Epoch: 31/251 Acc: 0.411357 Loss: 0.772453 Execution time: 1.240849\n",
      "[val] Epoch: 31/251 Acc: 0.502751 Loss: 0.440645 Execution time: 2.175696\n",
      "[train] Epoch: 41/251 Acc: 0.417790 Loss: 0.779149 Execution time: 1.234529\n",
      "[val] Epoch: 41/251 Acc: 0.517882 Loss: 0.438503 Execution time: 2.118100\n",
      "[train] Epoch: 51/251 Acc: 0.408219 Loss: 0.764133 Execution time: 1.097041\n",
      "[val] Epoch: 51/251 Acc: 0.522696 Loss: 0.436063 Execution time: 2.082355\n",
      "[train] Epoch: 61/251 Acc: 0.447945 Loss: 0.770259 Execution time: 1.107146\n",
      "[val] Epoch: 61/251 Acc: 0.525447 Loss: 0.436758 Execution time: 2.233952\n",
      "[train] Epoch: 71/251 Acc: 0.444898 Loss: 0.766205 Execution time: 1.105726\n",
      "[val] Epoch: 71/251 Acc: 0.524072 Loss: 0.435311 Execution time: 2.156014\n",
      "[train] Epoch: 81/251 Acc: 0.426027 Loss: 0.767667 Execution time: 1.126769\n",
      "[val] Epoch: 81/251 Acc: 0.518569 Loss: 0.432987 Execution time: 2.140451\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed=27):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    seed_everything()\n",
    "    args = arguments()\n",
    "    print(args.feature_extractor)\n",
    "    data_const = SurgicalSceneConstants()\n",
    "    run_model(args, data_const)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def reliability_diagram_multi(conf_avg, acc_avg, legend=None, leg_idx=0, n_bins=10):\n",
    "    plt.figure(2)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(np.arange(0, 1.1, 1/n_bins))\n",
    "    #plt.title(title)\n",
    "    plt.plot(conf_avg[acc_avg>0],acc_avg[acc_avg>0], marker='.', label = legend)\n",
    "    plt.legend()\n",
    "    plt.savefig('ece_rel_multi.png',dpi=300)\n",
    "\n",
    "def calibration_metrics(logits_all, labels_all, model_name='Class-aware TS'):\n",
    "    uce = uceloss( logits_all.cpu(), labels_all.cpu())\n",
    "    logits = logits_all.detach().cpu().numpy()\n",
    "    labels = labels_all.detach().cpu().numpy()\n",
    "    ece, acc, conf, Bm = ece_eval(logits, labels, bg_cls=-1)\n",
    "    sce = get_sce(logits, labels)\n",
    "    tace = get_tace(logits, labels)\n",
    "    brier = get_brier(logits, labels)\n",
    "    print('%s:, ece:%0.4f, sce:%0.4f, tace:%0.4f, brier:%.4f, uce:%.4f' %(model_name, ece, sce, tace, brier, uce.item()) )\n",
    "    reliability_diagram_multi(conf, acc, legend=model_name)\n",
    "\n",
    "logits_all_sm = F.softmax(logits_all_cts, dim=1)\n",
    "calibration_metrics(logits_all_sm, labels_all_cts, model_name='Class-aware TS')\n",
    "\n",
    "logits_all_sm = F.softmax(logits_all_ts, dim=1)\n",
    "calibration_metrics(logits_all_sm, labels_all_ts, model_name='TS')\n",
    "\n",
    "logits_all_sm = F.softmax(logits_all, dim=1)\n",
    "calibration_metrics(logits_all_sm, labels_all, model_name='CE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
