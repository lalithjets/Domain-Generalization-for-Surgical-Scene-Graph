{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "configurations of the network\n",
    "    \n",
    "    readout: G_ER_L_S = [1024+300+16+300+1024,  1024, 117]\n",
    "\n",
    "    node_func: G_N_L_S = [1024+1024, 1024]\n",
    "    node_lang_func: G_N_L_S2 = [300+300+300]\n",
    "    \n",
    "    edge_func : G_E_L_S = [1024*2+16, 1024]\n",
    "    edge_lang_func: [300*2, 1024]\n",
    "    \n",
    "    attn: [1024, 1]\n",
    "    attn_lang: [1024, 1]\n",
    "'''\n",
    "\n",
    "class CONFIGURATION(object):\n",
    "    '''\n",
    "    Configuration arguments: feature type, layer, bias, batch normalization, dropout, multi-attn\n",
    "    \n",
    "    readout           : fc_size, activation, bias, bn, droupout\n",
    "    gnn_node          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_node_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    gnn_edge          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_edge_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    gnn_attn          : fc_size, activation, bias, bn, droupout\n",
    "    gnn_attn_for_lang : fc_size, activation, bias, bn, droupout\n",
    "    '''\n",
    "    def __init__(self, layer=1, bias=True, bn=False, dropout=0.2, multi_attn=False):\n",
    "        \n",
    "        # if multi_attn:\n",
    "        if True:\n",
    "            if layer==1:\n",
    "                feature_size = 512\n",
    "                # readout\n",
    "                self.G_ER_L_S = [feature_size+300+16+300+feature_size, feature_size, 13]\n",
    "                self.G_ER_A   = ['ReLU', 'Identity']\n",
    "                self.G_ER_B   = bias    #true\n",
    "                self.G_ER_BN  = bn      #false\n",
    "                self.G_ER_D   = dropout #0.3\n",
    "                # self.G_ER_GRU = feature_size\n",
    "\n",
    "                # # gnn node function\n",
    "                self.G_N_L_S = [feature_size+feature_size, feature_size]\n",
    "                self.G_N_A   = ['ReLU']\n",
    "                self.G_N_B   = bias #true\n",
    "                self.G_N_BN  = bn      #false\n",
    "                self.G_N_D   = dropout #0.3\n",
    "                # self.G_N_GRU = feature_size\n",
    "\n",
    "                # # gnn node function for language\n",
    "                self.G_N_L_S2 = [300+300, 300]\n",
    "                self.G_N_A2   = ['ReLU']\n",
    "                self.G_N_B2   = bias    #true\n",
    "                self.G_N_BN2  = bn      #false\n",
    "                self.G_N_D2   = dropout #0.3\n",
    "                # self.G_N_GRU2 = feature_size\n",
    "\n",
    "                # gnn edge function1\n",
    "                self.G_E_L_S           = [feature_size*2+16, feature_size]\n",
    "                self.G_E_A             = ['ReLU']\n",
    "                self.G_E_B             = bias     # true\n",
    "                self.G_E_BN            = bn       # false\n",
    "                self.G_E_D             = dropout  # 0.3\n",
    "                self.G_E_c_std         = 1.0\n",
    "                self.G_E_c_std_factor  = 0.9      # 0.985 (LOG), 0.95 (gau)\n",
    "                self.G_E_c_epoch       = 20\n",
    "                self.G_E_c_kernel_size = 3\n",
    "                self.G_E_c_filter      = 'gau' # 'gau', 'LOG'\n",
    "\n",
    "                # gnn edge function2 for language\n",
    "                self.G_E_L_S2 = [300*2, feature_size]\n",
    "                self.G_E_A2   = ['ReLU']\n",
    "                self.G_E_B2   = bias     #true\n",
    "                self.G_E_BN2  = bn       #false\n",
    "                self.G_E_D2   = dropout  #0.3\n",
    "\n",
    "                # gnn attention mechanism\n",
    "                self.G_A_L_S = [feature_size, 1]\n",
    "                self.G_A_A   = ['LeakyReLU']\n",
    "                self.G_A_B   = bias     #true\n",
    "                self.G_A_BN  = bn       #false\n",
    "                self.G_A_D   = dropout  #0.3\n",
    "\n",
    "                # gnn attention mechanism2 for language\n",
    "                self.G_A_L_S2 = [feature_size, 1]\n",
    "                self.G_A_A2   = ['LeakyReLU']\n",
    "                self.G_A_B2   = bias    #true\n",
    "                self.G_A_BN2  = bn      #false\n",
    "                self.G_A_D2   = dropout #0.3\n",
    "                    \n",
    "    def save_config(self):\n",
    "        model_config = {'graph_head':{}, 'graph_node':{}, 'graph_edge':{}, 'graph_attn':{}}\n",
    "        CONFIG=self.__dict__\n",
    "        for k, v in CONFIG.items():\n",
    "            if 'G_H' in k:\n",
    "                model_config['graph_head'][k]=v\n",
    "            elif 'G_N' in k:\n",
    "                model_config['graph_node'][k]=v\n",
    "            elif 'G_E' in k:\n",
    "                model_config['graph_edge'][k]=v\n",
    "            elif 'G_A' in k:\n",
    "                model_config['graph_attn'][k]=v\n",
    "            else:\n",
    "                model_config[k]=v\n",
    "        \n",
    "        return model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_gaussian_filter_1D(kernel_size=3, sigma=2, channels=3):\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    \n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "    mean = (kernel_size - 1)/2.\n",
    "    variance = sigma**2.\n",
    "    xy_grid = torch.sum((xy_grid[:kernel_size,:kernel_size,:] - mean)**2., dim=-1)\n",
    "\n",
    "    # Calculate the 1-dimensional gaussian kernel\n",
    "    gaussian_kernel = (1./((math.sqrt(2.*math.pi)*sigma))) * \\\n",
    "                        torch.exp(-1* (xy_grid[int(kernel_size/2)]) / (2*variance))\n",
    "\n",
    "    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size)\n",
    "    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1)\n",
    "\n",
    "    padding = 1 if kernel_size==3 else 2 if kernel_size == 5 else 0\n",
    "    gaussian_filter = nn.Conv1d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels,\n",
    "                                bias=False, padding=padding)\n",
    "    gaussian_filter.weight.data = gaussian_kernel\n",
    "    gaussian_filter.weight.requires_grad = False \n",
    "    return gaussian_filter\n",
    "\n",
    "def get_laplaceOfGaussian_filter_1D(kernel_size=3, sigma=2, channels=3):\n",
    "    \n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "    mean = (kernel_size - 1)/2.\n",
    "\n",
    "    used_sigma = sigma\n",
    "    # Calculate the 2-dimensional gaussian kernel which is\n",
    "    log_kernel = (-1./(math.pi*(used_sigma**4))) \\\n",
    "                  * (1-(torch.sum((xy_grid[int(kernel_size/2)] - mean)**2., dim=-1) / (2*(used_sigma**2)))) \\\n",
    "                  * torch.exp(-torch.sum((xy_grid[int(kernel_size/2)] - mean)**2., dim=-1) / (2*(used_sigma**2)))\n",
    "    \n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    log_kernel = log_kernel / torch.sum(log_kernel)\n",
    "    log_kernel = log_kernel.view(1, 1, kernel_size)\n",
    "    log_kernel = log_kernel.repeat(channels, 1, 1)\n",
    "\n",
    "    padding = 1 if kernel_size==3 else 2 if kernel_size == 5 else 0\n",
    "    log_filter = nn.Conv1d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels,\n",
    "                                bias=False, padding=padding)\n",
    "    log_filter.weight.data = log_kernel\n",
    "    log_filter.weight.requires_grad = False\n",
    "    return log_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primary activation and MLP layer\n",
    "acivation:\n",
    "    Identity\n",
    "    ReLU\n",
    "    LeakyReLU\n",
    "MLP:\n",
    "    init: layer size, activation, bias, use_BN, dropout_probability\n",
    "    forward: x\n",
    "'''\n",
    "\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    '''\n",
    "    Identity class activation layer\n",
    "    x = x\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Identity,self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def get_activation(name):\n",
    "    '''\n",
    "    get_activation sub-function\n",
    "    argument: activatoin name (eg. ReLU, Identity, LeakyReLU)\n",
    "    '''\n",
    "    if name=='ReLU': return nn.ReLU(inplace=True)\n",
    "    elif name=='Identity': return Identity()\n",
    "    elif name=='LeakyReLU': return nn.LeakyReLU(0.2,inplace=True)\n",
    "    else: assert(False), 'Not Implemented'\n",
    "    #elif name=='Tanh': return nn.Tanh()\n",
    "    #elif name=='Sigmoid': return nn.Sigmoid()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Args:\n",
    "        layer_sizes: a list, [1024,1024,...]\n",
    "        activation: a list, ['ReLU', 'Tanh',...]\n",
    "        bias : bool\n",
    "        use_bn: bool\n",
    "        drop_prob: default is None, use drop out layer or not\n",
    "    '''\n",
    "    def __init__(self, layer_sizes, activation, bias=True, use_bn=False, drop_prob=None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.bn = use_bn\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=bias)\n",
    "            activate = get_activation(activation[i])\n",
    "            block = nn.Sequential(OrderedDict([(f'L{i}', layer), ]))\n",
    "            \n",
    "            # !NOTE:# Actually, it is inappropriate to use batch-normalization here\n",
    "            if use_bn:                                  \n",
    "                bn = nn.BatchNorm1d(layer_sizes[i+1])\n",
    "                block.add_module(f'B{i}', bn)\n",
    "            \n",
    "            # batch normalization is put before activation function \n",
    "            block.add_module(f'A{i}', activate)\n",
    "\n",
    "            # dropout probablility\n",
    "            if drop_prob:\n",
    "                block.add_module(f'D{i}', nn.Dropout(drop_prob))\n",
    "            \n",
    "            self.layers.append(block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # !NOTE: sometime the shape of x will be [1,N], and we cannot use batch-normailzation in that situation\n",
    "            if self.bn and x.shape[0]==1:\n",
    "                x = layer[0](x)\n",
    "                x = layer[:-1](x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "H_H_EdgeApplyModule\n",
    "    init    : config, multi_attn \n",
    "    forward : edge\n",
    "    \n",
    "H_NodeApplyModule\n",
    "    init    : config\n",
    "    forward : node\n",
    "    \n",
    "E_AttentionModule1\n",
    "    init    : config\n",
    "    forward : edge\n",
    "    \n",
    "GNN\n",
    "    init    : config, multi_attn, diff_edge\n",
    "    forward : g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_features\n",
    "    \n",
    "GRNN\n",
    "    init    : config, multi_attn, diff_edge\n",
    "    forward : b_graph, b_h_node_list, b_o_node_list, b_h_h_e_list, b_o_o_e_list, b_h_o_e_list, features, spatial_features, word2vec, valid, pop_features, initial_features\n",
    "'''\n",
    "\n",
    "import ipdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class H_H_EdgeApplyModule(nn.Module): #human to human edge\n",
    "    '''\n",
    "        init    : config, multi_attn \n",
    "        forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, use_cbs = False):\n",
    "        super(H_H_EdgeApplyModule, self).__init__()\n",
    "        self.use_cbs = use_cbs\n",
    "        if use_cbs:\n",
    "            self.init_std = CONFIG.G_E_c_std \n",
    "            self.cbs_std = CONFIG.G_E_c_std\n",
    "            self.cbs_std_factor = CONFIG.G_E_c_std_factor\n",
    "            self.cbs_epoch = CONFIG.G_E_c_epoch\n",
    "            self.cbs_kernel_size = CONFIG.G_E_c_kernel_size\n",
    "            self.cbs_filter = CONFIG.G_E_c_filter\n",
    "        \n",
    "        self.edge_fc = MLP(CONFIG.G_E_L_S, CONFIG.G_E_A, CONFIG.G_E_B, CONFIG.G_E_BN, CONFIG.G_E_D)\n",
    "        self.edge_fc_lang = MLP(CONFIG.G_E_L_S2, CONFIG.G_E_A2, CONFIG.G_E_B2, CONFIG.G_E_BN2, CONFIG.G_E_D2)\n",
    "    \n",
    "    def forward(self, edge):\n",
    "        feat = torch.cat([edge.src['n_f'], edge.data['s_f'], edge.dst['n_f']], dim=1)\n",
    "        feat_lang = torch.cat([edge.src['word2vec'], edge.dst['word2vec']], dim=1)\n",
    "        if self.use_cbs:\n",
    "            feat = self.kernel1(feat[:,None,:])\n",
    "            feat = torch.squeeze(feat, 1)\n",
    "        e_feat = self.edge_fc(feat)\n",
    "        e_feat_lang = self.edge_fc_lang(feat_lang)\n",
    "  \n",
    "        return {'e_f': e_feat, 'e_f_lang': e_feat_lang}\n",
    "\n",
    "    def get_new_kernels(self, epoch_count):\n",
    "        if self.use_cbs:\n",
    "            if epoch_count == 0:\n",
    "                self.cbs_std = self.init_std\n",
    "                \n",
    "            if epoch_count % self.cbs_epoch == 0 and epoch_count is not 0:\n",
    "                self.cbs_std *= self.cbs_std_factor\n",
    "            \n",
    "            if (self.cbs_filter == 'gau'): \n",
    "                self.kernel1 = get_gaussian_filter_1D(kernel_size=self.cbs_kernel_size, sigma= self.cbs_std, channels= 1)\n",
    "            elif (self.cbs_filter == 'LOG'): \n",
    "                self.kernel1 = get_laplaceOfGaussian_filter_1D(kernel_size=self.cbs_kernel_size, sigma= self.cbs_std, channels= 1)\n",
    "\n",
    "class H_NodeApplyModule(nn.Module): #human node\n",
    "    '''\n",
    "        init    : config\n",
    "        forward : node\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(H_NodeApplyModule, self).__init__()\n",
    "        self.node_fc = MLP(CONFIG.G_N_L_S, CONFIG.G_N_A, CONFIG.G_N_B, CONFIG.G_N_BN, CONFIG.G_N_D)\n",
    "        self.node_fc_lang = MLP(CONFIG.G_N_L_S2, CONFIG.G_N_A2, CONFIG.G_N_B2, CONFIG.G_N_BN2, CONFIG.G_N_D2)\n",
    "    \n",
    "    def forward(self, node):\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        feat = torch.cat([node.data['n_f'], node.data['z_f']], dim=1)\n",
    "        feat_lang = torch.cat([node.data['word2vec'], node.data['z_f_lang']], dim=1)\n",
    "        n_feat = self.node_fc(feat)\n",
    "        n_feat_lang = self.node_fc_lang(feat_lang)\n",
    "\n",
    "        return {'new_n_f': n_feat, 'new_n_f_lang': n_feat_lang}\n",
    "\n",
    "class E_AttentionModule1(nn.Module): #edge attention\n",
    "    '''\n",
    "        init    : config\n",
    "        forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(E_AttentionModule1, self).__init__()\n",
    "        self.attn_fc = MLP(CONFIG.G_A_L_S, CONFIG.G_A_A, CONFIG.G_A_B, CONFIG.G_A_BN, CONFIG.G_A_D)\n",
    "        self.attn_fc_lang = MLP(CONFIG.G_A_L_S2, CONFIG.G_A_A2, CONFIG.G_A_B2, CONFIG.G_A_BN2, CONFIG.G_A_D2)\n",
    "\n",
    "    def forward(self, edge):\n",
    "        a_feat = self.attn_fc(edge.data['e_f'])\n",
    "        a_feat_lang = self.attn_fc_lang(edge.data['e_f_lang'])\n",
    "        return {'a_feat': a_feat, 'a_feat_lang': a_feat_lang}\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    '''\n",
    "        init    : config, multi_attn, diff_edge\n",
    "        forward : g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_features\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, diff_edge=True, use_cbs = False):\n",
    "        super(GNN, self).__init__()\n",
    "        self.diff_edge = diff_edge # false\n",
    "        self.apply_h_h_edge = H_H_EdgeApplyModule(CONFIG, multi_attn, use_cbs)\n",
    "        self.apply_edge_attn1 = E_AttentionModule1(CONFIG)  \n",
    "        self.apply_h_node = H_NodeApplyModule(CONFIG)\n",
    "\n",
    "    def _message_func(self, edges):\n",
    "        return {'nei_n_f': edges.src['n_f'], 'nei_n_w': edges.src['word2vec'], 'e_f': edges.data['e_f'], 'e_f_lang': edges.data['e_f_lang'], 'a_feat': edges.data['a_feat'], 'a_feat_lang': edges.data['a_feat_lang']}\n",
    "\n",
    "    def _reduce_func(self, nodes):\n",
    "        alpha = F.softmax(nodes.mailbox['a_feat'], dim=1)\n",
    "        alpha_lang = F.softmax(nodes.mailbox['a_feat_lang'], dim=1)\n",
    "\n",
    "        z_raw_f = nodes.mailbox['nei_n_f']+nodes.mailbox['e_f']\n",
    "        z_f = torch.sum( alpha * z_raw_f, dim=1)\n",
    "\n",
    "        z_raw_f_lang = nodes.mailbox['nei_n_w']\n",
    "        z_f_lang = torch.sum(alpha_lang * z_raw_f_lang, dim=1)\n",
    "         \n",
    "        # we cannot return 'alpha' for the different dimension \n",
    "        if self.training or validation: return {'z_f': z_f, 'z_f_lang': z_f_lang}\n",
    "        else: return {'z_f': z_f, 'z_f_lang': z_f_lang, 'alpha': alpha, 'alpha_lang': alpha_lang}\n",
    "\n",
    "    def forward(self, g, h_node, o_node, h_h_e_list, o_o_e_list, h_o_e_list, pop_feat=False):\n",
    "        \n",
    "        g.apply_edges(self.apply_h_h_edge, g.edges())\n",
    "        g.apply_edges(self.apply_edge_attn1)\n",
    "        g.update_all(self._message_func, self._reduce_func)\n",
    "        g.apply_nodes(self.apply_h_node, h_node+o_node)\n",
    "\n",
    "        # !NOTE:PAY ATTENTION WHEN ADDING MORE FEATURE\n",
    "        g.ndata.pop('n_f')\n",
    "        g.ndata.pop('word2vec')\n",
    "\n",
    "        g.ndata.pop('z_f')\n",
    "        g.edata.pop('e_f')\n",
    "        g.edata.pop('a_feat')\n",
    "\n",
    "        g.ndata.pop('z_f_lang')\n",
    "        g.edata.pop('e_f_lang')\n",
    "        g.edata.pop('a_feat_lang')\n",
    "\n",
    "class GRNN(nn.Module):\n",
    "    '''\n",
    "    init: \n",
    "        config, multi_attn, diff_edge\n",
    "    forward: \n",
    "        batch_graph, batch_h_node_list, batch_obj_node_list,\n",
    "        batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list,\n",
    "        features, spatial_features, word2vec,\n",
    "        valid, pop_features, initial_features\n",
    "    '''\n",
    "    def __init__(self, CONFIG, multi_attn=False, diff_edge=True, use_cbs = False):\n",
    "        super(GRNN, self).__init__()\n",
    "        self.multi_attn = multi_attn #false\n",
    "        self.gnn = GNN(CONFIG, multi_attn, diff_edge, use_cbs)\n",
    "\n",
    "    def forward(self, batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec, valid=False, pop_feat=False, initial_feat=False):\n",
    "        \n",
    "        # !NOTE: if node_num==1, there will be something wrong to forward the attention mechanism\n",
    "        global validation \n",
    "        validation = valid\n",
    "\n",
    "        # initialize the graph with some datas\n",
    "        batch_graph.ndata['n_f'] = feat           # node: features \n",
    "        batch_graph.ndata['word2vec'] = word2vec  # node: words\n",
    "        batch_graph.edata['s_f'] = spatial_feat   # edge: spatial features\n",
    "\n",
    "        try:\n",
    "            self.gnn(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            ipdb.set_trace()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Predictor \n",
    "    init    : config\n",
    "    forward : edge\n",
    "\n",
    "AGRNN\n",
    "    init    : bias, bn, dropout, multi_attn, layer, diff_edge\n",
    "    forward : node_num, feat, spatial_feat, word2vec, roi_label, validation, choose_nodes, remove_nodes\n",
    "'''\n",
    "\n",
    "import dgl\n",
    "import ipdb\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchvision\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    '''\n",
    "    init    : config\n",
    "    forward : edge\n",
    "    '''\n",
    "    def __init__(self, CONFIG):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.classifier = MLP(CONFIG.G_ER_L_S, CONFIG.G_ER_A, CONFIG.G_ER_B, CONFIG.G_ER_BN, CONFIG.G_ER_D)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, edge):\n",
    "        feat = torch.cat([edge.dst['new_n_f'], edge.dst['new_n_f_lang'], edge.data['s_f'], edge.src['new_n_f_lang'], edge.src['new_n_f']], dim=1)\n",
    "        pred = self.classifier(feat)\n",
    "        # if the criterion is BCELoss, you need to uncomment the following code\n",
    "        # output = self.sigmoid(output)\n",
    "        return {'pred': pred}\n",
    "\n",
    "class AGRNN(nn.Module):\n",
    "    '''\n",
    "    init    : \n",
    "        feature_type, bias, bn, dropout, multi_attn, layer, diff_edge\n",
    "        \n",
    "    forward : \n",
    "        node_num, features, spatial_features, word2vec, roi_label,\n",
    "        validation, choose_nodes, remove_nodes\n",
    "    '''\n",
    "    def __init__(self, bias=True, bn=True, dropout=None, multi_attn=False, layer=1, diff_edge=True, use_cbs = False):\n",
    "        super(AGRNN, self).__init__()\n",
    " \n",
    "        self.multi_attn = multi_attn # false\n",
    "        self.layer = layer           # 1 layer\n",
    "        self.diff_edge = diff_edge   # false\n",
    "        \n",
    "        self.CONFIG1 = CONFIGURATION(layer=1, bias=bias, bn=bn, dropout=dropout, multi_attn=multi_attn)\n",
    "\n",
    "        self.grnn1 = GRNN(self.CONFIG1, multi_attn=multi_attn, diff_edge=diff_edge, use_cbs = use_cbs)\n",
    "        self.edge_readout = Predictor(self.CONFIG1)\n",
    "        \n",
    "    def _collect_edge(self, node_num, roi_label, node_space, diff_edge):\n",
    "        '''\n",
    "        arguments: node_num, roi_label, node_space, diff_edge\n",
    "        '''\n",
    "        \n",
    "        # get human nodes && object nodes\n",
    "        h_node_list = np.where(roi_label == 0)[0]\n",
    "        obj_node_list = np.where(roi_label != 0)[0]\n",
    "        edge_list = []\n",
    "        \n",
    "        h_h_e_list = []\n",
    "        o_o_e_list = []\n",
    "        h_o_e_list = []\n",
    "        \n",
    "        readout_edge_list = []\n",
    "        readout_h_h_e_list = []\n",
    "        readout_h_o_e_list = []\n",
    "        \n",
    "        # get all edge in the fully-connected graph, edge_list, For node_num = 2, edge_list = [(0, 1), (1, 0)]\n",
    "        for src in range(node_num):\n",
    "            for dst in range(node_num):\n",
    "                if src == dst:\n",
    "                    continue\n",
    "                else:\n",
    "                    edge_list.append((src, dst))\n",
    "        \n",
    "        # readout_edge_list, get corresponding readout edge in the graph\n",
    "        src_box_list = np.arange(roi_label.shape[0])\n",
    "        for dst in h_node_list:\n",
    "            # if dst == roi_label.shape[0]-1:\n",
    "            #    continue\n",
    "            # src_box_list = src_box_list[1:]\n",
    "            for src in src_box_list:\n",
    "                if src not in h_node_list:\n",
    "                    readout_edge_list.append((src, dst))\n",
    "        \n",
    "        # readout h_h_e_list, get corresponding readout h_h edges && h_o edges\n",
    "        temp_h_node_list = h_node_list[:]\n",
    "        for dst in h_node_list:\n",
    "            if dst == h_node_list.shape[0]-1:\n",
    "                continue\n",
    "            temp_h_node_list = temp_h_node_list[1:]\n",
    "            for src in temp_h_node_list:\n",
    "                if src == dst: continue\n",
    "                readout_h_h_e_list.append((src, dst))\n",
    "\n",
    "        # readout h_o_e_list\n",
    "        readout_h_o_e_list = [x for x in readout_edge_list if x not in readout_h_h_e_list]\n",
    "\n",
    "        # add node space to match the batch graph\n",
    "        h_node_list = (np.array(h_node_list)+node_space).tolist()\n",
    "        obj_node_list = (np.array(obj_node_list)+node_space).tolist()\n",
    "        \n",
    "        h_h_e_list = (np.array(h_h_e_list)+node_space).tolist() #empty no diff_edge\n",
    "        o_o_e_list = (np.array(o_o_e_list)+node_space).tolist() #empty no diff_edge\n",
    "        h_o_e_list = (np.array(h_o_e_list)+node_space).tolist() #empty no diff_edge\n",
    "\n",
    "        readout_h_h_e_list = (np.array(readout_h_h_e_list)+node_space).tolist()\n",
    "        readout_h_o_e_list = (np.array(readout_h_o_e_list)+node_space).tolist()   \n",
    "        readout_edge_list = (np.array(readout_edge_list)+node_space).tolist()\n",
    "\n",
    "        return edge_list, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list\n",
    "    \n",
    "    def _build_graph(self, node_num, roi_label, node_space, diff_edge):\n",
    "        '''\n",
    "        Declare graph, add_nodes, collect edges, add_edges\n",
    "        '''\n",
    "        graph = dgl.DGLGraph()\n",
    "        graph.add_nodes(node_num)\n",
    "\n",
    "        edge_list, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list = self._collect_edge(node_num, roi_label, node_space, diff_edge)\n",
    "        src, dst = tuple(zip(*edge_list))\n",
    "        graph.add_edges(src, dst)   # make the graph bi-directional\n",
    "\n",
    "        return graph, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list\n",
    "\n",
    "    def forward(self, node_num=None, feat=None, spatial_feat=None, word2vec=None, roi_label=None, validation=False, choose_nodes=None, remove_nodes=None):\n",
    "        \n",
    "        batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, batch_readout_edge_list, batch_readout_h_h_e_list, batch_readout_h_o_e_list = [], [], [], [], [], [], [], [], []\n",
    "        node_num_cum = np.cumsum(node_num) # !IMPORTANT\n",
    "        \n",
    "        for i in range(len(node_num)):\n",
    "            # set node space\n",
    "            node_space = 0\n",
    "            if i != 0:\n",
    "                node_space = node_num_cum[i-1]\n",
    "            graph, h_node_list, obj_node_list, h_h_e_list, o_o_e_list, h_o_e_list, readout_edge_list, readout_h_h_e_list, readout_h_o_e_list = self._build_graph(node_num[i], roi_label[i], node_space, diff_edge=self.diff_edge)\n",
    "            \n",
    "            # updata batch\n",
    "            batch_graph.append(graph)\n",
    "            batch_h_node_list += h_node_list\n",
    "            batch_obj_node_list += obj_node_list\n",
    "            \n",
    "            batch_h_h_e_list += h_h_e_list\n",
    "            batch_o_o_e_list += o_o_e_list\n",
    "            batch_h_o_e_list += h_o_e_list\n",
    "            \n",
    "            batch_readout_edge_list += readout_edge_list\n",
    "            batch_readout_h_h_e_list += readout_h_h_e_list\n",
    "            batch_readout_h_o_e_list += readout_h_o_e_list\n",
    "        \n",
    "        batch_graph = dgl.batch(batch_graph)\n",
    "        \n",
    "        # GRNN\n",
    "        self.grnn1(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec, validation, initial_feat=True)\n",
    "        batch_graph.apply_edges(self.edge_readout, tuple(zip(*(batch_readout_h_o_e_list+batch_readout_h_h_e_list))))\n",
    "        \n",
    "        if self.training or validation:\n",
    "            # !NOTE: cannot use \"batch_readout_h_o_e_list+batch_readout_h_h_e_list\" because of the wrong order\n",
    "            return batch_graph.edges[tuple(zip(*batch_readout_edge_list))].data['pred']\n",
    "        else:\n",
    "            return batch_graph.edges[tuple(zip(*batch_readout_edge_list))].data['pred'], \\\n",
    "                   batch_graph.nodes[batch_h_node_list].data['alpha'], \\\n",
    "                   batch_graph.nodes[batch_h_node_list].data['alpha_lang'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils.io as io\n",
    "\n",
    "class SurgicalSceneConstants():\n",
    "    def __init__( self):\n",
    "        self.instrument_classes = ('kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                      'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                      'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "        \n",
    "        #self.instrument_classes = ( 'kidney', 'bipolar_forceps', 'fenestrated_bipolar', \n",
    "        #                             'prograsp_forceps', 'large_needle_driver', 'vessel_sealer',\n",
    "        #                             'grasping_retractor', 'monopolar_curved_scissors', \n",
    "        #                             'ultrasound_probe', 'suction', 'clip_applier', 'stapler')\n",
    "        \n",
    "        self.action_classes = ( 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                                'Tool_Manipulation', 'Cutting', 'Cauterization', \n",
    "                                'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', \n",
    "                                'Ultrasound_Sensing')\n",
    "        self.xml_data_dir = 'datasets/instruments18/seq_'\n",
    "        self.word2vec_loc = 'datasets/surgicalscene_word2vec.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "    \n",
    "class SurgicalSceneDataset(Dataset):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, seq_set, data_dir, img_dir, dset, dataconst, feature_extractor, reduce_size = False):\n",
    "        \n",
    "        \n",
    "        self.data_size = 143\n",
    "        self.dataconst = dataconst\n",
    "        self.img_dir = img_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.reduce_size = reduce_size\n",
    "        \n",
    "        self.xml_dir_list = []\n",
    "        self.dset = []\n",
    "        \n",
    "        for domain in range(len(seq_set)):\n",
    "            domain_dir_list = []\n",
    "            for i in seq_set[domain]:\n",
    "                xml_dir_temp = data_dir[domain] + str(i) + '/xml/'\n",
    "                domain_dir_list = domain_dir_list + glob(xml_dir_temp + '/*.xml')\n",
    "            if self.reduce_size:\n",
    "                indices = np.random.permutation(len(domain_dir_list))\n",
    "                domain_dir_list = [domain_dir_list[j] for j in indices[0:self.data_size]]\n",
    "            for file in domain_dir_list: \n",
    "                self.xml_dir_list.append(file)\n",
    "                self.dset.append(dset[domain])\n",
    "        self.word2vec = h5py.File('datasets/surgicalscene_word2vec.hdf5', 'r')\n",
    "    \n",
    "    # word2vec\n",
    "    def _get_word2vec(self,node_ids, sgh = 0):\n",
    "        word2vec = np.empty((0,300))\n",
    "        for node_id in node_ids:\n",
    "            if sgh == 1 and node_id == 0:\n",
    "                vec = self.word2vec['tissue']\n",
    "            else:\n",
    "                vec = self.word2vec[self.dataconst.instrument_classes[node_id]]\n",
    "            word2vec = np.vstack((word2vec, vec))\n",
    "        return word2vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xml_dir_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        file_name = os.path.splitext(os.path.basename(self.xml_dir_list[idx]))[0]\n",
    "        file_root = os.path.dirname(os.path.dirname(self.xml_dir_list[idx]))\n",
    "        if len(self.img_dir) == 1:\n",
    "            _img_loc = os.path.join(file_root+self.img_dir[0]+ file_name + '.png')\n",
    "        else:\n",
    "            _img_loc = os.path.join(file_root+self.img_dir[self.dset[idx]]+ file_name + '.png')\n",
    "        frame_data = h5py.File(os.path.join(file_root+'/vsgat/'+self.feature_extractor+'/'+ file_name + '_features.hdf5'), 'r')    \n",
    "        data = {}\n",
    "        data['img_name'] = frame_data['img_name'].value[:] + '.jpg'\n",
    "        data['img_loc'] = _img_loc\n",
    "        \n",
    "        data['node_num'] = frame_data['node_num'].value\n",
    "        data['roi_labels'] = frame_data['classes'][:]\n",
    "        data['det_boxes'] = frame_data['boxes'][:]\n",
    "        \n",
    "        \n",
    "        data['edge_labels'] = frame_data['edge_labels'][:]\n",
    "        data['edge_num'] = data['edge_labels'].shape[0]\n",
    "        \n",
    "        data['features'] = frame_data['node_features'][:]\n",
    "        data['spatial_feat'] = frame_data['spatial_features'][:]\n",
    "        \n",
    "        \n",
    "        data['word2vec'] = self._get_word2vec(data['roi_labels'], self.dset[idx])\n",
    "        return data\n",
    "\n",
    "# for DatasetLoader\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "        Default collate_fn(): https://github.com/pytorch/pytorch/blob/1d53d0756668ce641e4f109200d9c65b003d05fa/torch/utils/data/_utils/collate.py#L43\n",
    "    '''\n",
    "    batch_data = {}\n",
    "    batch_data['img_name'] = []\n",
    "    batch_data['img_loc'] = []\n",
    "    batch_data['node_num'] = []\n",
    "    batch_data['roi_labels'] = []\n",
    "    batch_data['det_boxes'] = []\n",
    "    batch_data['edge_labels'] = []\n",
    "    batch_data['edge_num'] = []\n",
    "    batch_data['features'] = []\n",
    "    batch_data['spatial_feat'] = []\n",
    "    batch_data['word2vec'] = []\n",
    "    \n",
    "    for data in batch:\n",
    "        batch_data['img_name'].append(data['img_name'])\n",
    "        batch_data['img_loc'].append(data['img_loc'])\n",
    "        batch_data['node_num'].append(data['node_num'])\n",
    "        batch_data['roi_labels'].append(data['roi_labels'])\n",
    "        batch_data['det_boxes'].append(data['det_boxes'])\n",
    "        batch_data['edge_labels'].append(data['edge_labels'])\n",
    "        batch_data['edge_num'].append(data['edge_num'])\n",
    "        batch_data['features'].append(data['features'])\n",
    "        batch_data['spatial_feat'].append(data['spatial_feat'])\n",
    "        batch_data['word2vec'].append(data['word2vec'])\n",
    "        \n",
    "    batch_data['edge_labels'] = torch.FloatTensor(np.concatenate(batch_data['edge_labels'], axis=0))\n",
    "    batch_data['features'] = torch.FloatTensor(np.concatenate(batch_data['features'], axis=0))\n",
    "    batch_data['spatial_feat'] = torch.FloatTensor(np.concatenate(batch_data['spatial_feat'], axis=0))\n",
    "    batch_data['word2vec'] = torch.FloatTensor(np.concatenate(batch_data['word2vec'], axis=0))\n",
    "    \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch as t\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plot\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def vis_img(img, node_classes, bboxs,  det_action, score_thresh = 0.7):\n",
    "    \n",
    "    Drawer = ImageDraw.Draw(img)\n",
    "    line_width = 3\n",
    "    outline = '#FF0000'\n",
    "    font = ImageFont.truetype(font='/usr/share/fonts/truetype/freefont/FreeMono.ttf', size=25)\n",
    "    \n",
    "    im_w,im_h = img.size\n",
    "    node_num = len(node_classes)\n",
    "    edge_num = len(det_action)\n",
    "    tissue_num = len(np.where(node_classes == 1)[0])\n",
    "    \n",
    "    for node in range(node_num):\n",
    "        \n",
    "        r_color = random.choice(np.arange(256))\n",
    "        g_color = random.choice(np.arange(256))\n",
    "        b_color = random.choice(np.arange(256))\n",
    "        \n",
    "        text = data_const.instrument_classes[node_classes[node]]\n",
    "        h, w = font.getsize(text)\n",
    "        Drawer.rectangle(list(bboxs[node]), outline=outline, width=line_width)\n",
    "        Drawer.text(xy=(bboxs[node][0], bboxs[node][1]-w-1), text=text, font=font, fill=(r_color,g_color,b_color))\n",
    "  \n",
    "    edge_idx = 0\n",
    "    \n",
    "    for tissue in range(tissue_num):\n",
    "        for instrument in range(tissue+1, node_num):\n",
    "            \n",
    "            #action_idx = np.where(det_action[edge_idx] > score_thresh)[0]\n",
    "            action_idx = np.argmax(det_action[edge_idx])\n",
    "#             print('det_action', det_action[edge_idx])\n",
    "#             print('action_idx',action_idx)\n",
    "            \n",
    "            text = data_const.action_classes[action_idx]\n",
    "            r_color = random.choice(np.arange(256))\n",
    "            g_color = random.choice(np.arange(256))\n",
    "            b_color = random.choice(np.arange(256))\n",
    "        \n",
    "            x1,y1,x2,y2 = bboxs[tissue]\n",
    "            x1_,y1_,x2_,y2_ = bboxs[instrument]\n",
    "            \n",
    "            c0 = int(0.5*x1)+int(0.5*x2)\n",
    "            c0 = max(0,min(c0,im_w-1))\n",
    "            r0 = int(0.5*y1)+int(0.5*y2)\n",
    "            r0 = max(0,min(r0,im_h-1))\n",
    "            c1 = int(0.5*x1_)+int(0.5*x2_)\n",
    "            c1 = max(0,min(c1,im_w-1))\n",
    "            r1 = int(0.5*y1_)+int(0.5*y2_)\n",
    "            r1 = max(0,min(r1,im_h-1))\n",
    "            Drawer.line(((c0,r0),(c1,r1)), fill=(r_color,g_color,b_color), width=3)\n",
    "            Drawer.text(xy=(c1, r1), text=text, font=font, fill=(r_color,g_color,b_color))\n",
    "\n",
    "            edge_idx +=1\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn.metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def ece_eval(preds, targets, n_bins=10, bg_cls = 0):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    confidences, predictions = np.max(preds,1), np.argmax(preds,1)\n",
    "    confidences, predictions = confidences[targets>bg_cls], predictions[targets>bg_cls]\n",
    "    accuracies = (predictions == targets[targets>bg_cls]) \n",
    "    Bm, acc, conf = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)\n",
    "    ece = 0.0\n",
    "    bin_idx = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)\n",
    "        #in_bin = in_bin[targets>backgound_class]\n",
    "        bin_size = np.sum(in_bin)\n",
    "        \n",
    "        Bm[bin_idx] = bin_size\n",
    "        if bin_size > 0:  \n",
    "            accuracy_in_bin = np.sum(accuracies[in_bin])\n",
    "            acc[bin_idx] = accuracy_in_bin / Bm[bin_idx]\n",
    "            confidence_in_bin = np.sum(confidences[in_bin])\n",
    "            conf[bin_idx] = confidence_in_bin / Bm[bin_idx]\n",
    "        bin_idx += 1\n",
    "        \n",
    "    ece_all = Bm * np.abs((acc - conf))/ Bm.sum()\n",
    "    ece = ece_all.sum() \n",
    "    return ece, acc, conf, Bm\n",
    "\n",
    "def get_sce(preds, targets, n_bins=10, **args):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    n_objects, n_classes = preds.shape\n",
    "    res = 0.0\n",
    "    for cur_class in range(n_classes):\n",
    "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "            cur_class_conf = preds[:, cur_class]\n",
    "            in_bin = np.logical_and(cur_class_conf > bin_lower, cur_class_conf <= bin_upper)\n",
    "\n",
    "            # cur_class_acc is ground truth probability of chosen class being the correct one inside the bin.\n",
    "            # NOT fraction of correct predictions in the bin\n",
    "            # because it is compared with predicted probability\n",
    "            bin_acc = (targets[in_bin] == cur_class)\n",
    "            \n",
    "            bin_conf = cur_class_conf[in_bin]\n",
    "\n",
    "            bin_size = np.sum(in_bin)\n",
    "            \n",
    "            if bin_size > 0:\n",
    "                avg_confidence_in_bin = np.mean(bin_conf)\n",
    "                avg_accuracy_in_bin = np.mean(bin_acc)\n",
    "                delta = np.abs(avg_confidence_in_bin - avg_accuracy_in_bin)\n",
    "#                 print(f'bin size {bin_size}, bin conf {avg_confidence_in_bin}, bin acc {avg_accuracy_in_bin}')\n",
    "                res += delta * bin_size / (n_objects * n_classes)\n",
    "    return res\n",
    "\n",
    "def get_tace(preds, targets, n_bins=15, threshold=1e-3, **args):\n",
    "    n_objects, n_classes = preds.shape\n",
    "    \n",
    "    res = 0.0\n",
    "    for cur_class in range(n_classes):\n",
    "        cur_class_conf = preds[:, cur_class]\n",
    "        \n",
    "        targets_sorted = targets[cur_class_conf.argsort()]\n",
    "        cur_class_conf_sorted = np.sort(cur_class_conf)\n",
    "        \n",
    "        targets_sorted = targets_sorted[cur_class_conf_sorted > threshold]\n",
    "        cur_class_conf_sorted = cur_class_conf_sorted[cur_class_conf_sorted > threshold]\n",
    "        \n",
    "        bin_size = len(cur_class_conf_sorted) // n_bins\n",
    "                \n",
    "        for bin_i in range(n_bins):\n",
    "            bin_start_ind = bin_i * bin_size\n",
    "            if bin_i < n_bins-1:\n",
    "                bin_end_ind = bin_start_ind + bin_size\n",
    "            else:\n",
    "                bin_end_ind = len(targets_sorted)\n",
    "                bin_size = bin_end_ind - bin_start_ind  # extend last bin until the end of prediction array\n",
    "            bin_acc = (targets_sorted[bin_start_ind : bin_end_ind] == cur_class)\n",
    "            bin_conf = cur_class_conf_sorted[bin_start_ind : bin_end_ind]\n",
    "            avg_confidence_in_bin = np.mean(bin_conf)\n",
    "            avg_accuracy_in_bin = np.mean(bin_acc)\n",
    "            delta = np.abs(avg_confidence_in_bin - avg_accuracy_in_bin)\n",
    "#             print(f'bin size {bin_size}, bin conf {avg_confidence_in_bin}, bin acc {avg_accuracy_in_bin}')\n",
    "            res += delta * bin_size / (n_objects * n_classes)\n",
    "            \n",
    "    return res\n",
    "\n",
    "def get_brier(preds, targets, **args):\n",
    "    one_hot_targets = np.zeros(preds.shape)\n",
    "    one_hot_targets[np.arange(len(targets)), targets] = 1.0\n",
    "    return np.mean(np.sum((preds - one_hot_targets) ** 2, axis=1))\n",
    "\n",
    "def nentr(p, base=None):\n",
    "    \"\"\"\n",
    "    Calculates entropy of p to the base b. If base is None, the natural logarithm is used.\n",
    "    :param p: batches of class label probability distributions (softmax output)\n",
    "    :param base: base b\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    eps = torch.tensor([1e-16], device=p.device)\n",
    "    if base:\n",
    "        base = torch.tensor([base], device=p.device, dtype=torch.float32)\n",
    "        return (p.mul(p.add(eps).log().div(base.log()))).sum(dim=1).abs()\n",
    "    else:\n",
    "        return (p.mul(p.add(eps).log())).sum(dim=1).abs()\n",
    "\n",
    "def uceloss(softmaxes, labels, n_bins=15):\n",
    "    d = softmaxes.device\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1, device=d)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    _, predictions = torch.max(softmaxes, 1)\n",
    "    _, labels = torch.max(labels, 1)\n",
    "    errors = predictions.ne(labels)\n",
    "    uncertainties = nentr(softmaxes, base=softmaxes.size(1))\n",
    "    errors_in_bin_list = []\n",
    "    avg_entropy_in_bin_list = []\n",
    "\n",
    "    uce = torch.zeros(1)\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # Calculate |uncert - err| in each bin\n",
    "        in_bin = uncertainties.gt(bin_lower.item()) * uncertainties.le(bin_upper.item())\n",
    "        prop_in_bin = in_bin.float().mean()  # |Bm| / n\n",
    "        if prop_in_bin.item() > 0.0:\n",
    "            errors_in_bin = errors[in_bin].float().mean()  # err()\n",
    "            avg_entropy_in_bin = uncertainties[in_bin].mean()  # uncert()\n",
    "            uce += torch.abs(avg_entropy_in_bin - errors_in_bin) * prop_in_bin\n",
    "\n",
    "            errors_in_bin_list.append(errors_in_bin)\n",
    "            avg_entropy_in_bin_list.append(avg_entropy_in_bin)\n",
    "\n",
    "    err_in_bin = torch.tensor(errors_in_bin_list, device=d)\n",
    "    avg_entropy_in_bin = torch.tensor(avg_entropy_in_bin_list, device=d)\n",
    "\n",
    "    return uce#, err_in_bin, avg_entropy_in_bin\n",
    "\n",
    "def compute_mean_avg_prec(y_true, y_score):\n",
    "    try:\n",
    "        avg_prec = sklearn.metrics.average_precision_score(y_true, y_score, average=None)\n",
    "        mean_avg_prec = np.nansum(avg_prec) / len(avg_prec)\n",
    "    except ValueError:\n",
    "        mean_avg_prec = 0\n",
    "\n",
    "    return mean_avg_prec\n",
    "\n",
    "def reliability_diagram_multi(conf_avg, acc_avg, rdname, legend=None, leg_idx=0, n_bins=10):\n",
    "    plt.clf()\n",
    "    plt.figure(2)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(np.arange(0, 1.1, 1/n_bins))\n",
    "    #plt.title(title)\n",
    "    plt.plot(conf_avg[acc_avg>0],acc_avg[acc_avg>0], marker='.', label = legend)\n",
    "    plt.legend()\n",
    "    plt.savefig('figures/reliability_diagram/'+rdname+'ece_rel_multi.png',dpi=300)\n",
    "\n",
    "def calibration_metrics(logits_all, labels_all, rdname, plot=False, model_name='graph_network'):\n",
    "    uce = uceloss( logits_all.cpu(), labels_all.cpu())\n",
    "    \n",
    "    logits = logits_all.detach().cpu().numpy()\n",
    "    labels = labels_all.detach().cpu().numpy()\n",
    "    map_value = compute_mean_avg_prec(labels, logits)\n",
    "    \n",
    "    labels = np.argmax(labels, axis=-1)\n",
    "    ece, acc, conf, Bm = ece_eval(logits, labels, bg_cls=-1)\n",
    "    sce = get_sce(logits, labels)\n",
    "    tace = get_tace(logits, labels)\n",
    "    brier = get_brier(logits, labels)\n",
    "    #print('%s:, ece:%0.4f, sce:%0.4f, tace:%0.4f, brier:%.4f, uce:%.4f' %(model_name, ece, sce, tace, brier, uce.item()) )\n",
    "    if plot: reliability_diagram_multi(conf, acc, rdname, legend=model_name)\n",
    "    return(map_value, ece, sce, tace, brier, uce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import utils.io as io\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def seed_everything(seed=27):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def calculate_cls_freq(dataloader, num_classes):\n",
    "    cls_freq = np.zeros((num_classes,1))\n",
    "    for data in dataloader:\n",
    "        edge_labels = data['edge_labels']\n",
    "        edge_labels = np.argmax(edge_labels.cpu().data.numpy(), axis=-1)    \n",
    "        for i in edge_labels:\n",
    "            cls_freq[i] += 1\n",
    "    return cls_freq\n",
    "\n",
    "def evaluate(args, data_const, model, seq, device, dname, rdname, plot = False):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    train_dataset = SurgicalSceneDataset(seq_set = seq['train_seq'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle= True, \\\n",
    "                            collate_fn=collate_fn)\n",
    "    \n",
    "    val_dataset = SurgicalSceneDataset(seq_set = seq['val_seq'], data_dir = seq['data_dir'], \\\n",
    "                            img_dir = seq['img_dir'], dset = seq['dset'], dataconst = data_const, \\\n",
    "                            feature_extractor = args.feature_extractor, reduce_size = False)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle= True, \\\n",
    "                            collate_fn=collate_fn)\n",
    "    \n",
    "    # model evaluate\n",
    "    model.eval()\n",
    "    \n",
    "    # criterion and scheduler\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "    # each epoch has a training and validation step                   \n",
    "    edge_count = 0\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if args.use_cda_t:\n",
    "        cls_freq = calculate_cls_freq(train_dataloader, len(data_const.action_classes))\n",
    "        cls_freq_log = torch.tensor(cls_freq).log()\n",
    "        cls_freq_log_norm = cls_freq_log/cls_freq_log.max()\n",
    "        temp = args.t_scale - (1-cls_freq_log_norm.view(-1))\n",
    "        print(temp)\n",
    "        temp = temp.to(device)\n",
    "    \n",
    "    for data in val_dataloader:\n",
    "        train_data = data\n",
    "        img_name = train_data['img_name']\n",
    "        img_loc = train_data['img_loc']\n",
    "        node_num = train_data['node_num']\n",
    "        roi_labels = train_data['roi_labels']\n",
    "        det_boxes = train_data['det_boxes']\n",
    "        edge_labels = train_data['edge_labels']\n",
    "        edge_num = train_data['edge_num']\n",
    "        features = train_data['features']\n",
    "        spatial_feat = train_data['spatial_feat']\n",
    "        word2vec = train_data['word2vec']\n",
    "        features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = model(node_num, features, spatial_feat, word2vec, roi_labels, validation=True)\n",
    "            \n",
    "            if args.use_t: outputs/args.t_scale\n",
    "            elif args.use_cda_t: outputs = outputs/temp\n",
    "\n",
    "            logits_list.append(outputs)\n",
    "            labels_list.append(edge_labels)       \n",
    "            \n",
    "            # loss and accuracy\n",
    "            loss = criterion(outputs, edge_labels.float())\n",
    "            acc = np.sum(np.equal(np.argmax(outputs.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "            \n",
    "        # accumulate loss and accuracy of the batch\n",
    "        total_loss += loss.item() * edge_labels.shape[0]\n",
    "        total_acc  += acc\n",
    "        edge_count += edge_labels.shape[0]\n",
    "    \n",
    "    logits_all = torch.cat(logits_list).cuda()\n",
    "    labels_all = torch.cat(labels_list).cuda()\n",
    "    \n",
    "    # calculate the loss and accuracy\n",
    "    total_acc = total_acc / edge_count\n",
    "    total_loss = total_loss / len(val_dataloader)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    logits_all = F.softmax(logits_all, dim=1)\n",
    "    map_value, ece, sce, tace, brier, uce = calibration_metrics(logits_all, labels_all, rdname, plot=plot, model_name='graph')\n",
    "    print('acc: %0.6f map: %0.6f loss: %0.6f, ece:%0.6f, sce:%0.6f, tace:%0.6f, brier:%.6f, uce:%.6f' %(total_acc, map_value, total_loss, ece, sce, tace, brier, uce.item()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.545220 map: 0.310203 loss: 12.917654, ece:0.175220, sce:0.038586, tace:0.039160, brier:0.670585, uce:0.190325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.545220 map: 0.307793 loss: 12.876908, ece:0.172491, sce:0.038838, tace:0.038694, brier:0.669546, uce:0.188104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.544358 map: 0.305496 loss: 12.874796, ece:0.165998, sce:0.039054, tace:0.038985, brier:0.670010, uce:0.192863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.549526 map: 0.304686 loss: 12.837717, ece:0.160228, sce:0.038232, tace:0.038162, brier:0.665709, uce:0.177740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.552110 map: 0.304942 loss: 12.816202, ece:0.161559, sce:0.038554, tace:0.038012, brier:0.665234, uce:0.178831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.552972 map: 0.305635 loss: 12.764017, ece:0.156353, sce:0.037605, tace:0.037102, brier:0.659169, uce:0.180840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.348123 map: 0.106250 loss: 17.185218, ece:0.283281, sce:0.066186, tace:0.066656, brier:0.902736, uce:0.272965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.348123 map: 0.106433 loss: 17.188054, ece:0.303973, sce:0.064887, tace:0.066371, brier:0.898044, uce:0.279116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.348123 map: 0.106396 loss: 17.231335, ece:0.301995, sce:0.064336, tace:0.065320, brier:0.899439, uce:0.280195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.341297 map: 0.106598 loss: 17.254846, ece:0.293608, sce:0.064030, tace:0.065383, brier:0.901655, uce:0.285418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.337884 map: 0.106686 loss: 17.333086, ece:0.302423, sce:0.064565, tace:0.065238, brier:0.904095, uce:0.286310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.334471 map: 0.106355 loss: 17.329972, ece:0.307509, sce:0.065009, tace:0.066627, brier:0.906684, uce:0.297568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.505502 map: 0.278866 loss: 13.866001, ece:0.164523, sce:0.037760, tace:0.038028, brier:0.717367, uce:0.173048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.505502 map: 0.275691 loss: 13.834940, ece:0.171182, sce:0.038443, tace:0.038599, brier:0.715592, uce:0.183237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.504814 map: 0.272162 loss: 13.842916, ece:0.171009, sce:0.038492, tace:0.038708, brier:0.716243, uce:0.186807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.507565 map: 0.270959 loss: 13.819301, ece:0.161588, sce:0.037966, tace:0.038129, brier:0.713255, uce:0.173611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.508941 map: 0.270987 loss: 13.819954, ece:0.168911, sce:0.038637, tace:0.038430, brier:0.713368, uce:0.177160\n",
      "acc: 0.508941 map: 0.271440 loss: 13.778674, ece:0.163794, sce:0.037746, tace:0.037846, brier:0.709047, uce:0.178500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "arguments\n",
    "    Hyperparameters, file location, optimizer, network, data_processing\n",
    "'''\n",
    "ver = 'd2g_t_resnet18_09_ts'\n",
    "f_e = 'resnet18_09_ts'\n",
    "\n",
    "class arguments():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.batch_size = 32\n",
    "\n",
    "        # network\n",
    "        self.layers= 1\n",
    "        self.bn = False\n",
    "        self.drop_prob = 0.3\n",
    "        self.bias = True\n",
    "        self.multi_attn = False\n",
    "        self.diff_edge = False\n",
    "\n",
    "        # data_processing\n",
    "        self.sampler = 0\n",
    "        self.data_aug = False\n",
    "        self.feature_extractor = f_e\n",
    "        \n",
    "        # CBS\n",
    "        self.use_cbs = False\n",
    "        \n",
    "        # temperature_scaling\n",
    "        self.use_t = True\n",
    "        self.use_cda_t = False\n",
    "        self.t_scale = 1.5\n",
    "        \n",
    "        self.testset = [1,2,12]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    seed_everything()\n",
    "    args = arguments()\n",
    "    data_const = SurgicalSceneConstants()\n",
    "    \n",
    "    for domain in args.testset:\n",
    "        # val dataset\n",
    "        if domain == 1:\n",
    "            train_seq = [[2,3,4,6,7,9,10,11,12,14,15]]\n",
    "            val_seq = [[1,5,16]]\n",
    "            data_dir = ['datasets/instruments18/seq_']\n",
    "            img_dir = ['/left_frames/']\n",
    "            dset = [0]\n",
    "            seq = {'train_seq': train_seq, 'val_seq': val_seq, 'data_dir': data_dir, 'img_dir':img_dir, 'dset': dset}\n",
    "\n",
    "        elif domain == 2:\n",
    "            train_seq = [[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]]\n",
    "            val_seq = [[16,17,18,19,20,21,22]]\n",
    "            data_dir = ['datasets/SGH_dataset_2020/']\n",
    "            img_dir = ['/resized_frames/']\n",
    "            dset = [1]\n",
    "            seq = {'train_seq': train_seq, 'val_seq': val_seq, 'data_dir': data_dir, 'img_dir':img_dir, 'dset': dset}\n",
    "\n",
    "        elif domain == 12:\n",
    "            train_seq = [[2,3,4,6,7,9,10,11,12,14,15], [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]]\n",
    "            val_seq = [[1,5,16],[16,17,18,19,20,21,22]]\n",
    "            data_dir = ['datasets/instruments18/seq_', 'datasets/SGH_dataset_2020/']\n",
    "            img_dir = ['/left_frames/', '/resized_frames/']\n",
    "            dset = [0, 1]\n",
    "            seq = {'train_seq': train_seq, 'val_seq': val_seq, 'data_dir': data_dir, 'img_dir':img_dir, 'dset': dset}\n",
    "\n",
    "        # model\n",
    "        model = AGRNN(bias=args.bias, bn=args.bn, dropout=args.drop_prob, multi_attn=args.multi_attn, layer=args.layers, diff_edge=args.diff_edge, use_cbs = args.use_cbs)\n",
    "        if args.use_cbs: model.grnn1.gnn.apply_h_h_edge.get_new_kernels(0)\n",
    "\n",
    "        for i in [30,40,50,60,70,80]:\n",
    "    #         pretrain_model = 'checkpoints/'+ver+'/'+ver+'/'+'epoch_train/checkpoint_D2F'+str(i)+'_epoch.pth'\n",
    "            pretrain_model = 'checkpoints/'+ver+'/'+ver+'/'+'epoch_train/checkpoint_D2F'+str(i)+'_epoch.pth'\n",
    "            checkpoints = torch.load(pretrain_model)\n",
    "            model.load_state_dict(checkpoints['state_dict'])\n",
    "\n",
    "            # use cpu or cuda\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            model.to(device)\n",
    "\n",
    "            evaluate(args, data_const, model,seq, device, \"D12\", str(i))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.572902 map: 0.256132 loss: 14.258467, ece:0.109412, sce:0.032408, tace:0.030969, brier:0.620716, uce:0.105018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    for i in [250]:\n",
    "        pretrain_model = 'checkpoints/'+ver+'/'+ver+'/'+'epoch_train/checkpoint_D1'+str(i)+'_epoch.pth'\n",
    "        checkpoints = torch.load(pretrain_model)\n",
    "        model.load_state_dict(checkpoints['state_dict'])\n",
    "    \n",
    "        # use cpu or cuda\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "    \n",
    "        evaluate(args, data_const, model,seq, device, \"D12\", str(i), plot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_classes = ( 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                                'Tool_Manipulation', 'Cutting', 'Cauterization', \n",
    "                                'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', \n",
    "                                'Ultrasound_Sensing')\n",
    "len(action_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
